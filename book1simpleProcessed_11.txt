COMPUTER GRAPHICS C VERSION DONALD HEARN  M PAULINE BAKER Contents   PREFACE xvii A Survey of Computer Graphics Computer Aided Design Presentation Graphics in Computer Art Entertainment Education and Training Visualization Image Processing Graphical User Interfaces  Overview of Graphics Systems Video Display Devices Refresh Cathode Ray Tubes Raster Scan Displays Random Scan Displays Color CRT Monitors Direct View Storage Tubes Flat Panel Displays Three Dimensional Viewing Devices Stereoscopic and Virtual Reality Systems Raster Scan Systems Video Controller Raster Scan Display Processor Random Scan Systems Graphics Monitors and Workstations Input Devices Keyboards Mouse Trackball and Spaceball Joysticks Data Glove Digitizers Image Scanners Touch Panels Light Pens Voice Systems Hard Copy Devices Graphics Software Coordinate Representations Graphics Functions Software Standards PHIGS Workstations Summary References Exercises Contents  Output Primitives Points and Lines Line Drawing Algorithms 8b DDA Algorithm Bresenham s Line Algorithm Parallel Line Algorithms Loading the Frame Buffer Line Function Circle Generating Algorithms Properties of Circles Midpoint Circle Algorithm  Ellipse Generating Algorithms Properties of Ellipses Midpoint Ellipse Algorithm Other Curves Conic Sections Polynomials and Spline Curves Parallel Curve Algorithms Curve Functions Pixel Addressing and Object Geometry Screen Grid Coordinates Maintaining Geometric Properties of Displayed Objects Filled Area Primitives Scan Line Polygon Fill Algorithm Inside Outside Tests Scan Line Fill of Curved Boundary Areas Boundary Fill Algorithm Flood Fill Algorithm Fill Area Functions Cell Array Character Generation Summary Applications References Exercises Attributes of Output Primitives Line Attributes Line Type Line Width Pen and Brush Options Line Color Curve Attributes Color and Grayscale Levels Color Tables Grayscale Area Fill Attributes Fill Styles Pattern Fill Soft Fill Character Attributes Text Attributes Marker Attributes Bundled Attributes Bundled Line Attributes Bundled Area Fill Attributes Bundled Text Attributes Bundled Marker Attributes Inquiry Functions Antialiasing Supersampling Straight Line Segments Pixel Weighting Masks Area Sampling Straight Line Segments Filtering Techniques Pixel Phasing Compensating for Line Intensity Differences Antialiasing Area Boundaries Summary References Exercises Two Dimensional Geometric  Transformations Basic Transformations Translation Rotation Scaling Matrix Representations and Homogeneous Coordinates Composite Transformations Translations Rotations Scalings General Pivot Point Rotation General Fixed Point Scaling General Scaling Directions Concatenation Properties General Composite Transformations and Computational Efficiency Other Transformations Reflection Shear Transformations Between Coordinate Systems Contents Affine Transformations Transformation Functions Raster Methods for Transformations Summary References Exercises Two Dimensional Viewing The Viewing Pipeline Viewing Coordinate Reference Frame Window to Viewport Coordinate Transformation Two Dimensional Viewing Functions Clipping Operations Point Clipping Line Clipping Cohen Sutherland Line Clipping Liang Barsky Line Clipping Nicholl Lee Nicholl Line Clipping Line Clipping Using Nonrectangular Clip Windows Splitting Concave Polygons Polygon Clipping Sutherland Hodgeman Polygon Clipping Weiler Atherton Polygon Clipping Other Polygon Clipping Algorithms Curve Clipping Text Clipping Exterior Clipping Summary References Exercises Contents Structures and Hierarchical  Modeling Structure Concepts Basic Structure Functions Setting Structure Attributes Editing Structures Structure Lists and the Element Pointer Setting the Edit Mode Inserting Structure Elements Replacing Structure Elements Deleting Structure Elements Labeling Structure Elements Copying Elements from One Structure to Another Basic Modeling Concepts Model Representations Symbol Hierarchies Modeling Packages Hierarchical Modeling with Structures Local Coordinates and Modeling Transformations Modeling Transformations Structure Hierarchies Summary References Exercises Graphical User Interfaces and Interactive Input Methods The User Dialogue Windows and Icons  Accommodating Multiple Skill Levels Consistency Minimizing Memorization Backup and Error Handling Feedback Input of Graphical Data Logical Classification of Input Devices Locator Devices Stroke Devices String Devices Valuator Devices Choice Devices Pick Devices Input Functions Input Modes Request Mode Locator and Stroke Input in Request Mode String Input in Request Mode Valuator Input in Request Mode Choice Input in Request Mode Pick Input in Request Mode Sample Mode Event Mode Concurrent Use of Input Modes Initial Values for Input Device Parameters Interactive Picture Construction Techniques Basic Positioning Methods Constraints Grids Gravity Field Rubber Band Methods Dragging Painting and Drawing Virtual Reality Environments Summary References Exercises Three Dimensional Concepts Three Dimensional Display Methods Parallel Projection Perspective Projection Depth Cueing Visible Line and Surface Identification Surface Rendering Exploded and Cutaway Views Three Dimensional and Stereoscopic Views  Three Dimensional Graphics Packages Three Dimensional Object Representations 10 Polygon Surfaces Polygon Tables Plane Equations Polygon Meshes 10 Curved Lines and Surfaces 10 Quadric Surfaces Sphere S11 Ellipsoid 3u Torus Contents Superquadrics Superellipse Superellipsoid Blobby Objects Spline Representations Interpolation and Approximation Splines Parametric Continuity Conditions Geometric Continuity Conditions Spline Specifications Cubic Spline Interpolation Methods Natural Cubic Splines Hermite Interpolation Cardinal Splines Kochanek Bartels Splines B zier Curves and Surfaces B zier Curves Properties of B zier Curves Design Techniques Using B zier Curves Cubic B zier Curves B zier Surfaces B Spline Curves and Surfaces B Spline Curves Uniform Periodic B Splines Cubic Periodic B Splines Open Uniform B Splines Nonuniform B Splines B Spline Surfaces 10 Beta Splines Beta Spline Continuity Conditions Cubic Periodic Beta Spline Matrix Representation 11 Rational Splines Contents Conversion Between Spline Representations Displaying Spline Curves and Surfaces Horner s Rule Forward Difference Calculations Subdivision Methods Sweep Representations Constructive Solid Geometry Methods Octrees BSP Trees Fractal Geometry Methods Fractal Generation Procedures Classification of Fractals Fractal Dimension Geometric Construction of Deterministic Self Similar Fractals Geometric Construction of Statistically Self Similar Fractals Affine Fractal Construction Methods Random Midpoint Displacement Methods Controlling Terrain Topography Self Squaring Fractals Self Inverse Fractals Shape Grammars and Other Procedural Methods Particle Systems Physically Based Modeling Visualization of Data Sets Visual Representations for Scalar Fields Visual Representations for Vector Fields Visual Representations for Tensor Fields Visual Representations for Multivariate Data Fields Summary References Exercises Three Dimensional Geometric and Modeling Transformations W1 Translation 11 Rotation Coordinate Axes Rotations General Three Dimensional Rotations Rotations with Quaternions 11 Scaling 11 Other Transformations Reflections Shears 11 Composite Transformations 11 Three Dimensional Transformation Functions W1e7 Modeling and Cocrdinate Transformations Summary References Exercises 12 Three Dimensional Viewing Viewing Pipeline 12 Viewing Coordinates Specifying the View Plane Transformation from World to Viewing Coordinates Projections Parallel Projections Perspective Projections View Volumes and General Projection Transformations General Parallel Projection Transformations General Perspective Projection Transformations Clipping Normalized View Volumes Viewport Clipping Clipping in Homogeneous Coordinates Hardware Implementations Three Dimensional Viewing Functions Summary References Exercises Visible Surface Detection Methods 13 Classification of Visible Surface Detection Algorithms 13 Back Face Detection 13 Depth Buffer Method 13 A Buffer Methad 13 Scan Line Method 13 Depth Sorting Method 13 BSP Tree Method 13 Area Subdivision Method 13 Octree Methods 13 Ray Casting Method 13 Curved Surfaces Curved Surface Representations Surface Contour Plots Contents Wireframe Methods Visibitity Detection Functions Summary References Exercises Methods 14 Light Sources 14 Basic Illumination Models Ambient Light Diffuse Reflection Specular Reflection and the Phong Model Combined Diffuse and Specular Reflections with Multiple Light Sources Warn Model Intensity Attenuation Color Considerations Transparency Shadows 14 Displaying Light Intensities Assigning Intensity Levels Gamma Correction and Video Lookup Tables Displaying Continuous Tone Images 14 Halftone Patterns and Dithering Techniques Halftone Approximations Dithering Techniques 14 Polygon Rendering Methods Constant Intensity Shading Gouraud Shading Phong Shading Contents Fast Phong Shading Ray Tracing Methods Basic Ray Tracing Algorithm Ray Surface Intersection Calculations Reducing Object Intersection Calculations Space Subdivision Methods Antialiased Ray Tracing Distributed Ray Tracing Radiosity Lighting Model Basic Radiosity Model Progressive Refinement Radiosity Method Environment Mapping Adding Surface Detail Modeling Surface Detail with Polygons Texture Mapping Procedural Texturing Methods Bump Mapping Frame Mapping Summary References Exercises Color Models and Color Applications 15 Properties of Light 15 Standard Primaries and the Chromaticity Diagram XYZ Color Model CIF Chromaticity Diagram 15 Intuitive Color Concepts 15 RGB Color Model 15 YIQ Color Model 15 CMY Color Model 15 HSV Color Model 15 Conversion Between HSV and RGB Models 15 HLS Color Model 15 Color Selection and Applications Summary References Exercises Computer  Animation 16 Design of Animation Sequences 16 General Computer Animation Functions 16 Raster Animations 16 Computer Animation Languages 16 Key Frame Systems Morphing Simulating Accelerations 16 Motion Specifications Direct Motion Specification Goal Directed Systems Kinematics and Dynamics Summary References Exercises Mathematics for Computer Graphics A Coordinate Reference Frames Two Dimensional Cartesian Reference Frames Polar Coordinates in the xy Plane Three Dimensional Cartesian Reference Frames Three Dimensional Curvilinear Coordinate Systems Solid Angle Points and Vectors Vector Addition and Scalar Multiplication Scalar Product of Two Vectors Vector Product of Two Vectors Basis Vectors and the Metric Tensor Orthonormal Basis Metric Tensor Matrices Scalar Multiplication and Matrix Addition Matrix Multiplication Contents Matrix Transpose Determinant of a Matrix Matrix Inverse A Complex Numbers A Quaternions A Nonparametric Representations A Parametric Representations A Numerical Methods Solving Sets of Linear Equations Finding Roots of Nonlinear Equations Evaluating Integrals Fitting Curves to Data Sets Computer Graphics C Version CHAPTER  A Survey of Computer Graphics      omputers have become a powerful tool for the rapid and economical pro  duction of pictures There is virtually no area in which graphical displays cannot be used to some advantage and so it is not surprising to find the use of computer graphics so widespread Although early applications in engineering and science had to rely on expensive and cumbersome equipment advances in computer technology have made interactive computer graphics a practical tool Today we find computer graphics used routinely in such diverse areas as science engineering medicine business industry government art entertainment ad vertising education and training  summarizes the many applications of graphics in simulations education and graph presentations Before we get into the details of how to do computer graphics we first take a short tour through a gallery of graphics applications    i  Frgtere t Examples of computer graphics applications  Courtesy of DICOMED Corporation   A major use of computer graphics is in design processes particularly for engi neering and architectural systems but almost all products are now computer de signed Generally referred to as CAD computer aided design methods are now routinely used in the design of buildings automobiles aircraft watercraft space craft computers textiles and many many other products  For some design applications objects are first displayed in a wireframe out line form that shows the overall shape and internal features of objects Wireframe displays also allow designers to quickly see the effects of interactive adjustments to design shapes Figures and give examples of wireframe displays in de sign applications  Software packages for CAD applications typically provide the designer with a multi window environment as in Figs and The various displayed windows can show enlarged sections or different views of objects  Circuits such as the one shown in  and networks for communica tions water supply or other utilities are constructed with repeated placement of a few graphical shapes The shapes used in a design represent the different net work or circuit components Standard shapes for electrical electronic and logic circuits are often supplied by the design package For other applications a de signer can create personalized symbols that are to be used to construct the net work or circuit The system is then designed by successively placing components into the layout with the graphics package automatically providing the connec tions between components This allows the designer to quickly try out alternate circuit schematics for minimizing the number of components or the space re quired for the system   Color coded wireframe display for an automobile wheel assembly  Courtesy of Eoans  Sutherland   Color coded wireframe displays of body designs for an aircraft and an automobile  Courtesy of a Evans  Sutherland and b Megatek Corporation  Animations are often used in CAD applications Real time animations using wireframe displays on a video monitor are useful for testing performance of a ve hicle or system as demonstrated in  When we do not display objects with rendered surfaces the calculations for each segment of the animation can be per formed quickly to produce a smooth real time motion on the screen Also wire frame displays allow the designer to see into the interior of the vehicle and to watch the behavior of inner components during motion Animations in virtual reality environments are used to determine how vehicle operators are affected by  Multiple window color coded CAD workstation displays  Courtesy of Intergraph Corporation   A circuit design application using multiple windows and color coded logic components displayed on a Sun workstation with attached speaker and microphone  Courtesy of Sun Microsystems    Simulation of vehicle performance during lane changes  Courtesy of Evans  Sutherland and Mechanical Dynamics Inc  certain motions As the tractor operator in  manipulates the controls the headset presents a stereoscopic view   of the front loader bucket or the backhoe just as if the operator were in the tractor seat This allows the designer to explore various positions of the bucket or backhoe that might obstruct the op erator s view which can then be taken into account in the overall tractor design shows a composite wide angle view from the tractor seat displayed on a standard video monitor instead of in a virtual three dimensional scene And  shows a view of the tractor that can be displayed in a separate window or on another monitor    Operating a tractor in a virtual reality environment As the controls are moved the operator views the front loader backhoe and surroundings through the headset  Courtesy of the National Center for Supercomputing Applications University of Illinois at Urbana Champaign and Caterpillar  Ine     A headset view of the backhoe presented to the tractor operator  Courtesy of the National Center for Supercomputing Applications University of Illinois at Urbana Champaign and Caterpillar Inc   Operator s view of the tractor bucket composited in several sections to form a wide angle view on a standard monitor  Courtesy of the National Center for Supercomputing Applications University of Mlinois at Urbana Champaign and Caterpillar inc  Chapter A Survey of Computer Graphics  View of the tractor displayed on a standard monitor  Courtesy of the National Center for Supercomputing Applications University of Illinois at Urbana Champaign and Caterpillar Inc  When object designs are complete or nearly complete realistic lighting models and surface rendering are applied to produce displays that will show the appearance of the final product Examples of this are given in  Realistic displays are also generated for advertising of automobiles and other vehicles using special lighting effects and background scenes    The manufacturing process is also tied in to the computer description of de signed objects to automate the construction of the product A circuit board lay out for example can be transformed into a description of the individual processes needed to construct the layout Some mechanical parts are manufac tured by describing how the surfaces are to be formed with machine tools Figure 13 shows the path to be taken by machine tools over the surfaces of an object during its construction Numerically controlled machine tools are then set up to manufacture the part according to these construction layouts   Realistic renderings of design products  Courtesy of a Intergraph Corporation and b Evans  Sutherland   Studio lighting effects and realistic ACAD layout for describing the surface rendering techniques are numerically controlled machining applied to produce advertising of a part The part surface is pieces for finished products The displayed in one color and the tool data for this rendering of a Chrysler path in another color  Courtesy of Laser was supplied by Chrysler Los Alamos National Laboratory  Corporation  Courtesy of Eric Haines 3D EYE Inc    a         i MICHEN e   ime TT ee re rr  rid i a fl Ig li fl uz eof r Ep IG Faby Adio II Ltd  i RENEE att s t  Ht   Architectural CAD layout for a building design  Courtesy of Precision Visuals Inc  Boulder Colorado   Architects use interactive graphics methods to lay out floor plans such as  that show the positioning of rooms doors windows stairs shelves counters and other building features Working from the display of a building layout on a video monitor an electrical designer can try out arrangements for wiring electrical outlets and fire warning systems Also facility layout packages can be applied to the layout to determine space utilization in an office or on a manufacturing floor  Realistic displays of architectural designs as in  permit both archi tects and their clients to study the appearance of a single building or a group of buildings such as a campus or industrial complex With virtual reality systems designers can even go for a simulated walk through the rooms or around the outsides of buildings to better appreciate the overall effect of a particular design In addition to realistic exterior building displays architectural CAD packages also provide facilities for experimenting with three dimensional interior layouts and lighting    Many other kinds of systems and products are designed using either gen eral CAD packages or specially developed CAD software  for exam ple shows a rug pattern designed with a CAD system  eageguater   Realistic three dimensional renderings of building designs  a A street level perspective for the World Trade Center project  Courtesy of Skidmore Owings  Merrill   b Architectural visualization of an atrium created for a comptter animation by Marialine Prieur Lyon France  Courtesy of Thomson Digital Image Inc   A hotel corridor providing a sense fixtures along an undulating path and creating a sense of entry by using light towers at each hotel room  Courtesy of Skidmore Owings  Merrill   Another major application area is presentation graphics used to produce illus trations for reports or to generate mm slides or transparencies for use with projectors Presentation graphics is commonly used to summarize financial sta tistical mathematical scientific and economic data for research reports manage rial reports consumer information bulletins and other types of reports Worksta tion devices and service bureaus exist for converting screen displays into mm slides or overhead transparencies for use in presentations Typical examples of presentation graphics are bar charts line graphs surface graphs pie charts and other displays showing relationships between multiple parameters  gives examples of two dimensional graphics combined with ge ographical information This illustration shows three color coded bar charts com bined onto one graph and a pie chart with three sections Similar graphs and charts can be displayed in three dimensions to provide additional information Three dimensional graphs are sometimes used simply for effect they can provide a more dramatic or more attractive presentation of data relationships The charts in  include a three dimensional bar graph and an exploded pie chart  Additional examples of three dimensional graphs are shown in Figs 20 and 21 shows one kind of surface plot and  shows a two dimensional contour plot with a height surface  Oriental rug pattern created with of movement by placing light computer graphics design methods  Courtesy of Lexidata Corporation  W Chapter A Survey of Computer Graphics  Two dimensional bar chart and pie chart linked to a geographical chart  Courtesy of Computer Associates copyright  All rights reserved   Showing relationships with a surface chart  Courtesy of Computer Associates copyright  All rights reserved   Three dimensional bar chart exploded pie chart and line graph  Courtesy of Computer Associates copyright  All rights reserved   Plotting two dimensional contours in the ground plane with a height field plotted as a surface above the ground plane  Courtesy of Computer Associates copyright  All rights reserved     Time chart displaying relevant information about project tasks  Courtesy of Computer Associates copyright  All rights reserved  illustrates a time chart used in task planning Time charts and task network layouts are used in project management to schedule and monitor the progress of projects  Computer graphics methods are widely used in both fine art and commercial art applications Artists use a variety of computer methods including special pur pose hardware artist s paintbrush programs such as Lumena  other paint pack ages such as PixelPaint and SuperPaint  specially developed software symbolic mathematics packages such as Mathematica  CAD packages desktop publish ing software and animation packages that provide facilities for designing object shapes and specifiying object motions  illustrates the basic idea behind a paintbrush program that al lows artists to paint pictures on the screen of a video monitor Actually the pic ture is usually painted electronically on a graphics tablet digitizer using a sty lus which can simulate different brush strokes brush widths and colors A paintbrush program was used to create the characters in  who seem to be busy on a creation of their own   A paintbrush system with a Wacom cordless pressure sensitive stylus was used to produce the electronic painting in  that simulates the brush strokes of Van Gogh The stylus translates changing hand pressure into variable line widths brush sizes and color gradations shows a watercolor painting produced with this stylus and with software that allows the artist to cre ate watercolor pastel or oil brush effects that simulate different drying out times wetness and footprint gives an example of paintbrush methods combined with scanned images  Fine artists use a variety of other computer technologies to produce images To create pictures such as the one shown in  the artist uses a combina tion of three dimensional modeling packages texture mapping drawing pro grams and CAD software In  we have a painting produced on a pen Section Computer Art   p  Cartoon drawing produced with a paintbrush program symbolically illustrating an artist at work on a video monitor  Courtesy of Gould Inc  Imaging  Graphics Division and Aurora Imaging  plotter with specially designed software that can create automatic art without intervention from the artist  shows an example of mathematical art This artist uses a com bination of mathematical functions fractal procedures Mathematica software ink jet printers and other systems to create a variety of three dimensional and two dimensional shapes and stereoscopic image pairs Another example of elec   Cartoon demonstrations of an artist creating a picture with a paintbrush system The picture drawn ona graphics tablet is displayed on the video monitor as the elves look on In b  the cartoon is superimposed on the famous Thomas Nast drawing of Saint Nicholas which was input to the system with a video camera then scaled and positioned  Courtesy Gould Inc  Imaging  Graphics Division and Aurora Imaging  igh  A Van Gogh look alike created by graphics artist Elizabeth O Rourke with a cordless pressure sensitive stylus  Courtesy of Wacorn Technology Corporation   An electronic watercolor painted by John Derry of Time Arts Inc using a cordless pressure sensitive stylus and Lumena gouache brush software  Courtesy of Wacom Technology Corporation   The artist of this picture called Electronic Avalanche makes a statement about our entanglement with technology using a personal computer with a graphics tablet and Lumena software to combine renderings of leaves flower petals and electronics components with scanned images  Courtesy of the Williams Gallery Copyright  by Joan Truckenbrod The School of the Art Institute of Chicago    From a series called Spheres of Influence this electronic painting entitled Whigmalaree was created with a combination of methods using a graphics tablet three dimensional modeling texture mapping and a series of transformations  Courtesy of the Williams Gallery Copyright  by Wynne Ragland  r   Electronic art output to a pen plotter from software specially designed by the artist to emulate his style The pen plotter includes multiple pens and painting instruments including Chinese brushes  Courtesy of the Williams Gallery Copyright  by Roman Verostko Minneapolis College of Art  Design   This creation is based on a visualization of Fermat s Last Theorem x  y   with n  by Andrew Hanson Department of Computer Science Indiana University The image was rendered using Mathematica and Wavefront software  Courtesy of the Williams Gallery Copyright  by Stewart Dickson   Using mathematical functions fractal procedures and supercomputers this artist composer experiments with various designs to synthesize form and color with musical composition  Courtesy of Brian Evans Vanderbilt University  tronic art created with the aid of mathematical relationships is shown in  Section The artwork of this composer is often designed in relation to frequency varia Computer An tions and other parameters in a musical composition to produce a video that inte grates visual and aural patterns  Although we have spent some time discussing current techniques for gen erating electronic images in the fine arts these methods are also applied in com mercial art for logos and other designs page layouts combining text and graph ics TV advertising spots and other areas A workstation for producing page layouts that combine text and graphics is illustrated in   For many applications of commercial art and in motion pictures and other applications  photorealistic techniques are used to render images of a product shows an example of logo design and  gives three computer graphics images for product advertising Animations are also used frequently in advertising and television commercials are produced frame by frame where  Page layout workstation  Courtesy Three dimensional rendering for a of Visual Technology  logo  Courtesy of Vertigo Technology Inc   aj b   Product advertising  Courtesy of a Audrey Fleisher and b and c SOFTIMAGE Inc  V7 each frame of the motion is rendered and saved as an image file In each succes sive frame the motion is simulated by moving object positions slightly from their positions in the previous frame When all frames in the animation sequence have been rendered the frames are transferred to film or stored in a video buffer for playback Film animations require frames for each second in the animation se quence If the animation is to be played back on a video monitor frames per second are required  A common graphics method employed in many commercials is morphing where one object is transformed metamorphosed into another This method has been used in TV commercials to turn an oil can into an automobile engine an au tomobile into a tiger a puddle of water into a tire and one person s face into an other face An example of morphing is given in    Computer graphics methods are now commonly used in making motion pic tures music videos and television shows Sometimes the graphics scenes are dis played by themselves and sometimes graphics objects are combined with the ac tors and live scenes  A graphics scene generated for the movie Siar Trek The Wrath of Khan is shown in  The planet and spaceship are drawn in wireframe form and will be shaded with rendering methods to produce solid surfaces shows scenes generated with advanced modeling and surface rendering meth ods for two award winning short films  Many TV series regularly employ computer graphics methods shows a scene produced for the series Deep Space Nine And  shows a wireframe person combined with actors in a live scene for the series Stay Tuned   Graphics developed for the Paramount Pictures movie Star Trek The Wrath of Khan  Courtesy of Evans  Sutherland  In  we have a highly realistic image taken from a reconstruction of thir Section teenth century Dadu now Beijing for a Japanese broadcast Entertainment Music videos use graphics in several ways Graphics objects can be com  bined with the live action as in Fig 38 or graphics and image processing tech  niques can be used to produce a transformation of one person or object into an  other morphing  An example of morphing is shown in the sequence of scenes in   produced for the David Byrne video She s Mad  a  b   a A computer generated scene from the film Red s Dream copyright  Pixar  b A computer generated scene from the film Knickknack copyright  Pixar  Courtesy of Pixar    A graphics scene in the TV series Deep Space Nine  Courtesy of Rhythm  Hues Studios  Chapter A Survey of Computer Graphics   Graphics combined with a live scene in the TV series Stay Tuned  Courtesy of Rhythm  Hues Studios    An image from a reconstruction of thirteenth century Dadu Beijing today  created by Taisei Corporation Tokyo and rendered with TDI software  Courtesy of Thompson Digital Image Inc  Section Education and Training   Examples of morphing from the David Byrne video She s Mad  Courtesy of David Byrne Index Video and Pacific Data Images   Computer generated models of physical financial and economic systems are often used as educational aids Models of physical systems physiological sys tems population trends or equipment such as the color coded diagram in Fig  can help trainees to understand the operation of the system  For some training applications special systems are designed Examples of such specialized systems are the simulators for practice sessions or training of ship captains aircraft pilots heavy equipment operators and air traffic control personnel Some simulators have no video screens for example a flight simula tor with only a control panel for instrument flying But most simulators provide graphics screens for visual operation Two examples of large simulators with in ternal viewing systems are shown in Figs 42 and 43 Another type of viewing system is shown in  Here a viewing screen with multiple panels is mounted in front of the simulator and color projectors display the flight scene on the screen panels Similar viewing systems are used in simulators for training air craft control tower personnel gives an example of the instructor s area in a flight simulator The keyboard is used to input parameters affecting the airplane performance or the environment and the pen plotter is used to chart the path of the aircraft during a training session  Scenes generated for various simulators are shown in Figs 46 through  An output from an automobile driving simulator is given in  This simulator is used to investigate the behavior of drivers in critical situations The drivers reactions are then used as a basis for optimizing vehicle design to maxi mize traffic safety rae Te  Color coded diagram used ta A large enclosed flight simulator explain the operation of a nuclear with a full color visual system and reactor  Courtesy of Los Alamos six degrees of freedom in its National Laboratory  motion  Courtesy of Frasca International   A military tank simulator with a visual imagery system  Courtesy of Mediatech and GE Aerospace   Section Education and Training  A flight simulator with an external full color viewing system  Courtesy of Frasca International    An instructor s area in a flight simulator The equipment allows the instructor to monitor flight conditions and to set airplane and environment parameters  Courtesy of Frasca International    Flight simulator imagery  Courtesy of Evans  Sutherland   Imagery generated for a naval simulator  Courtesy of Eoans  Sutherland   Space shuttle imagery  Courtesy of Mediatech and GE Aerospace   Imagery from an automobile simulator used to test driver reaction  Courtesy of Evans  Sutherland   Scientists engineers medical personnel business analysts and others often need to analyze large amounts of information or to study the behavior of certain processes Numerical simulations carried out on supercomputers frequently pro duce data files containing thousands and even millions of data values Similarly satellite cameras and other sources are amassing large data files faster than they can be interpreted Scanning these large sets of numbers tu determine trends and relationships is a tedious and ineffective process But if the data are converted to a visual form the trends and patterns are often immediately apparent Figure shows an example of a large data set that has been converted to a color coded display of relative heights above a ground plane Once we have plotted the den sity values in this way we can see easily the overall pattern of the data Produc ing graphical representations for scientific engineering and medical data sets and processes is generally referred to as scientific visualization And the term busi ness visualization is used in connection with data sets related to commerce indus try and other nonscientific areas  There are many different kinds of data sets and effective visualization schemes depend on the characteristics of the data A collection of data can con tain scalar values vectors higher order tensors or any combination of these data types And data sets can be two dimensional or three dimensional Color coding is just one way to visualize a data set Additional techniques include contour plots graphs and charts surface renderings and visualizations of volume interi ors In addition image processing techniques are combined with computer graphics to produce many of the data visualizations  Mathematicians physical scientists and others use visual techniques to an alyze mathematical functions and processes or simply to produce interesting graphical representations A color plot of mathematical curve functions is shown in  and a surface plot of a function is shown in  Fractal proce  Section Visualization Chapter A Survey of Computer Graphics   A color coded plot with million density points of relative brightness observed for the Whirlpool Nebula reveals two distinct galaxies  Courtesy of Los Alamos National Laboratory  Tw  Mathematical curve functions plotted in various color combinations  Courtesy of Melvin L Prueitt Los Alamos National Laboratory   Lighting effects and surface rendering techniques were applied to produce this surface Tepresentation for a three dimensional function  Courtesy of Wolfram Research Inc The Maker of Mathematica  dures using quaternions generated the object shown in  and a topologi Section cal structure is displayed in  Scientists are also developing methods for _ Visualization visualizing general classes of data shows a general technique for graphing and modeling data distributed over a spherical surface  A few of the many other visualization applications are shown in Figs 56 through 69 These figures show airflow over the surface of a space shuttle nu merical modeling of thunderstorms study of crack propagation in metals a color coded plot of fluid density over an airfoil a cross sectional slicer for data sets protein modeling stereoscopic viewing of molecular structure a model of the ocean floor a Kuwaiti oil fire simulation an air pollution study a corn grow ing study reconstruction of Arizona s Chaco Canyon ruins and a graph of auto mobile accident statistics   A four dimensional object projected into three dimensional space then projected to a video monitor and color coded The object was generated using quaternions and fractal squaring procedures with an octant subtracted to show the complex Julia set  Courtesy of John C Hart School of Electrical Engineering and Computer Science Washington State University   Four views from a real time interactive computer animation study of minimal surfaces  snails  in the sphere projected to three dimensional Euclidean space  Courtesy of George Francis Department of Mathematics and the National Center for Supercomputing Applications University of Minois at Urbana Champaign Copyright    A method for graphing and modeling data distributed over a spherical surface  Courtesy of Greg Nielson Computer Sctence Department Arizona State University    A visualization of stream surfaces flowing past a space shuttle by Jeff Hultquist and Eric Raible NASA Ames  Courtesy of Sam Uselton NASA Ames Research Center   Numerical model of the surface of a thunderstorm  Courtesy of Bob Wilkelmson Department of Atmospheric Sciences and the National Center for Supercomputing Applications University of Mlinois at Urbana Champaign    Numerical model of airflow inside a thunderstorm  Courtesy of Bob Wilhelmson Department of Atmospheric Sciences and the National Center for Supercomputing Applications University of Minois at Urbana Champaign   Color coded visualization of stress energy density in a crack Propagation study for metal plates modeled by Bob Haber  Courtesy of the National Center for Supercomputing Applications University of Ilinois at Urbana Champaign   Commercial slicer dicer software showing color coded data values over cross sectional slices of a data set  Courtesy of Spyglass Inc  Section Visualization  A fluid dynamic simulation showing a color coded plot of fluid density over a span of grid planes around an aircraft wing developed by Lee Hian Quek John Eickemeyer and Jeffery Tan  Courtesy of the Information Technology Institute Republic of Singapore   Visualization of a protein structure by Jay Siegel and Kim Baldridge SDSC  Courtesy of Stephanie Sides San Diego Supercomputer Center    Stereoscopic viewing of a molecular structure using a boom device  Courtesy of the National Center for Supercomputing Applications University of Illinois at Urbana Champaign   One image from a stereogcopic pair showing a visualization of the ocean floor obtained from satellite data by David Sandwell and Chris Small Scripps Institution of Ocean ography and Jim Mcleod SDSC  Courtesy of Stephanie Sides San Diego Supercomputer Center   A simulation of the effects of the Kuwaiti oil fire by Gary Glatzmeier Chuck Hanson and Paul Hinker  Courtesy of Mike Krogh Advanced Computing Laboratory at Los Alamos National Laboratory  Section Visualization  A visualization of pollution over the earth s surface by Tom Palmer Cray Research Inc  NCSC Chris Landreth NCSC and Dave Bock NCSC Pollutant SO is plotted as a blue surface acid rain deposition is a color plane on the map surface and rain concentration is shown as clear cylinders  Courtesy of the North Carolina Supercomputing Center MCNC  A visualization of the reconstruction of the ruins at Chaco Canyon Arizona  Courtesy of Melvin L Prueitt Los Alamos National Laboratory Data supplied by Stephen H Lekson   One frame of an animation sequence showing the development of a corn ear  Courtesy of the National Center for Supercomputing Applications University of Mlinois at Urbana Champaign  Ls eg ont regen we Can tec the pti Tel Pee os rere ie eae ahr Analysis  A prototype technique called Win Viz for visualizing tabular multidimensional data is used here to correlate statistical information on pedestrians involved in automobile accidents developed by a visualization team at ITT  Courtesy of Lee Hian Quek Information Technology Institute Republic of Singapore    Although methods used in computer graphics and image processing overlap the two areas are concerned with fundamentally different operations In computer graphics a computer is used to create a picture Image processing on the other hand applies techniques to modify or interpret existing pictures such as pho tographs and TV scans  wo principal applications of image processing are  improving picture quality and  machine perception of visual information as used in robotics  To apply image processing methods we first digitize a photograph or other picture into an image file Then digital methods can be applied to rearrange pic ture parts to enhance color separations or to improve the quality of shading An example of the application of image processing methods to enhance the quality of a picture is shown in  These techniques are used extensively in com mercial art applications that involve the retouching and rearranging of sections of photographs and other artwork Similar methods are used to analyze satellite photos of the earth and photos of galaxies  Medical applications also make extensive use of image processing tech niques for picture enhancements in tomography and in simulations of opera tions Tomography is a technique of X ray photography that allows cross sec tional views of physiological systems to be displayed Both computed X ray tomography CT and position emission tomography PET use projection methods to reconstruct cross sections from digital data These techniques are also used to  A blurred photograph of a license plate becomes legible after the application of image processing techniques  Courtesy of Los Alamos National Laboratory  monitor internal functions and show cross sections during surgery Other med ical imaging techniques include ultrasonics and nuclear medicine scanners With ultrasonics high frequency sound waves instead of X rays are used to generate digital data Nuclear medicine scanners collect digital data from radiation emit ted from ingested radionuclides and plot color coded images  Image processing and computer graphics are typically combined in many applications Medicine for example uses these techniques to model and study physical functions to design artificial limbs and to plan and practice surgery The last application is generally referred to as computer aided surgery Two dimensional cross sections of the body are obtained using imaging tech niques Then the slices are viewed and manipulated using graphics methods to simulate actual surgical procedures and to try out different surgical cuts Exam ples of these medical applications are shown in Figs 71 and 72   One frame from a computer animation visualizing cardiac activation levels within regions of a semitramsparent volume rendered dog heart Medical data provided by William Smith Ed Simpson and G Allan Johnson Duke University Image rendering software by Tom Palmer Cray Research Inc  NCSC  Courtesy of Dave Bock North Carolina Supercomputing Center MCNC  One image from a stereoscopic pair showing the bones of a human hand The images were rendered by Inmo Yoon D E Thompson and W N Waggenspack Jr LSU from a data set obtained with CT scans by Rehabilitation Research GWLNHDC These images show a possible tendon path for reconstructive surgery  Courtesy of IMRLAB Mechanical Engineering Louisiana State University  Section Image Processing   Chapter A Survey of Computer Graphics   GRAPHICAL USER INTERFACES  It is common now for software packages to provide a graphical interface A major component of a graphical interface is a window manager that allows a user to display multiple window areas Each window can contain a different process that can contain graphical or nongraphical displays To make a particular win dow active we simply click in that window using an interactive pointing device  Interfaces also display menus and icons for fast selection of processing op tions or parameter values An icon is a graphical symbol that is designed to look like the processing option it represents The advantages of icons are that they take up less screen space than corresponding textual descriptions and they can be understood more quickly if well designed Menus contain lists of textual descrip tions and icons  illustrates a typical graphical interface containing a window manager menu displays and icons In this example the menus allow selection of processing options color values and graphics parameters The icons represent options for painting drawing zooming typing text strings and other operations connected with picture construction   A graphical user interface showing multiple window areas menus and icons  Courtesy of Image In Corporation  CHAPTER  Overview of Graphics Systems      D ue to the widespread recognition of the power and utility of computer graphics in virtually all fields a broad range of graphics hardware and software systems is now available Graphics capabilities for both two dimen sional and three dimensional applications are now common on general purpose computers including many hand held calculators With personal computers we can use a wide variety of interactive input devices and graphics software pack ages For higher quality applications we can choose from a number of sophisti cated special purpose graphics hardware systems and technologies In this chap ter we explore the basic features of graphics hardware components and graphics software packages  Typically the primary output device in a graphics system is a video monitor Fig  The operation of most video monitors is based on the standard cathode ray tube CRT design but several other technologies exist and solid state monitors may eventually predominate   A computer graphics workstation  Courtesy of Tektronix Inc  Refresh Cathode Ray Tubes illustrates the basic operation of a CRT A beam of electrons cathode rays  emitted by an electron gun passes through focusing and deflection systems that direct the beam toward specified positions on the phosphor coated screen The phosphor then emits a small spot of light at each position contacted by the electron beam Because the light emitted by the phosphor fades very rapidly some method is needed for maintaining the screen picture One way to keep the phosphor glowing is to redraw the picture repeatedly by quickly directing the electron beam back over the same points This type of display is called a refresh CRT  The primary components of an electron gun in a CRT are the heated metal cathode and a control grid   Heat is supplied to the cathode by directing a current through a coil of wire called the filament inside the cylindrical cathode structure This causes electrons to be boiled off the hot cathode surface In the vacuum inside the CRT envelope the free negatively charged electrons are then accelerated toward the phosphor coating by a high positive voltage The acceler      Magnetic Deflection Coils Phosphor Focusing Coated System Screen Electron Connector Electron Beam Pins Gun  Basic design of a magnetic deflection CRT     Electron Focusing Beam Cathode Anode Path Heating Filament Control Accelerating Grid Anode  Operation of an electron gun with an accelerating anode  Section Video Display Devices Chapter Overview of Graphics Systems  ating voltage can be generated with a positively charged metal coating on the in side of the CRT envelope near the phosphor screen or an accelerating anode can be used as in  Sometimes the electron gun is built to contain the acceler ating anode and focusing system within the same unit  Intensity of the electron beam is controlled by setting voltage levels on the control grid which is a metal cylinder that fits over the cathode A high negative voltage applied to the control grid will shut off the beam by repelling electrons and stopping them from passing through the small hole at the end of the control grid structure A smailer negative voltage on the control grid simply decreases the number of electrons passing through Since the amount of light emitted by the phosphor coating depends on the number of electrons striking the screen we control the brightness of a display by varying the voltage on the control grid We specify the intensity level for individual screen positions with graphics software commands as discussed in Chapter  The focusing system in a CRT is needed to force the electron beam to con verge into a small spot as it strikes the phosphor Otherwise the electrons would repel each other and the beam would spread out as it approaches the screen Fo cusing is accomplished with either electric or magnetic fields Electrostatic focus ing is commonly used in television and computer graphics monitors With elec trostatic focusing the electron beam passes through a positively charged metal cylinder that forms an electrostatic lens as shown in  The action of the electrostatic lens focuses the electron beam at the center of the screen in exactly the same way that an optical lens focuses a beam of light at a particular focal dis tance Similar lens focusing effects can be accomplished with a magnetic field set up by a coil mounted around the outside of the CRT envelope Magnetic lens fo cusing produces the smallest spot size on the screen and is used in special purpose devices  Additional focusing hardware is used in high precision systems to keep the beam in focus at all screen positions The distance that the electron beam must travel to different points on the screen varies because the radius of curvature for most CRTs is greater than the distance from the focusing system to the screen center Therefore the electron beam will be focused properly only at the center of the screen As the beam moves to the outer edges of ihe screen displayed images become blurred To compensate for this the system can adjust the focusing ac cording to the screen position of the beam  As with focusing deflection of the electron beam can be controlled either with electric fields or with magnetic fields Cathode ray tubes are now commenly constructed with magnetic deflection coils mounted on the outside of the CRT envelope as illustrated in  Two pairs of coils are used with the coils in each pair mounted on opposite sides of the neck of the CRT envelope One pair is mounted on the top and bottom of the neck and the other pair is mounted on opposite sides of the neck The magnetic field produced by each pair of coils re sults in a transverse deflection force that is perpendicular both to the direction of the magnetic field and to the direction of travel of the electron beam Horizontal deflection is accomplished with one pair of coils and vertical deflection by the other pair The proper deflection amounts are attained by adjusting the current through the coils When electrostatic deflection is used two pairs of parallel plates are mounted inside the CRT envelope One pair of plates is mounted hori zontally to control the vertical deflection and the other pair is mounted vertically to control horizontal deflection    Spots of light are produced on the screen by the transfer of the CRT beam energy to the phosphor When the electrons in the beam collide with the phos      Vertical Phospher Focusing Deflection  Costed System Plates Screen Electron Connector Electron Horizontal Beam Pins Gun Deflection Plates  Electrostatic deflection of the electron beam in a CRT  phor coating they are stopped and their kinetic energy is absorbed by the phos phor Part of the beam energy is converted by friction into heat energy and the remainder causes electrons in the phosphor atoms to move up to higher quan tum energy levels After a short time the excited phosphor electrons begin dropping back to their stable ground state giving up their extra energy as small quantums of light energy What we see on the screen is the combined effect of all the electron light emissions a glowing spot that quickly fades after all the excited phosphor electrons have returned to their ground energy level The frequency or color of the light emitted by the phosphor is proportional to the energy differ ence between the excited quantum state and the ground state  Different kinds of phosphors are available for use in a CRT Besides color a major difference between phosphors is their persistence how long they continue to emit light that is have excited electrons returning to the ground state after the CRT beam is removed Persistence is defined as the time it takes the emitted light from the screen to decay to one tenth of its original intensity Lower persistence phosphors require higher refresh rates to maintain a picture on the screen without flicker A phosphor with low persistence is useful for animation a high persistence phosphor is useful for displaying highly complex static pic tures Although some phosphors have a persistence greater than second graph ics monitors are usually constructed with a persistence in the range from to microseconds shows the intensity distribution of a spot on the screen The in tensity is greatest at the center of the spot and decreases with a Gaussian distrib ution out to the edges of the spot This distribution corresponds to the cross sectional electron density distribution of the CRT beam   The maximum number of points that can be displayed without overlap on a CRT is referred to as the resolution A more precise definition of resolution is the number of points per centimeter that can be plotted horizontally and vertically although it is often simply stated as the total number of points in each direction Spot intensity has a Gaussian distribution   so two adjacent spots will appear distinct as long as their separation is greater than the diameter at which each spot has an intensity of about percent of that at the center of the spot This overlap position is illustrated in  Spot size also depends on intensity As more electrons are accelerated toward the phospher per second the CRT beam diameter and the illuminated spot increase In addition the increased exci tation energy tends to spread to neighboring phosphor atoms not directly in the  Intensity distribution of an illuminated phosphor spot a CRT screen  on Chapter Overview of Graphics Systems  Two illuminated phosphor spots are distinguishable when their separation is greater than the diameter at which a spot intensity has fallen to percent of maximum  path of the beam which further increases the spot diameter Thus resolution of a CRT is dependent on the type of phosphor the intensity to be displayed and the focusing and deflection systems Typical resolution on high quality systems is by with higher resolutions available on many systems High resolution systems are often referred to as high definition systems The physical size of a graphics monitor is given as the length of the screen diagonal with sizes varying from about inches to inches or more A CRT monitor can be at tached to a variety of computer systems so the number of screen points that can actually be plotted depends on the capabilities of the system to which it is at tached  Another property of video monitors is aspect ratio This number gives the ratio of vertical points to horizontal points necessary to produce equal length lines in both directions on the screen  Sometimes aspect ratio is stated in terms of the ratio of horizontal to vertical points  An aspect ratio of means that a ver tical line plotted with three points has the same length as a horizontal line plot ted with four points  Raster Scan Displays The most common type of graphics monitor employing a CRT is the raster scan display based on television technology In a raster scan system the electron beam is swept across the screen one row at a time from top to bottom As the electron beam moves across each row the beam intensity is turned on and off to create a pattern of illuminated spots Picture definition is stored in a memory area called the refresh buffer or frame buffer This memory area holds the set of intensity values for all the screen points Stored intensity values are then re trieved from the refresh buffer and painted on the screen one row scan line at a time   Each screen point is referred to as a pixel or pel shortened forms of picture element  The capability of a raster scan system to store inten sity information for each screen point makes it well suited for the realistic displav of scenes containing subtle shading and color patterns Home television sets and printers are examples of other systems using raster scan methcds  Intensity range for pixel positions depends on the capability of the raster system In a simple black and white system each screen point is either on or oft so only one bit per pixel is needed to control the intensity of screen positions For a bilevel system a bit value of indicates that the electron beam is to be turned on at that position and a value of indicates that the beam intensity is to be off Additional bits are needed when color and intensity variations can be displayed Up to bits per pixel are included in high quality systems which can require several megabytes of storage for the frame buffer depending on the resolution of the system A system with bits per pixel and a screen resolution of bv requires megabytes of storage for the frame buffer On a black and white system with one bit per pixel the frame buffer is commonly called a bitmap For systems with multiple bits per pixel the frame buffer is often referred to as a pixmap  Refreshing on raster scan displays is carried out at the rate of to frames per second although some systems are designed for higher refresh rates Sometimes refresh rates are described in units of cycles per second or Hertz Hz  where a cycle corresponds to one frame Using these units we would de scribe a refresh rate of frames per second as simply Hz At the end of each scan line the electron beam returns to the left side of the screen to begin displav ing the next scan line The return to the left of the screen after refreshing each  A raster scan system displays an object as a set of discrete points across each scan line  scan line is called the horizontal retrace of the electron beam And at the end of each frame displayed in 80th to 60th of a second  the electron beam returns vertical retrace to the top left corner of the screen to begin the next frame  On some raster scan systems and in TV sets  each frame is displayed in two passes using an interlaced refresh procedure In the first pass the beam sweeps across every other scan line from top to bottom Then after the vertical re trace the beam sweeps out the remaining scan lines   Interlacing of the scan lines in this way allows us to see the entire screen displayed in one half the time it would have taken to sweep across all the lines at once from top to bottom Interlacing is primarily used with slower refreshing rates On an older frame per second noninterlaced display for instance some flicker is noticeable But with interlacing each of the two passes can be accomplished in 60th of a sec ond which brings the refresh rate nearer to frames per second This is an effec tive technique for avoiding flicker providing that adjacent scan lines contain sim ilar display information  Random Scan Displays When operated as a random scan display unit a CRT has the electron beam di rected only to the parts of the screen where a picture is to be drawn Random scan monitors draw a picture one line at a time and for this reason are also re ferred to as vector displays or stroke writing or calligraphic displays  The component lines of a picture can be drawn and refreshed by a random scan sys  Chapter Overview of Graphics Systems   Interlacing scan lines on a raster scan display First all points on the even numbered solid scan lines are displayed then all points along the odd numbered dashed lines are displayed  tem in any specified order   A pen plotter operates in a similar way and is an example of a random scan hard copy device  Refresh rate on a random scan system depends on the number of lines to be displayed Picture definition is now stored as a set of line drawing commands in an area of memory referred to as the refresh display file Sometimes the refresh display file is called the display list display program or simply the refresh buffer To display a specified picture the system cycles through the set of com mands in the display file drawing each component line in turn After all line drawing commands have been processed the system cycles back to the first line command in the list Random scan displays are designed to draw all the compo nent lines of a picture to times each second High quality vector systems are capable of handling approximately 000 short lines at this refresh rate When a small set of lines is to be displayed each refresh cycle is delayed to avoid refresh rates greater than frames per second Otherwise faster refreshing of the set of lines could burn out the phosphor  Random scan systems are designed for line drawing applications and can not display realistic shaded scenes Since picture definition is stored as a set of line drawing instructions and not as a set of intensity values for all screen points vector displays generally have higher resolution than raster systems Also vector displays produce smooth line drawings because the CRT beam directly follows the line path A raster system in contrast produces jagged lines that are plotted as discrete point sets  Color CRT Monitors A CRT monitor displays color pictures by using a combination of phosphors that emit different colored light By combining the emitted light from the different phosphors a range of colors can be generated The two basic techniques for pro ducing color displays with a CRT are the beam penetration method and the shadow mask method  The beam penetration method for displaying color pictures has been used with random scan monitors Two layers of phosphor usually red and green are   A random scan system draws the component lines of an object in any order specified  coated onto the inside of the CRT screen and the displayed color depends on how far the electron beam penetrates into the phosphor layers A beam of slow electrons excites only the outer red layer A beam of very fast electrons penetrates through the red layer and excites the inner green layer At intermediate beam speeds combinations of red and green light are emitted to show two additional colors orange and yellow The speed of the electrons and hence the screen color at any point is controlled by the beam acceleration voltage Beam penetration has been an inexpensive way to produce color in random scan monitors but only four colors are possible and the quality of pictures is not as good as with other methods  Shadow mask methods are commonly used in raster scan systems includ ing color TV because they produce a much wider range of colors than the beam penetration method A shadow mask CRT has three phosphor color dots at each pixel position One phosphor dot emits a red light another emits a green light and the third emits a blue light This type of CRT has three electron guns one for each color dot and a shadow mask grid just behind the phosphor coated screen illustrates the delia delta shadow mask method commonly used in color CRT systems The three electron beams are deflected and focused as a group onto the shadow mask which contains a series of holes aligned with the phosphor dot patterns When the three beams pass through a hole in the shadow mask they activate a dot triangle which appears as a small color spot on the screen The phosphor dots in the triangles are arranged so that each electron beam can activate only its corresponding color dot when it passes through the  Electron Guns Cc o _  x Selectian  G of Shadow Mask Magnified Phosphor Dot Blue Triangle _ a  Screen Operation of a delta delta shadow mask CRT Three electron guns aligned with the triangular color dot patterns on the screen are directed to each dot triangle by a shadow mask  shadow mask Another configuration for the three electron guns is an in line arrangement in which the three electron guns and the corresponding red green blue color dots on the screen are aligned along one scan line instead of in a triangular pattern This in line arrangement of electron guns is easier to keep in alignment and is commonly used in high resolution color CRTs  We obtain color variations in a shadow mask CRT by varying the intensity levels of the three electron beams By turning off the red and green guns we get only the color coming from the blue phosphor Other combinations of beam in tensities produce a small light spot for each pixel position since our eyes tend to merge the three colors into one composite The color we see depends on the amount of excitation of the red green and blue phosphors A white or gray area is the result of activating all three dots with equal intensity Yellow is pro duced with the green and red dots only magenta is produced with the blue and red dots and cyan shows up when blue and green are activated equally In some low cost systems the electron beam can only be set to on or off limiting displavs to eight colors More sophisticated systems can set intermediate intensity levels for the electron beams allowing several million different colors to be generated  Color graphics systems can be designed to be used with several types of CRT display devices Some inexpensive home computer systems and video games are designed for use with a color TV set and an RF radio frequency mod ulator The purpose of the RF m dulator is to simulate the signal from a broad cast TV station This means that the color and intensity information of the picture must be combined and superimposed on the broadcast frequency carrier signal that the TV needs to have as input Then the circuitry in the TV takes this signal from the RF modulator extracts the picture information and paints it on the screen As we might expect this extra handling of the picture information by the RF modulator and TV circuitry decreases the quality of displayed images  Composite monitors are adaptations of TV sets that allow bypass of the broadcast circuitry These display devices still require that the picture informa tion be combined but no carrier signal is needed Picture information is com bined into a composite signal and then separated by the monitor so the resulting picture quality is still not the best attainable  Color CRTs in graphics systems are designed as RGB monitors These mon itors use shadow mask methods and take the intensity level for each electron gun red green and blue directly from the computer system without any intermedi ate processing High quality raster graphics systems have bits per pixel in the frame buffer allowing voltage settings for each electron gun and nearly million color choices for each pixel An RGB color system with bits of storage per pixel is generally referred to as a full color system or a true color system  Direct View Storage Tubes An alternative method for maintaining a screen image is to store the picture in formation inside the CRT instead of refreshing the screen A direct view storage tube DVST stores the picture information as a charge distribution just behind the phosphor coated screen Two electron guns are used in a DVST One the pri mary gun is used to store the picture pattern the second the flood gun main tains the picture display  A DVST monitor has both disadvantages and advantages compared to the refresh CRT Because no refreshing is needed very complex pictures can be dis played at very high resolutions without flicker Disadvantages of DVST systems are that they ordinarily do not display color and that selected parts of a picture cannot be erased To eliminate a picture section the entire screen must be erased and the modified picture redrawn The erasing and redrawing process can take several seconds for a complex picture For these reasons storage displays have been largely replaced by raster systems  Flat Panel Displays Although most graphics monitors are still constructed with CRTs other technolo gies are emerging that may soon replace CRT monitors The term flat panel dis play refers to a class of video devices that have reduced volume weight and power requirements compared to a CRT A significant feature of flat panel dis plays is that they are thinner than CRTs and we can hang them on walls or wear them on our wrists Since we can even write on some flat panel displays they will soon be available as pocket notepads Current uses for flat panel displays in clude small TV monitors calculators pocket video games laptop computers armrest viewing of movies on airlines as advertisement boards in elevators and as graphics displays in applications requiring rugged portable monitors  We can separate flat panel displays into two categories emissive displays and nonemissive displays The emissive displays or emitters are devices that convert electrical energy into light Plasma panels thin film electroluminescent displays and light emitting diodes are examples of emissive displays Flat CRTs have also been devised in which electron beams are accelerated parallel to the screen then deflected to the screen But flat CRTs have not proved to be as successful as other emissive devices Nonemmissive displays or nonemitters use optical effects to convert sunlight or light from some other source into graph ics patterns The most important example of a nonemissive flat panel display is a liquid crystal device  Plasma panels also called gas discharge displays are constructed by fill ing the region between two glass plates with a mixture of gases that usually in  Section Video Display Devices  Glass Plate cludes neon A series of vertical conducting ribbons is placed on one glass panel and a sect of horizontal ribbons is built into the other glass panel   Firing voltages applied to a pair of horizontal and vertical conductors cause the gas at the intersection of the two conductors to break down into a glowing plasma of electrons and ions Picture definition is stored in a refresh buffer and the firing voltages are applied to refresh the pixel positions at the intersections of the con ductors times per second Alternating current methods are used to provide faster application of the firing voltages and thus brighter displays Separation between pixels is provided by the electric field of the conductors shows a high definition plasma panel One disadvantage of plasma panels has been that they were strictly monochromatic devices but systems have been de veloped that are now capable of displaying color and grayscale  Thin film electroluminescent displays are similar in construction to a plasma panel The difference is that the region between the glass plates is filled with a phosphor such as zinc sulfide doped with manganese instead of a gas   When a sufficiently high voltage is applied to a pair of crossing elec trodes the phosphor becomes a conductor in the area of the intersection of the two electrodes Electrical energy is then absorbed by the manganese atoms which then release the energy as a spot of light similar to the glowing plasma ef fect in a plasma panel Electroluminescent displays require more power than plasma panels and good color and gray scale displays are hard to achieve  A third type of emissive device is the light emitting diode LED  A matrix of diodes is arranged to form the pixel positions in the display and picture defin ition is stored in a refresh buffer As in scan line refreshing of a CRT information  Basic design of a plasma panel A plasma panel display with a display device resolution of by and a screen diagonal of meters  Courtesy of Photonics Systems  Conductors  Glass Plate Glass Plate Phosphor  Basic design of a thin film electroluminescent display device  is read from the refresh buffer and converted to voltage levels that are applied to the diodes to produce the light patterns in the display  Liquid crystal displays LCDs are commonly used in small systems such as calculators   and portable laptop computers   These non emissive devices produce a picture by passing polarized light from the surround ings or from an internal light source through a liquid crystal material that can be aligned to either block or transmit the light  The term liquid crystal refers to the fact that these compounds have a crys talline arrangement of molecules yet they flow like a liquid Flat panel displays commonly use nematic threadlike liquid crystal compounds that tend to keep the long axes of the rod shaped molecules aligned A flat panel display can then be constructed with a nematic liquid crystal as demonstrated in  Two glass plates each containing a light polarizer at right angles to the other plate sandwich the liquid crystal material Rows of horizontal transparent conductors are built into one glass plate and columns of vertical conductors are put into the other plate The intersection uf two conductors defines a pixel position Nor mally the molecules are aligned as shown in the on state of  Polarized light passing through the material is twisted so that it will pass through the op posite polarizer The light is then reflected back to the viewer To tum off the pixel we apply a voltage to the two intersecting conductors to align the mole cules so that the light is not twisted This type of flat panel device is referred to as a passive matrix LCD Picture definitions are stored in a refresh buffer and the screen is refreshed at the rate of frames per second as in the emissive devices Back lighting is also commonly applied using solid state electronic devices so that the system is not completely dependent on outside light sources Colors can be displayed by using different materials or dyes and by placing a triad of color pixels at each screen location Another method for constructing LCDs is to place a transistor at each pixel location using thin film transistor technology The tran sistors are used to control the voltage at pixel locations and to prevent charge from gradually leaking out of the liquid crystal cells These devices are called active matrix displays  Section Video Display Devices  A hand calculator with an LCD screen  Courtesy of Texas Instruments    A backlit passive matrix liquid crystal display in a laptop computer featuring colors a screen resolution of by and a screen diagonal of inches  Courtesy of Apple Computer Inc   Polarizer   Off State aon The light twisting shutter effect used in the design of most liquid crystal display devices  Three Dimensianal Viewing Devices Section       Video Display Devices Graphics monitors for the display of three dimensional scenes have been devised using a technique that reflects a CRT image from a vibrating flexible mirror The operation of such a system is demonstrated in  As the varifocal mirror vibrates it changes focal length These vibrations are synchronized with the dis play of an object on a CRT so that each point on the object is reflected from the mirror into a spatial position corresponding to the distance of that point from a specified viewing position This allows us to walk around an object or scene and view it from different sides  shows the Genisco SpaceGraph system which uses a vibrating mirror to project three dimensional objects into a cm by cm by cm vol ume This system is also capable of displaying two dimensional cross sectional slices of objects selected at different depths Such systems have been used in medical applications to analyze data from ultrasonography and CAT scan de vices in geological applications to analyze topological and seismic data in de sign applications involving solid objects and in three dimensional simulations of systems such as molecules and terrain  Projected E Image Timing and  Control y     System    z  Vibrating Flexible Mirror  f  a a  f f    f f     be Viewer  peration of a three dimensional display system using a vibrating mirror that changes focal length to match the depth of points in a scene   The SpaceGraph interactive graphics system displays objects in three dimensions using a vibrating flexible mirror  Courtesy of Genisco Computers Corporation  Chapter2 Stereoscopic and Virtual Reality Systems Overvi f ics  erview of Graphics Systems Another technique for representing three dimensional objects is displaying stereoscopic views This method does not produce true three dimensional im ages but it does provide a three dimensional effect by presenting a different view to each eye of an observer so that scenes do appear to have depth    To obtain a stereoscopic projection we first need to obtain two views of a scene generated from a viewing direction corresponding to each eye left and tight  We can constrict the two views as computer generated scenes with differ ent viewing positions or we can use a stereo camera pair to photograph some object or scene When we simultaneous look at the left view with the left eye and the right view with the right eye the two views merge into a single image and we perceive a scene with depth shows two views of a computer generated scene for stereographic projection To increase viewing comfort the areas at the left and right edges of this scene that are visible to only one eye have been eliminated  Viewing a stereoscopic projection  Courtesy of StereoG raphics Corporation  eve a Co  Right  A stereoscopic viewing pair  Courtesy of Jerry Farm  One way to produce a stereoscopic effect is to display each of the two views with a raster system on alternate refresh cycles The screen is viewed through glasses with each lens designed to act as a rapidly alternating shutter that is syn chronized to block out one of the views shows a pair of stereoscopic glasses constructed with liquid crystal shutters and an infrared emitter that syn chronizes the glasses with the views on the screen  Stereoscopic viewing is also a component in virtual reality systems where users can step into a scene and interact with the environment A headset   containing an optical system to generate the stereoscopic views is commonly used in conjuction with interactive input devices to locate and manip ulate objects in the scene A sensing system in the headset keeps track of the viewer s position so that the front and back of objects can be seen as the viewer  Glasses for viewing a stereoscopic scene and an infrared synchronizing emitter  Courtesy of StereoGraphics Corporation   A headset used in virtual reality systems  Courtesy of Virtual Research  Section Video Display Devices    Interacting with a virtual reality environment  Courtesy of the National Center for Supercomputing Applications University of Mlinois at Urbana Champaign   walks through and interacts with the display illustrates interaction with a virtua scene using a headset and a data glove worn on the right hand Section  An interactive virtual reality environment can also be viewed with stereo scopic glasses and a video monitor instead of a headset This provides a means for obtaining a lower cost virtual reality system As an example  shows an ultrasound tracking device with six degrees of freedom The tracking device is placed on top of the video display and is used to monitor head movements so that the viewing position for a scene can be changed as head position changes   An ultrasound tracking device used with stereoscopic glasses to track head position  Courtesy of StereoGraphics Corporation   Interactive raster graphics systems typically employ several processing units In addition to the central processing unit or CPU a special purpose processor called the video controller or display controller is used to control the operation of the display device Organization of a simple raster system is shown in  Here the frame buffer can be anywhere in the system memory and the video controller accesses the frame buffer to refresh the screen In addition to the video controller more sophisticated raster systems employ other processors as co processors and accelerators to implement various graphics operations  Video Controller shows a commonly used organization for raster systems A fixed area of the system memory is reserved for the frame buffer and the video controller is given direct access to the frame buffer memory  Frame buffer locations and the corresponding screen positions are refer enced in Cartesian coordinates For many graphics monitors the coordinate ori  VO Devices  Architecture of a simple raster graphics system   Architecture of a raster system with a fixed portion of the system memory reserved for the frame buffer  Section Raster Scan Systems   The origin of the coordinate system for identifying screen positions is usually specified in the lower left corner  gin is defined at the lower left screen corner   The screen surface is then represented as the first quadrant of a two dimensional system with positive x values increasing to the right and positive y values increasing from bottom to top  On some personal computers the coordinate origin is referenced at the upper left corner of the screen so the y values are inverted  Scan lines are then labeled from ym at the top of the screen to at the bottom Along each scan line screen pixel positions are labeled from to x ux  In  the basic refresh operations of the video controller are dia grammed Two registers are used to store the coordinates of the screen pixels Ini tially the x register is set to and the y register is set to Yq The value stored in the frame buffer for this pixel position is then retrieved and used to set the inten sity of the CRT beam Then the x register is incremented by and the process re peated for the next pixel on the top scan line This procedure is repeated for each pixel along the scan line After the last pixel on the top scam line has been processed the x register is reset to and the y register is decremented by Pixels along this scan line are then processed in turn and the procedure is repeated for each successive scan line After cycling through all pixels along the bottom scan line y   the video controller resets the registers to the first pixel position on the top scan line and the refresh process starts over  Since the screen must be refreshed at the rate of frames per second the simple procedure illustrated in  cannot be accommodated by typical RAM chips The cycle time is too slow To speed up pixel processing video con trollers can retrieve multiple pixel values from the refresh buffer on each pass The multiple pixel intensities are then stored in a separate register and used to control the CRT beam intensity for a group of adjacent pixels When that group of pixels has been processed the next block of pixel values is retrieved from the frarne buffer  A number of other operations can be performed by the video controller be sides the basic refreshing operations For various applications the video con   Honzontal ana De Variical Deflection Voltages  Basic video controller refresh operations   Processor Memory  I System Bus   Devices Architecture of a raster graphics system with a display processor  troller can retrieve pixel intensities from different memory areas on different re fresh cycles In high quality systems for example two frame buffers are often provided so that one buffer can be used for refreshing while the other is being filled with intensity values Then the two buffers can switch roles This provides a fast mechanism for generating real time animations since different views of moving objects can be successively loaded into the refresh buffers Also some transformations can be accomplished by the video controller Areas of the screen can be enlarged reduced or moved from one location to another during the re fresh cycles In addition the video controller often contains a lookup table so that pixel values in the frame buffer are used to access the lookup table instead of controlling the CRT beam intensity directly This provides a fast method for changing screen intensity values and we discuss lookup tables in more detail ir Chapter Finally some systems are designed to allow the video controller to mix the frame buffer image with an input image from a television camera or other input device  Raster Scan Display Processor shows one way to set up the organization of a raster system contain ing a separate display processor sometimes referred to as a graphics controller or a display coprocessor The purpose of the display processor is to free the CPU from the graphics chores In addition to the system memory a separate display processor memory area can also be provided  A major task of the display processor is digitizing a picture definition given in an application program into a set of pixel intensity values for storage in the frame buffer This digitization process is called scan conversion Graphics com mands specifying straight lines and other geometric objects are scan converted into a set of discrete intensity points Scan converting a straight line segment for example means that we have to locate the pixel positions closest to the line path and store the intensity for each position in the frame buffer Similar methods are used for scan converting curved lines and polygon outlines Characters can be defined with rectangular grids as in  or they can be defined with curved  A character defined as a rectangular grid of pixel positions  C  A character defined as a curve outline  outlines as in  The array size for character grids can vary from about by to by or more for higher quality displays A character grid is displayed by superimposing the rectangular grid pattern into the frame buffer at a specified coordinate position With characters that are defined as curve outlines character shapes are scan converted into the frame buffer  Display processors are also designed to perform a number of additional op erations These functions include generating various line styles dashed dotted or solid  displaying color areas and performing certain transformations and ma nipulations on displayed objects Also display processors are typically designed to interface with interactive input devices such as a mouse  In an effort to reduce memory requirements in raster systems methods have been devised for organizing the frame buffer as a linked list and encoding the intensity information One way to do this is to store each scan line as a set of integer pairs One number of each pair indicates an intensity value and the sec ond number specifies the number of adjacent pixels on the scan line that are to have that intensity This technique called min length encoding can result in a considerable saving in storage space if a picture is to be constructed mostly with long runs of a single color each A similar approach can be taken when pixel in tensities change linearly Another approach is to encode the raster as a set of rec tangular areas cell encoding  The disadvantages of encoding runs are that in tensity changes are difficult to make and storage requirements actually increase as the length of the runs decreases In addition it is difficult for the display con troller to process the raster when many short runs are involved  The organization of a simple random scan vector systern is shown in  An application program is input and stored in the system memory along with a graphics package Graphics commands in the application program are translated by the graphics package into a display file stored in the system memory This dis play file is then accessed by the display processor to refresh the screen The dis play processor cycles through each command in the display file program once during every refresh cycle Sometimes the display processor in a random scan system is referred to as a display processing unit or a graphics controller  VO Devices  Architecture of a simple random scan system Graphics patterns are drawn on a random scan system by directing the electron beam along the component lines of the picture Lines are defined by the values for their coordinate endpoints and these input coordinate values are con verted to x and y deflection voltages A scene is then drawn one line at a time by positioning the beam to fill in the line between specified endpoints  Most graphics monitors today operate as raster scan displays and here we sur vey a few of the many graphics hardware configurations available Graphics sys tems range from small general purpose computer systems with graphics capabil ities   to sophisticated full color systems that are designed specifically for graphics applications   A typical screen resolution for personal com   A desktop general purpose computer system that can be used for graphics applications  Courtesy of Apple Computer Inc   Section Graphics Monilors and Workstations  Computer graphics workstations with keyboard and mouse input devices  a The Iris Indigo  Courtesy of Siticon Graphics Corporation  b SPARCstation  Courtesy of Sun Microsystems   puter systems such as the Apple Quadra shown in  is by al though screen resolution and other system capabilities vary depending on the size and cost of the system Diagonal screen dimensions for general purpose per sonal computer systems can range from to inches and allowable color se lections range from to over 000 For workstations specifically designed for graphics applications such as the systems shown in  typical screen reso lution is by with a screen diagonal of inches or more Graphics workstations can be configured with from to bits per pixel full color sys tems  with higher screen resolutions faster processors and other options avail able in high end systems  shows a high definition graphics monitor used in applications such as air traffic control simulation medical imaging and CAD This system has a diagonal screen size of inches resolutions ranging from by to by with refresh rates of Hz or Hz noninterlaced  A multiscreen system called the MediaWall shown in  provides a large wall sized display area This system is designed for applications that re quire large area displays in brightly lighted environments such as at trade shows conventions retail stores museums or passenger terminals MediaWall operates by splitting images into a number of sections and distributing the sec tions over an array of monitors or projectors using a graphics adapter and satel lite control units An array of up to by monitors each with a resolution of by can be used in the MediaWall to provide an overall resolution of by for either static scenes or animations Scenes can be displayed behind mul lions as in  or the mullions can be eliminated to display a continuous picture with no breaks between the various sections  Many graphics workstations such as some of those shown in  are configured with two monitors One monitor can be used to show all features of an object or scene while the second monitor displays the detail in some part of the picture Another use for dual monitor systems is to view a picture on one monitor and display graphics options menus for manipulating the picture com ponents on the other monitor   A very high resolution by color monitor  Courtesy of BARCO Chromatics    The MediaWall A multiscreen display system The image displayed on this by array of monitors was created by Deneba Software  Courtesy of RGB Spectrum     Single and dual monitor graphics workstations  Courtesy of Intergraph Corporation  Figures 38 and 39 illustrate examples of interactive graphics worksta tions containing multiple input and other devices A typical setup for CAD appli cations is shown in  Various keyboards button boxes tablets and mice are attached to the video monitors for use in the design process  shows features of some types of artist s workstations  Multiple workstations for a CAD group  Courtesy of Hewlett Packard Company  S  An artisl s workstation featuring a color raster monitor keyboard graphics tablet with hand cursor and a light table in addition to data storage and telecommunications devices  Courtesy of DICOMED Corporation   Various devices are available for data input on graphics workstations Most sys  tems have a keyboard and one or more additional devices specially designed for interactive input These include a mouse trackball spaceball joystick digitizers dials and button boxes Some other input devices usea in particular applications are data gloves touch panels image scanners and voice systems  Keyboards An alphanumeric keyboard on a graphics system is used primarily as a device for entering text strings The keyboard is an efficient device for inputting such nongraphic data as picture labels associated with a graphics display Keyboards can also be provided with features to facilitate entry of screen coordinates menu selections or graphics functions  Cursor contro keys and function keys are common features on general purpose keyboards Function keys allow users to enter frequently used opera tions in a single keystroke and cursor control keys can be used to select dis played objects or coordinate positions by positioning the screen cursor Other types of cursor positioning devices such as a trackball or joystick are included on some keyboards Additionally a numeric keypad is often included on the key board for fast entry of numeric data Typical examples of general purpose key boards are given in Figs  33 and 34  shows an ergonomic keyboard design  For specialized applications input to a graphics application may come from a set of buttons dials or switches that select data values or customized graphics operations gives an example of a button box and a set of input dials Buttons and switches are often used to input predefined functions and dials are common devices for entering scalar values Real numbers within some defined range are selected for input with dial rotations Potentiometers are used to mea sure dial rotations which are then converted to deflection voltages for cursor movement  Mouse A mouse is small hand held box used to position the screen cursor Wheels or rollers on the bottom of the mouse can be used to record the amount and direc   Ergonomically designed keyboard with removable palm rests The slope of each half of the keyboard can be adjusted separately  Courtesy of Apple Computer Inc  Section input Devices  tion of movement Another method for detecting mouse motion is with an opti cal sensor For these systems the mouse is moved over a special mouse pad that has a grid of horizontal and vertical lines The optical sensor detects movement across the lines in the grid  Since a mouse can be picked up and put down at another position without change in cursor movement it is used for making relative changes in the position of the screen cursor One two or three buttons are usually included on the top of the mouse for signaling the execution of some operation such as recording cur sor position or invoking a function Most general purpose graphics systems now include a mouse and a keyboard as the major input devices as in Figs  33 and 34  Additional devices can be included in the basic mouse design to increase the number of allowable input parameters The Z mouse in  includes   A button box a and a set of input dials b  Courtesy of Vector General    The Z mouse features three buttons a mouse ball underneath a thumbwheel on the side and a trackball on top  Courtesy of Multipoint Technology Corporation  three buttons a thumbwheel on the side a trackball on the top and a standard mouse ball underneath This design provides six degrees of freedom to select spatial positions rotations and other parameters With the Z mouse we can pick up an object rotate it and move it in any direction or we can navigate our view ing position and orientation through a three dimensional scene Applications of the Z mouse include virtual reality CAD and animation  Trackball and Spacebail As the name implies a trackball is a ball that can be rotated with the fingers or palm of the hand as in  to produce screen cursor movement Poten tiometers attached to the ball measure the amount and direction of rotation Trackballs are often mounted on keyboards   or other devices such as the Z mouse    While a trackball is a two dimensional positioning device a spaceball Fig 45 provides six degrees of freedom Unlike the trackball a spaceball does not actually move Strain gauges measure the amount of pressure applied to the spaceball to provide input for spatial positioning and orientation as the ball is pushed or pulled in various directions Spaceballs are used for three dimensional positioning and selection operations in virtual reality systems modeling anima tion CAD and other applications  Joysticks A joystick consists of a small vertical lever called the stick mounted on a base that is used to steer the screen cursor around Most joysticks select screen posi tions with actual stick movement others respond to pressure on the stick Figure 44 shows a movable joystick Some joysticks are mounted on a keyboard oth ers function as stand alone units  The distance that the stick is moved in any direction from its center position corresponds to screen cursor movement in that direction Potentiometers mounted at the base of the joystick measure the amount of movement and springs return the stick to the center position when it is released One or more buttons can be programmed to act as input switches to signal certain actions once a screen position has been selected   A three button track ball  Courtesy of Measurement Systems Inc  Norwalk Connecticut   Section Input Devices Chapter  Overview of Graphics Systems  A moveable joystick  Courtesy of CalComp Group Sanders Associates Inc  In another type of movable joystick the stick is used to activate switches that cause the screen cursor to move at a constant rate in the direction selected Eight switches arranged in a circle are sometimes provided so that the stick can select any one of eight directions for cursor movement Pressurg sensitive joy sticks also called isometric joysticks have a nonmovable stick Pressure on the stick is measured with strain gauges and converted to movement of the cursor in the direction specified  Data Glove shows a data glove that can be used to grasp a virtual object The glove is constructed with a series of sensors that detect hand and finger motions Electromagnetic coupling between transmitting antennas and receiving antennas is used to provide information about the position and orientation of the hand The transmitting and receiving antennas can each be structured as a set of three mutually perpendicular coils forming a three dimensional Cartesian coordinate system Input from the glove can be used to position or manipulate objects in a virtual scene A two dimensional projection of the scene can be viewed on a video monitor or a three dimensional projection can be viewed with a headset  Digitizers A common device for drawing painting or interactively selecting coordinate po sitions on an object is a digitizer  These devices can be used to input coordinate values in either a two dimensional or a three dimensional space Typically a dig itizer is used to scan over a drawing or object and to input a set of discrete coor dinate positions which can be joined with straight line segments to approximate the curve or surface shapes  One type of digitizer is the graphics tablet also referred to as a data tablet  which is used to input two dimensional coordinates by activating a hand cursor or stylus at selected positions on a flat surface A hand cursor contains cross hairs for sighting positions while a stylus is a pencil shaped device that is pointed at  A virtual reality scene displayed on a two dimensional video monitor with input from a data glove and a spaceball  Courtesy of The Computer Graphics Center Darmstadt Germany  positions on the tablet Figures 46 and 47 show examples of desktop and floor model tablets using hand cursors that are available with  or buttons Examples of stylus input with a tablet are shown in Figs 48 and 49 The artist s digitizing system in  uses electromagnetic resonance to detect the three dimensional position of the stylus This allows an artist to produce different brush strokes with different pressures on the tablet surface Tablet size varies from by inches for desktop models to by inches or larger for floor models Graphics tablets provide a highly accurate method for selecting coordi nate positions with an accuracy that varies from about mm on desktop mod els to about 05 mm or less on larger models  Many graphics tablets are constructed with a rectangular grid of wires em bedded in the tablet surface Electromagnetic pulses are generated in sequence ae  The SummaSketch III desktop tablet with a button hand cursor  Courtesy of Summagraphics Corporation  Section input Devices   The NotePad desktop tablet with stylus  Courtesy of CalComp Digitizer Division  a part of CalComp Inc    The Microgrid If tablet with a button hand cursor designed for digitizing larger drawings  Courtesy of Summagraphics Corporation   along the wires and an electric signal is induced in a wire coil in an activated sty lus or hand cursor to record a tablet position Depending on the technology ei ther signal strength coded pulses or phase shifts can be used to determine the position on the tablet  Acoustic or sonic tablets use sound waves to detect a stylus position Ei ther strip microphones or point microphones can be used to detect the sound emitted by an electrical spark from a stylus tip The position of the stylus is calcu   An artist s digitizer system with a pressure sensitive cordless stylus  Courtesy of Wacom Technology Corporation  lated by timing the arrival of the generated sound at the different microphone positions An advantage of two dimensional accoustic tablets is that the micro phones can be placed on any surface to form the tablet work area This can be convenient for various applications such as digitizing drawings in a book  Three dimensional digitizers use sonic or electromagnetic transmissions to record positions One electromagnetic transmission method is similar to that used in the data glove A coupling between the transmitter and receiver is used to compute the location of a stylus as it moves over the surface of an object Fig ure 50 shows a three dimensional digitizer designed for Apple Macintosh com puters As the points are selected on a nonmetallic object a wireframe outline of the surface is displayed on the computer screen Once the surface outline is con structed it can be shaded with lighting effects to produce a realistic display of the object Resolution of this system is from mm to 08 mm depending on the model  Image Scanners Drawings graphs color and black and white photos or text can be stored for computer processing with an image scanner by passing an optical scanning mechanism over the information to be stored The gradations of gray scale or color are then recorded and stored in an array Once we have the internal repre sentation of a picture we can apply transformations to rotate scale or crop the picture to a particular screen area We can also apply various image processing methods to modify the array representation of the picture For scanned text input various editing operations can be performed on the stored documents Some scanners are able to scan either graphical representations or text and they come in a variety of sizes and capabilities A small hand model scanner is shown in  while  and 53 show larger models   A three dimensional digitizing system for use with Apple Macintosh computers  Courtesy of  Mira Imaging  Section Input Devices   A hand held scanner that can be used to input either text or graphics images  Courtesy of Thunderware Inc    Desktop full color scanners  a Flatbed scanner with a resolution of dots per inch  Courtesy of Sharp Electronics Corporation  b Drum scanner with a selectable resolution from to dots per inch  Courtesy of Howtek Inc  Touch Panels As the name implies touch panels allow displayed objects or screen positions to be selected with the touch of a finger A typical application of touch panels is for the selection of processing options that are represented with graphical icons Some systems such as the plasma panels shown in  are designed with touch screens Other systems can be adapted for touch input by fitting a transpar ent device with a touch sensing mechanism over the video monitor screen Touch input can be recorded using optical electrical or acoustical methods  Optical touch panels employ a line of infrared light emitting diodes LEDs along one vertical edge and along one horizontal edge of the frame The opposite vertical and horizontal edges contain light detectors These detectors are used to record which beams are interrupted when the panel is touched The two crossing  A large floor model scanner used to scan architectural and engineering drawings up to inches wide and feet long  Courtesy of Summagraphics Corporation  beams that are interrupted identify the horizontal and vertical coordinates of the screen position selected Positions can be selected with an accuracy of about inch With closely spaced LEDs it is possible to break two horizontal or two ver tical beams simultaneously In this case an average position between the two in terrupted beams is recorded The LEDs operate at infrared frequencies so that the light is not visible to a user illustrates the arrangement of LEDs in an optical touch panel that is designed to match the color and contours of the system to which it is to be fitted  An electrical touch panel is constructed with two transparent plates sepa rated by a smal distance One of the plates is coated with a conducting material and the other plate is coated with a resistive material When the outer plate is touched it is forced into contact with the inner plate This contact creates a volt age drop across the resistive plate that is converted to the coordinate values of the selected screen position  In acoustical touch panels high frequency sound waves are generated in the horizontal and vertical directions across a glass plate Touching the screen causes part of each wave to be reflected from the finger to the emitters The screen position at the point of contact is calculated from a measurement of the time in terval between the transmission of each wave and its reflection to the emitter  Plasma panels with touch screens  Courtesy of Photonics Systems  Section Input Devices Chapter Overview of Graphics Systems   An optical touch panel showing the arrangement of infrared LED units and detectors around the edges of the frame  Courtesy of Carroll Touch Inc  Light Pens shows the design of one type of light pen Such pencil shaped de vices are used to select screen positions by detecting the light coming from points on the CRT screen They are sensitive to the short burst of light emitted from the phosphor coating at the instant the electron beam strikes a particular point Other light sources such as the background light in the room are usually not detected by a light pen An activated light pen pointed at a spot on the screen as the elec tron beam lights up that spot generates an electrical pulse that causes the coordi nate position of the electron beam to be recorded As with cursor positioning de vices recorded light pen coordinates can be used to position an object or to select a processing option  Although light pens are still with us they are not as popular as they once were since they have several disadvantages compared to other input devices that have been developed For one when a light pen is pointed at the screen part of the screen image is obscured by the hand and pen And prolonged use of the light pen can cause arm fatigue Also light pens require special implementations for some applications because they cannot detect positions within black areas To be able to select positions in any screen area with a light pen we must have some nonzero intensity assigned to each screen pixel In addition light pens sometimes give false readings due to background lighting in a room  Voice Systems Speech recognizers are used in some graphics workstations as input devices to accept voice commands The voice system input can be used to initiate graphics  A light pen activated with a button switch  Courtesy of Interactive Computer Products  operations or to enter data These systems operate by matching an input against a predefined dictionary of words and phrases  A dictionary is set up for a particular operator by having the operator speak the command words to be used into the system Each word is spoken several times and the system analyzes the word and establishes a frequency pattern for that word in the dictionary along with the corresponding function to be per formed Later when a voice command is given the system searches the dictio nary for a frequency pattern match Voice input is typically spoken into a micro phone mounted on a headset as in  The microphone is designed to minimize input of other background sounds If a different operator is to use the system the dictionary must be reestablished with that operator s voice patterns Voice systems have some advantage over other input devices since the attention of the operator does not have to be switched from one device to another to enter a command  A speech recognition system  Courtesy of Threshold Technology Inc  Section Input Devices  Chapter  Overview of Graphics Systems HARD COPY DEVICES We can obtain hard copy output for our images in several formats For presenta tions or archiving we can send image files to devices or service bureaus that will produce mm slides or overhead transparencies To put images on film we can simply photograph a scene displayed on a video monitor And we can put our pictures on paper by directing graphics output to a printer or plotter  The quality of the pictures obtained from a device depends on dot size and the number of dots per inch or lines per inch that can be displayed To produce smooth characters in printed text strings higher quality printers shift dot posi tions so that adjacent dots overlap  Printers produce output by either impact or nonimpact methods Impact printers press formed character faces against an inked ribbon onto the paper A line printer is an example of an impact device with the typefaces mounted on bands chains drums or wheels Nonimpact printers and plotters use laser tech niques ink jet sprays xerographic processes as used in photocopying ma chines  electrostatic methods and electrothermal methods to get images onto paper  Character impact printers often have a dot matrix print head containing a rectangular array of protruding wire pins with the number of pins depending on the quality of the printer Individual characters or graphics patterns are obtained by retracting certain pins so that the remaining pins form the pattern to be printed shows a picture printed on a dot matrix printer  In a laser device a laser beam creates a charge distribution on a rotating drum coated with a photoelectric material such as selenium Toner is applied to the drum and then transferred ta paper shows examples of desktop laser printers with a resolution of dots per inch  Ink jet methods produce output by squirting ink in horizontal rows across a roll of paper wrapped on a drum The electrically charged ink stream is deflected by an electric field to produce dot matrix patterns A desktop ink jet plotter with   A picture generated on a dot matrix printer showing how the density of the dot patterns can be varied to produce light and dark areas  Courtesy of Apple Computer Inc   Small footprint laser printers  Courtesy of Texas Instruments  a resolution of dots per inch is shown in  and examples of larger high resolution ink jet printer  plotters are shown in   An electrostatic device places a negative charge on the paper one complete Tow at a time along the length of the paper Then the paper is exposed to a toner The toner is positively charged and so is attracted to the negatively charged areas where it adheres to produce the specified output A color electrostatic printer plotter is shown in  Electrothermal methods use heat in a dot matrix print head to output patterns on heat sensitive paper  We can get limited color output on an impact printer by using different colored ribbons Nonimpact devices use various techniques to combine three color pigments cyan magenta and yellow to produce a range of color patterns Laser and xerographic devices deposit the three pigments on separate passes ink jet methods shoot the three colors simultaneously on a single pass along each print line on the paper   A dot per inch desktop ink jet  plotter  Courtesy of Summagraphics Corporation  Section Hard Copy Devices  lal ib   Floor model ink jet color printers that use variable dot size to achieve an equivalent resolution of to dots per inch  Courtesy of IRIS Graphics Inc  Bedford Massachusetts    An electrostatic printer that can display dots per inch  Courtesy of CalComp Digitizer Division a part of CalComp Inc  Drafting layouts and other drawings are typically generated with ink jet or pen plotters A pen plotter has one or more pens mounted on a carriage or cross bar that spans a sheet of paper Pens with varying colors and widths are used to produce a variety of shadings and line styles Wet ink ball point and felt tip pens are all possible choices for use with a pen plotter Plotter paper can lie flat or be rolled onto a drum or belt Crossbars can be either moveable or stationary while the pen moves back and forth along the bar Either clamps a vacuum or an electrostatic charge hold the paper in position An example of a table top flatbed pen plotter is given in  and a larger rollfeed pen plotter is shown in    A desktop pen plotter with a resolution of 025 mum  Courtesy of Summagraphies Corporation    A large rollfeed pen plotter with automatic multicolor  pen changer and a resolution of 0127 mm  Courtesy of Summagraphics Corporation   There are two general classifications for graphics software general programming  packages and special purpose applications packages A general graphics pro gramming package provides an extensive set of graphics functions that can be Section Graphics Software  used in a high level programming language such as C or FORTRAN An exam ple of a general graphics programming package is the GL Graphics Library sys tem on Silicon Graphics equipment Basic functions in a general package include those for generating picture components straight lines polygons circles and other figures  setting color and intensity values selecting views and applying transformations By contrast application graphics packages are designed for nonprogrammers that users can generate displays without worrying about how graphics operations work The interface to the graphics routines in such packages allows users to communicate with the programs in their own terms Ex amples of such applications packages are the artist s painting programs and vari ous business medical and CAD systems  Coordinate Representations With few exceptions general graphics packages are designed to be used with Cartesian coordinate specifications If coordinate values for a picture are speci fied in some other reference frame spherical hyberbolic etc  they must be con verted to Cartesian coordinates before they can be input to the graphics package Special purpose packages may allow use of other coordinate frames that are ap propriate to the application In general several different Cartesian reference frames are used to construct and display a scene We can construct the shape of individual objects such as trees or furniture in a scene within separate coordi nate reference frames called modeling coordinates or sometimes local coordi nates or master coordinates Once individual object shapes have been specified we can place the objects into appropriate positions within the scene using a refer ence frame called world coordinates Finally the world coordinate description of the scene is transferred to one or more output device reference frames for dis play These display coordinate systems are referred to as device coordinates or screen coordinates in the case of a video monitor Modeling and world coordinate definitions allow us to set any convenient floating point or integer di mensions without being hampered by the constraints of a particular output de vice For some scenes we might want to specify object dimensions in fractions of a foot while for other applications we might want to use millimeters kilometers or light years   Generally a graphics system first converts world coordinate positions to normalized device coordinates in the range from to before final conversion to specific device coordinates This makes the system independent of the various devices that might be used at a particular workstation  illustrates the sequence of coordinate transformations from modeling coordinates to device co ordinates for a two dimensional application An initial modeling coordinate po sition nc Ya in this illustration is transferred to a device coordinate position Xtc Yac With the sequence  mer Yc  Laver Yrs  pcr Yc   Aatcr Ye  The modeling and world coordinate positions in this transformation can be anv floating point values normalized coordinates satisfy the inequalities  x   y   and the device coordinates x and y  are integers within the range  to Xmaxr Youx for a particular output device To accommodate differences in scales and aspect ratios normalized coordinates are mapped into a square area of the output device so that proper proportions are maintained Graphics Functions Section      Graphics Software A general purpose graphics package provides users with a variety of functions for creating and manipulating pictures These routines can be categorized accord ing to whether they deal with output input attributes transformations viewing or general control  The basic building blocks for pictures are referred to as output primitives They include character strings and geometric entities such as points straight lines curved lines filled areas polygons circles etc  and shapes defined with arrays of color points Routines for generating output primitives provide the basic tools for constructing pictures  Attributes are the properties of the output primitives that is an attribute describes how a particular primitive is to be displayed They include intensity and color specifications line styles text styles and area filling patterns Func tions within this category can be used to set attributes for an individual primitive class or for groups of output primitives  We can change the size position or orientation of an object within a scene using geometric transformations Similar modeling transformations are used to construct a scene using object descriptions given in modeling coordinates  Given the primitive and attribute definition of a picture in world coordi nates a graphics package projects a selected view of the picture on an output de vice Viewing transformations are used to specify the view that is to be pre sented and the portion of the output display area that is to be used  Pictures can be subdivided into component parts called structures or seg ments or objects depending on the software package in use Each structure de fines one logical unit of the picture A scene with several objects could reference each individual object in a separate named structure Routines for processing pose     sl Video Monitor sy       Modeling jhe  Rs ransformations  a ae ed World Nort  Coordinates Coordinates Other  Output Device Coordinates   The transformation sequence from modeling coordinates to device coordinates for a two  dimensional scene Object shapes are defined in local modeling coordipate systems then positioned within the overall world coordinate scene World coordinate specifications are then transformed into normalized coordinates At the final step individual device drivers wansier the normalized coordinate representation of the scene to the output devices for isplay Chapter Overview of Graphics Systems  structures carry out operations such as the creation modification and transfor mation of structures  Interactive graphics applications use various kinds of input devices such as a mouse a tablet or a joystick Input functions are used tu control and process the data flow from these interactive devices Finally a graphics package contains a number of housekeeping tasks such as clearing a display screen and initializing parameters We can Jump the func tions for carrving out hese chores under the heading control operations  Sottware Standards The primary goal of standardized graphics software is portability When pack ages are designed with tandard graphics functions software can be moved cas ily from one hardware system to another and used in different implementations and applications Without standards programs designed for one hardware sys tem often cannot be transferred to another system without extensive rewnting of the programs  International and national standards planning organizations in many coun tries have cooperated in an effort to develop a generally accepted standard for computer graphics After considerable effort this work on standards led to the development of the Graphical Kernel System GKS  This system was adupted as the first graphics software standard by the International Standards Organiza tion SO and by various national standards organizations including the Ameri can National Standards Institute ANSI  Although GKS was originally designed as a two dimensional graphics package a three dimensional GKS extension was subsequently developed  he second software standard to be developed and ap proved by the standards orgainzations was PHIGS Programmer s Hierarchical Interactive Graphics Standard  which is an extension of GKS Increased capabil ities for object modeling color specifications surface rendering and picture ma nipulations are provided m PHIGS Subsequently an extension of PHIGS called PHIGS  was developed to provide three dimensional surface shading capabili ties not available in PHICS  Standard graphics tunctions are defined as a set of specifications that is in dependent of anv progr mming language A language binding is then defined for a particular high level programming language This binding gives the svntax for accessing the various standard graphics functions from this language For ex ample the general form of the PHIGS and GKS function for specifying a se quence of n connected two dimensional straight line segments is pelylirein x y  In FORTRAN this procec ure is implemented as a subroutine with the name GPL A graphics programmer using FORTRAN would invoke this procedure with the subroutine call statement CALL GPL N X Y  where  and Y are one dimensional arrays of coordinate values for the line endpoints In C the proce dure would be invoked with ppclyline n pts  where pts is the list of co ordinate endpoint positicns Each language binding is defined to make best use of the corresponding language capabilities and to handle various syntax issues such as data types parameter passing and errors  In the following chapters we use the standard functions defined in PHIGS as a framework for discussing basic graphics concepts and the design and appli cation of graphics packayes Example programs are presented in Pascal to illus trate the algorithms for implementation of the graphics functions and to illustrate also some applications of the functions Descriptive names for functions based on the PHIGS definitions are used whenever a graphics function is referenced in a program  Although PHIGS presents a specification for basic graphics functions it does not provide a standard methodology for a graphics interface to output de vices Nor does it specify methods for storing and transmitting pictures Separate standards have been developed for these areas Standardization for device inter face methods is given in the Computer Graphics Interface CGI system And the Computer Graphics Metafile CGM system specifies standards for archiv ing and transporting pictures  PHIGS Workstations Generally the term workstation refers to a computer system with a combination of input and output devices that is designed for a single user In PHIGS and GKS however the term workstation is used to identify various combinations of graphics hardware and software A PHIGS workstation can be a single output device a single input device a combination of input and output devices a file or even a window displayed ona video monitor  To define and use various workstations within an applications program we need to specify a workstation identifier and the workstation type The following statements give the general structure of a PHIGS program  openPhigs errorFile memorySize  openWorkstation ws connection type  create and display picture  closeWorkstation ws  closePnigs where parameter errorFile is to contain any error messages that are gener ated and parameter memorySize specifies the size of an internal storage area The workstation identifier an integer is given in parameter ws and parameter connection states the access mechanism for the workstation Parameter type specifies the particular category for the workstation such as an input device an output device a combination outin device or an input or output metafile  Any number of workstations can be open in a particular application with input coming from the various open input devices and output directed to all the open output devices We discuss input and output methods in applications pro grams in Chapter  after we have explored the basic procedures for creating and manipulating pictures  SUMMARY In this chapter we have surveyed the major hardware and software features of computer graphics systems Hardware components include video monitors hard copy devices keyboards and other devices for graphics input or output Graphics software includes special applications packages and general program ming packages  The predominant graphics display device is the raster refresh monitor based on television technology A raster system uses a frame buffer to store inten sity information for each screen position pixel  Pictures are then painted on the Summary  Chapters Overview of Graphics Systems screen by retrieving this information from the frame buffer as the electron beam in the CRT sweeps across each scan line from top to bottom Older vector dis plays construct pictures by drawing lines between specified line endpoints Pic ture information is then stored asa set of line drawing instructions  Many other video display devices are available In particular flat panel dis play technology is developing at a rapid rate and these devices may largely re place raster displays in the near future At present flat panel displays are com monly used in small systems and in special purpose systems Flat panel displays include plasma panels and liquid crystal devices Although vector monitors can be used to display high quality line drawings improvements in raster display technology have caused vector monitors to be largely replaced with raster sys tems  Other display technologies include three dimensional and stereoscopic viewing systems Virtual reality systems can include either a stereoscopic head set or a standard video monitor  For graphical input we have a range of devices to choose from Keyboards button boxes and dials are used to input text data values or programming op tions The most popular pointing device is the mouse but trackballs space balls joysticks cursor control keys and thumbwheels are also used to position the screen cursor In virtual reality environments data gloves are commonly used Other input devices include image scanners digitizers touch panels light pens and vaice systems  Hard copy devices for graphics workstations include standard printers and plotters in addition to devices for producing slides transparencies and film out put Printing methods include dot matrix laser ink jet electrostatic and elec trothermal Plotter methods include pen plotting and combination printer plotter devices  Graphics software can be roughly classified as applications packages or programming packages Applications graphics software include CAD packages drawing and painting programs graphing packages and visualization pro grams Common graphics programming packages include PHIGS PHIGS  GKS 3D GKS and GL Software standards such as PHIGS GKS CGl and CGM are evolving and are becoming widely available on a variety of machines  Normally graphics packages require coordinate specifications to be given with respect to Cartesian reference frames Each object for a scene can be defined in a separate modeling Cartesian coordinate system which is then mapped to world coordinates to construct the scene From world coordinates objects are transferred to normalized device coordinates then to the final display device co ordinates The transformations from modeling coordinates to normalized device coordinates are independent of particular devices that might be used in an appli cation Device drivers are then used to convert normalized coordinates to integer device coordinates   Functions in graphics programming packages can be divided into the fol lowing categories output primitives attributes geometric and modeling trans formations viewing transformations structure operations input functions and control operations  Some graphics systems such as PHIGS and GKS use the concept of a workstation to specify devices or software that are to be used for input or out put in a particular application A workstation identifier in these systems can refer to a file a single device such as a raster monitor or a combination of devices such as a monitor keyboard and a mouse Multiple workstations can be open to provide input or to receive output in a graphics application  REFERENCES A general treatment of electronic displays including flat panel devices is available in Sherr  Flat panel devices are discussed in Depp and Howard  Tannas  pro vides a reference for both flat panel displays and CRTs Additional information on raster graphics architecture can be found in Foley et al   Three dimensional terminals are discussed in Fuchs et al   Johnson  and Ikedo  Head mounted dis plays and virtual reality environments are discussed in Chung et al   For information on PHIGS and PHIGS  see Hopgood and Duce  Howard et al  Gaskins  and Blake  Information on the two dimensional GKS stan dard and on the evolution of graphics standards is available in Hopgood et al   An additional reference for GKS is Enderle Kansy and Pfaff   EXERCISES  List the operating characteristics for the following display technologies raster refresh systems vecter refresh systems plasma panels and CDs  List some applications appropriate for each of the display technologies in Exercise  Determine the resolution pixels per centimeter in the x and y directions for the video monitor in use on your system Determine the aspect ratio and explain how relative proportions of objects can be maintained on your svsiem  Consider three different raster systems with resolutrons of by by and by What size frame buffer in bytes is needed for each of these sys tems to store bits per pixel How much storage is required for each system if bits per pixel are to be stored  Suppose an RGB raster system is to be designed using an inch by tO inch screen with a resolution of pixels per inch in each direction If we want to store bits per pixel in the frame buffer how much storage in bytes do we need for the frame buffer  How long would it take to load a by frame buffer with bits per pixel if bits can be transferred per second How long would it take to load a bit per pixel frame buffer with a resolution of by 24 using this same transfer rate  Suppose we have a computer with bits per word and a transfer rate of mip one million instructions per second  How long would t take to fill the frame buffer of a dpi dot per inch laser printer with a page size vf inches by inches  Consider two raster systems with resolutions of by and by How many pixels could be accessed per second in each of these systems by a display con troller that refreshes the screen al a rate of frames per second Whal is the access time per pixel in ach system  nN wo  Suppose we have a video monitor with a display area that measures inches across and inches high If the resolution is by and the aspect ratio is what 1s the diameter of each screen point  10 How much lime is spent scanning across each row of pixels during screen refresh on a raster system with a resolution of by and a refresh rate of frames per second  11 Consider a noninterlaced raster monitor with a resolution of a by ay m scan lines and n pixels per scan line  a refresh rate of frames per secand a horizontal retrace time Of thong aNd a vertical retrace time of t   What is the fraction of the total refresh time per frame spent in retrace of the elect on beany  12 What is the fraction of the total refresh time per Irame spent in retrace of the electron beam for a noninterlaced raster system with a cesolution of by a refresh rate of Hz a horizontal retrace time of microseconds and a vertical retrace time of microseconds  Exercises B2  13  16  17 18  Assuming that a certain full color bit per pixel RGB raster system has a by frame buffer how many distinct color choices intensity levels would we have available How many different colors could we display at any one time   Compare the advantages and disadvantages of a three dimensional monito using a varifocal mirror with stereoscopic system   List the different input and output components that are typically used with virtual  Teality systems Also explain how users interact with a virtual scene displayed with dif ferent output devices such as two dimensional and stereoscopic monitors  Explain how virtual reality systems can be used in design applications What are some other applications for virtual reality systems  List some applications for large screen displays  Explain the differences between a general graphics system designed for a programmer and one designed for a specific application such as architectural design CHAPTER    Output Primitives     af  A picture can be described in several ways Assuming we have a raster dis  play a picture is completely specified by the set of intensities for the pixel positions in the display At the other extreme we can describe a picture as a set of complex objects such as trees and terrain or furniture and walls positioned at specified coordinate locations within the scene Shapes and colors of the objects can be described internally with pixel arrays or with sets of basic geometric struc tures such as straight line segments and polygon color areas The scene is then displayed either by loading the pixel arrays into the frame buffer or by scan con verting the basic geometric structure specifications into pixel patterns Typically graphics programming packages provide functions to describe a scene in terms of these basic geometric structures referred to as output primitives and to group sets of output primitives into more complex structures Each output primi tive is specified with input coordinate data and other information about the way that abject is to be displayed Points and straight line segments are the simplest geometric components of pictures Additional output primitives that can be used to construct a picture include circles and other conic sections quadric surfaces spline curves and surfaces polygon color areas and character strings We begin our discussion of picture generation procedures by examining device level algo rithms for displaying two dimensional output primitives with particular empha sis On scan conversion methods for raster graphics systems In this chapter we also consider how output functions can be provided in graphics packages and we take a look at the output functions available in the PHIGS language  Point plotting is accomplished by converting a single coordinate position fur nished by an application program into appropriate operations for the output de vice in use With a CRT monitor for example the electron beam is turned on to il luminate the screen phosphor at the selected location How the electron beam is positioned depends on the display technology A random scan vector system stures point plotting instructions in the display list and coordinate values in these instructions are converted to deflection voltages that position the electron beam at the screen locations to be plotted during each refresh cycle For a black and white raster system on the other hand a point is plotted by setting the bit value corresponding to a specified screen position within the frame buffer to Then as the electron beam sweeps across each horizontal scan line it emits a burst of electrons plots a point whenever a value of is encountered in the frame buffer With an RGB system the frame buffer is loaded with the color codes for the intensities that are to be displayed at the screen pixel positions  Line drawing is accomplished by calculating intermediate positions along the line path between two specified endpoint positions An output device is then directed to fill in these positions between the endpoints For analog devices such as a vector pen plotter or a random scan display a straight line can be drawn smoothly from one endpoint to the other Linearly varying horizontal and verti cal deflection voltages are generated that are proportional to the required changes in the x and y directions to produce the smooth line  Digital devices display a straight line segment by plotting discrete points between the two endpoints Discrete coordinate positions along the line path are calculated from the equation of the line For a raster video display the line color intensity is then loaded into the frame buffer at the corresponding pixel coordi nates Reading from the frame buffer the video controller then plots the screen pixels Screen locations are referenced with integer values so plotted positions may only approximate actual line positions between two specified endpoints A computed line position of 48 51  for example would be converted to pixel position   This rounding of coordinate values to integers causes lines to be displayed with a stairstep appearance  the jaggies  as represented in Fig The characteristic stairstep shape of raster lines is particularly noticeable on sys tems with low resolution and we can improve their appearance somewhat by displaying them on high resolution systems More effective techniques for smoothing raster lines are based on adjusting pixel intensities along the line paths  For the raster graphics device level algorithms discussed in this chapter ob ject positions are specified directly in integer device coordinates For the time being we will assume that pixel positions are referenced according to scan line number and column number pixel position across a scan line  This addressing scheme is illustrated in  Scan lines are numbered consecutively from starting at the bottom of the screen and pixel columns are numbered from left to right across each scan line In Section 10 we consider alternative pixel ad dressing schemes  To load a specified color into the frame buffer at a position corresponding to column x along scan line y we will assume we have available a low level pro cedure of the form setPixel x y   Stairstep effect jaggies produced when a line is generated as a series of pixel positions  Sectian Points and lines x x  Line path between endpoint positions x  y  and x2 y   Scan  Line Number   Pixel Column Pixel positions referenced by scan Number line number and column number  We sometimes will also want to be able to retrieve the current frame buffer intensity setting for a specified location We accomplish this with the low level function getPixel x y  LINE DRAWING ALGORITHMS The Cartesian slope intercept equation for a straight line is y m x b  with m representing the slope of the line and b as the y intercept Given that the two endpoints of a line segment are specified at positions x  y  and xz   as shown in  we can determine values for the slope m and y intercept b with the following calculations  m  Bo  bey m x   Algorithms for displaying straight lines are based on the line equation and the calculations given in Eqs and For any given x interval Ax along a line we can compute the corresponding y interval Ay from Eq as Ay mAx  Similarly we can obtain the x interval Ax corresponding to a specified Ay as ar   These equations form the basis for determining deflection voltages in analog de vices For lines with slope magnitudes   dx can be set proportional to a small horizontal deflection voltage and the corresponding vertical deflection is then set proportional to Ay as calculated from Eq  For lines whose slopes have magnitudes m  Ay can be set proportional to a small vertical deflec tion voltage with the corresponding horizontal deflection voltage set propor tional to Ax calculated from Eq  For lines with m  Ax  Ay and the hori zontal and vertical deflections voltages are equal In each case a smooth line with slope m is generated between the specified endpoints  On raster systems lines are plotted with pixels and step sizes in the hori zontal and vertical directions are constrained by pixel separations That is we must sample a line at discrete positions and determine the nearest pixel to the line at each sampled position This scan conversion process for straight lines is il lustrated in  for a near horizontal line with discrete sample positions along the x axis  DDA Algorithm The digital differentia analyzer DDA is a scan conversion line algorithm based on calculating either Sy or Ax using Eq or Eq  We sample the line at unit in tervals in one coordinate and determine corresponding integer values nearest the line path for the other coordinate  Consider first a line with positive slope as shown in  If the slope is less than or equal to we sample at unit x intervals Ax  and compute each successive  value as Yeo  Yet mm G  Subscript  takes integer values starting from for the first point and increases by until the final endpoint is reached Since m can be any real number between and the calculated y values must be rounded to the nearest integer  For lines with a positive slope greater than we reverse the roles of x and y That is we sample at unit y intervals Ay  and calculate each succeeding x value as Nye  yt   Equations and are based on the assumption that lines are to be processed from the left endpoint to the right endpoint   If this processing is reversed so that the starting endpoint is at the right then either we have Ax  lLand Yrry  Ye  mt GB or when the slope is greater than we have Ay  with Xpay  Myo  m Equations through can also be used to calculate pixel positions along a line with negative slope If the absolute value of the slope is less than and the start endpoint is at the left we set Ax  and calculate y values with Eq  Section Line Drawing Algorithms   Straight line segment with five sampling positions along the x axis between x and x   When the start endpoint is at the right for the same slope  we set Ax  and obtain y positions from Eq  Similarly when the absolute value of a negative slope is greater than we use Ay  and Eq or we use Ay  and Eq  This algorithm is summarized in the following procedure which accepts as input the two endpoint pixel positions Horizontal and vertical differences be tween the endpoint positions are assigned to parameters dx and dy The differ ence with the greater magnitude determines the value of parameter steps Start ing with pixel position x  y  we determine the offset needed at each step to generate the next pixel position along the line path We loop through this process steps times If the magnitude of dx is greater than the magnitude of dy and xa is less than xb the values of the increments in the x and y directions are and m respectively If the greater change is in the x direction but xa is greater than xb then the decrements and m are used to generate each new point on the line Otherwise we use a unit increment or decrement in the y direction and an x in crement or decrement of m   include device h  define ROUND a  int  a   int dx  xb  xa dy  yb  ya steps k float xIncrement yIncrement   Xa y  ya if abs dxi  abs idy  steps  abs dx  else steps  abs dy  xIncrement  dx  float steps  yIncrement dy  float steps  void lineDDA int xa int ya int xb int yb  f  setPixel ROUND x  ROUND y  for k k steps k   x  s xIncrement y  yIncrement setPixel ROUNDix  ROUND y      The DDA algorithm is a faster method for calculating pixel positions than the direct use of Eq  It eliminates the multiplication in Eq by making use of raster characteristics so that appropriate increments are applied in the x or y direction to step to pixel positions along the line path The accumulation of roundoff error in successive additions of the floating point increment however can cause the calculated pixel positions to drift away from the true line path for long line segments Furthermore the rounding operations and floating point arithmetic in procedure ineDDA are still time consuming We can improve the performance of the DDA algorithm by separating the increments m and m into integer and fractional parts so that all calculations are reduced to integer opera tions A method for calculating m intrements in integer steps is discussed in Section 11 In the following sections we consider more general scan line proce dures that can be applied to both lines and curves  Bresenham s Line Algorithm An accurate and efficient raster line generating algorithm developed by Bresen ham scan converts lines using only incremental integer calculations that can be adapted to display circles and other curves Figures and illustrate sections of a display screen where straight line segments are to be drawn The vertical axes show scan line positions and the horizontal axes identify pixel columns Sampling at unit x intervals in these examples we need to decide which of two possible pixel positions is closer to the line path at each sample step Starting from the left endpoint shown in  we need to determine at the next sampie position whether to plot the pixel at position  or the one at   Simi larly  shows a negative slope line path starting from the left endpoint at pixel position   In this one do we select the next pixel position as  or as   These questions are answered with Bresenham s line algorithm by testing the sign of an integer parameter whose value is proportional to the differ ence between the separations of the two pixel positions from the actual line path  To illustrate Bresenham s approach we first consider the scan conversion process for lines with positive slope less than Pixel positions along a line path are then determined by sampling at unit x intervals Starting from the left end point xp Yo of a given line we step to each successive column x position and plot the pixel whose scan line y value is closest to the line path  demonstrates the ith step in this process Assuming we have determined that the pixel at x  y is to be displayed we next need to decide which pixel to plot in column x   Our choices are the pixels at positions x  y  and x  y   At sampling position x  we label vertical pixel separations from the mathematical line path as and d    The y coordinate on the mathemati cal line at pixel column position x is calculated as y ma  b 10 Then  yy ma b yy and d  ye i y ytl ma The difference between these two separations is d  dy  2m xy    2y  11 A decision parameter p for the kth step in the line algorithm can be ob tained by rearranging Eq 11 so that it involves only integer calculations We ac complish this by substituting m  Ay Ax where Ay and Ax are the vertical and horizontal separations of the endpoint positions and defining  p  Ax d  d   2Ay x  2Ax  yt 12  The sign of p is the same as the sign of d  d  since Ax  for our example Pa rameter c is constant and has the value 2Ay  Ax 2b   which is independent  T Specified  Line Path    WwW 13     Section of a display screen where a straight line segment 1s to be plotted starting from the pixel at column on scan line      Bo    to Specified  Line Path I 51 53  Section of a display screen where a negative slope line segment is to be plotted starting from the pixel at column on scan line    Yeo Yeon ye mxt ve ps       eed Mevr Meas   Section of the screen grid showing a pixel in column x on scan line y that is to be plotted along the path of a line segment with slope O m   Distances between pixel positions and the line y coordinate at sampling position x   of pixel position and will be eliminated in the recursive calculations for p If the pixel at y  is closer to the line path than the pixel at y  that is d  d  then de cision parameter p is negative In that case we plot the lower pixel otherwise we plot the upper pixel  Coordinate changes along the line occur in unit steps in either the x or y di rections Therefore we can obtain the values of successive decision parameters using incremental integer calculations At step k  the decision parameter is evaluated from Eq 12 as Pooy  2Ay  Xa  2AXs Yay tc Subtracting Eq 12 from the preceding equation we have Pest  Pr  2AyOas   Axl  Yd But x   x   s0 that Presi  Py t 2Ay  2ZAx Ys1  Yd 13 where the term y   y is either or depending on the sign cf parameter py This recursive calculation of decision parameters is performed at each inte ger x position starting at the left coordinate endpoint of the line The first para meter po is evaluated from Eq 12 at the starting pixel position xj yo and with m evaluated as Ay Ax Po  2Ay  Ax 14 We can summarize Bresenham line drawing for a line with a positive slope less than in the following listed steps The constants 2Ay and 2Ay  24x are cal  culated once for each line to be scan canverted so the arithmetic involves only integer addition and subtraction of these two constants  Bresenham s Line Drawing Algorithm for m   Input the two line endpoints and store the left endpoint in xq Yo  Load xo yo into the frame buffer that is plot the first point  Calculate constants Ax Ay 24y and 2Ay  2Ax and obtain the start  ing value for the decision parameter as Po  24y  Ax  At each x along the line starting at k  perform the following test If p   the next point to plot is x   yp and Pro  py  2Ay Otherwise the next point to plot is g  y  and Prot  Py  2Ay  2Ax  Repeat step Ax times    Example Bresenham Line Drawing To illustrate the algorithm we digitize the line with endpoints  and   This line has a slope of with The initial decision parameter has the value Po  2Ay  Ax  and the increments for calculating successive decision parameters are 2Ay 2Ay  2Axr  We plot the initial point x yo    and determine successive pixel posi tions along the line path from the decision parameter as  k Px Opa Vers k Px OGa1 Yea           14  14  10  10   A plot of the pixels generated along this line path is shown in   An implementation of Bresenham line drawing for slopes in the range  in is given in the following procedure Endpoint pixel positions for the line are passed to this procedure and pixels are plotted from the left endpoint to the right endpoint The call to set Pixel loads a preset color value into the frame buffer at the specified x y pixel position   include device h  void lineBres int xa int ya int xb int yb  int dx  abs xa  xb  dy  abs ya  yb  int p  dy  dx int twoDy   dy twoDyDx   idy  4x  int x y xEnd    Determine which point to use as start which as end  if ixa  xb   x  xb  y yb  xEnd  xa  else  Section Line Drawing Algorithms  setPixel x y  while x  xEnd   Xt4  if p  Pp  twoby else  yt  Pp  twoDyDx  setPixel x y      Bresenham s algorithm is generalized to lines with arbitrary slope by con sidering the symmetry between the various octants and quadrants of the xy plane For a line with positive slope greater than we interchange the roles of the x and y directions That is we step along the y direction in unit steps and cal culate successive x values nearest the line path Also we could revise the pro gram to plot pixels starting from either endpoint If the initial position for a line with positive slope is the right endpoint both x and y decrease as we step from right to left To ensure that the same pixels are plotted regardless of the starting endpoint we always choose the upper or the lower of the two candidate pixels whenever the two vertical separations from the line path are equal d  d  For negative slopes the procedures are similar except that now one coordinate de creases as the other increases Finally special cases can be handled separately Horizontal lines Ay   vertical lines Ax   and diagonal lines with lAx   Ay each can be loaded directly into the frame buffer without processing them through the line plotting algorithm  Parallel Line Algorithms The line generating algorithms we have discussed so far determine pixel posi tions sequentially With a parallel computer we can calculate pixel positions   Pixel positions along the line path between endpoints  and 21 25   plotted with Bresenham s line algorithm  along a line path simultaneously by partitioning the computations among the various processors available One approach to the partitioning problem is to adapt an existing sequential algorithm to take advantage of multiple processors Alternatively we can look for other ways to set up the processing so that pixel positions can be calculated efficiently in parallel An important consideration in devising a parallel algorithm is to balance the processing load among the avail able processors  Given n processors we can set up a parallel Bresenham line algorithm by subdividing the line path into n partitions and simultaneously generating line segments in each of the subintervals For a line with slope  m  and left endpoint coordinate position xo yp  we partition the line along the positive x di rection The distance between beginning x positions of adjacent partitions can be calculated as Ax  Art ny i G n Pp where Ax is the width of the line and the value for partition width Ax is com puted using integer division Numbering the partitions and the processors as  up to m  we calculate the starting x coordinate for the kth partition as X Xp  kAx  16  As an example suppose Ax  and we have n  processors Then the width of the partitions is and the starting x values for the partitions are xo x   Xo  and x   With this partitioning scheme the width of the last rightmost subinterval will be smaller than the others in some cases In addition if the line endpoints are not integers truncation errors can result in variable width parti tions along the length of the line  To apply Bresenham s algorithm over the partitions we need the initial value for the y coordinate and the initial value for the decision parameter in each partition The change Ay in the y direction over each partition is calculated from the line slope m and partition width Ax  Ay  mAx  17 At the kth partition the starting y coordinate is then Ye  Yo  round kAy  18  The initial decision parameter for Bresenlym s algorithm at the start of the kth subinterval is obtained from Eq 12  Py  kAx  2Ay  round kAy  2Ax  2Ay  Ax 19  Each processor then calculates pixel positions over its assigned subinterval using the starting decision parameter value for that subinterval and the starting coordi nates x  y  We can also reduce the floating point calculations to integer arith metic in the computations for starting values y and p by substituting m  Ay Ax and rearranging terms The extension of the parallel Bresenham algorithm to a line with slope greater than is achieved by partitioning the line in the y di  Section Line Drawing Algorithms  Bounding box for a line with coordinate extents Ax and Ay  rection and calculating beginning x values for the partitions For negative slopes we increment coordinate values in one direction and decrement in the other  Another way to set up parallel algorithms on raster systems is to assign each processor to a particular group of screen pixels With a sufficient number of processors such as a Connection Machine CM with over 000 processors  we can assign each processor to one pixel within some screen region This approach can be adapted to line display by assigning one processor to each of the pixels within the limits of the line coordinate extents bounding rectangle and calculating pixel distances from the line path The number of pixels within the bounding box of a line is Ax Ay   Perpendicular distance d from the line in  to a pixel with coordinates x y is obtained with the calculation d Ax By C 20 where A  SY linelength p  S __ linelength c 2eAY  yoAr linelength with linelength  VAx  Ay  Once the constants A B and C have been evaluated for the line each processor needs to perform two multiplications and two additions to compute the pixel distance d A pixel is plotted if d is less than a specified line thickness parameter  Instead of partitioning the screen into single pixels we can assign to each processor either a scan line or a column of pixels depending on the line slope Each processor then calculates the intersection of the line with the horizontal row or vertical column of pixels assigned that processor For a line with slope Iml  each processor simply solves the line equation for y given an x column value For a line with slope magnitude greater than the line equation is solved for x by each processor given a scan line y value Such direct methods although slaw on sequential machines can be performed very efficiently using multiple proces SOTS  When straight line segments and other objects are scan converted for display with a raster system frame buffer positions must be calculated We have as sumed that this is accomplished with the set Pixel procedure which stores in tensity values for the pixels at corresponding addresses within the frame buffer array Scan conversion algorithms generate pixel positions at successive unit in Ymnx t  Pere    e      tx yh  Gna     ere addr  addr x y Xrax Screen Frame Buffer  Pixel screen positions stored linearly in row major order within the frame buffer  tervals This allows us to use incremental methods to calculate frame buffer ad dresses  As a specific example suppose the frame butfer array is addressed in row major order and that pixel positions vary from  at the lower left screen cor ner to  max Ymax at the top right corner   For a bilevel system bit per pixel  the frame buffer bit address for pixel position x y is calculated as addr x y  addr  y x    I x 21  Moving across a scan line we can calculate the frame buffer address for the pixel at x  y as the following offset from the address for position x y  addr x  y  addr x y   22  Stepping diagonally up to the next scan line from x y  we get to the frame buffer address of x  y  with the calculation addr x  y   addr x y  x   23  where the constant x   is precomputed once for all line segments Similar in cremental calculations can be obtained from Eq 21 for unit steps in the nega tive x and y screen directions Each of these address calculations involves only a single integer additian  Methods for implementing the set Pixel procedure to store pixel intensity values depend on the capabilities of a particular system and the design require ments of the software package With systems that can display a range of intensity values for each pixel frame buffer address calculations would include pixel width number of bits  as well as the pixel screen location  A procedure for specifying straight line segments can be set up in a number of different forms In PHIGS GKS and some other packages the two dimensional line function is  Xmons Ymax   polyline in wePoints  where parameter n is assigned an integer value equal to the number of coordi nate positions to be input and wcPoints is the array of input world coordinate values for line segment endpoints This function is used to define a set of n  connected straight line segments Because series of connected line segments occur more often than isolated line segments in graphics applications polyline provides a more general line function To display a single straight line segment we set n and list the x and y values of the two endpoint coordinates in wePoints  As an example of the use of polyline the following statements generate two connected line segments with endpoints at     and   wePoints x  wePoints y  wePoints x  wePoints y  wePoints  x  wePoints y   polyline  wePoints  Coordinate references in the polyline function are stated as absolute coordi nate values This means that the values specified are the actual point positions in the coordinate system in use  Some graphics systems employ line and point functions with relative co ordinate specifications In this case coordinate values are stated as offsets from the last position referenced called the current position  For example if location  is the last position that has been referenced in an application program a rel ative coordinate specification of   corresponds to an absolute position of   An additional function is also available for setting the current position before the line routine is summoned With these packages a user lists only the single pair of offsets in the line command This signals the system to display a line start ing from the current position to a final position determined by the offsets The current position is then updated to this final line position A series of connected lines is produced with such packages by a sequence of line commands one for each line section to be drawn Some graphics packages provide options allowing the user to specify line endpoints using either relative or absolute coordinates  Implementation of the polyline procedure is accomplished by first per forming a series of coordinate transformations then making a sequence of calls to a device level Jine drawing routine In PHIGS the input line endpoints are ac tually specified in modeling coordinates which are then converted to world co ordinates Next world coordinates are converted to normalized coordinates then to device coordinates We discuss the details for carrving out these twa dimen sional coordinate transformations in Chapter Once in device coordinates we display the polyline by invoking a line routine such as Bresenham s algorithm n  times to connect the n coordinate points Each successive call passes the co ordinate pair needed to plot the next line section where the first endpoint of each coordinate pair is the last endpoint of the previous section To avoid setting the intensity of some endpoints twice we could modify the line algorithm so that the last endpoint of each segment is not platted We discuss methods for avoiding overlap of displayed objects in more detail in Section 10  Since the circle is a frequently used component in pictures and graphs a proce dure for generating either full circles or circular arcs is included in most graphics packages More generally a single procedure can be provided to display either circular or elliptical curves  Ye Properties of Circles A circle is defined as the set of points that are all at a given distance r from a cen ter position x  y    This distance relationship is expressed by the Pythagorean theorem in Cartesian coordinates as Circle with center coordinates x  y  and radius r  x xP  y y2ar 24 We could use this equation to calculate the position of points on a circle circum  ference by stepping along the x axis in unit steps from x  r to x  r and calcu lating the corresponding y values at each position as y  ytVP   FP 25  But this is not the best method for generating a circle One problem with this ap  proach is that it involves considerable computation at each step Moreover the spacing between plotted pixel positions is not uniform as demonstrated in Fig  _  13 We could adjust the spacing by interchanging x and y stepping through y   values and calculating x values whenever the absolute value of the slope of the Positive half of a circle cirele is greater than But this simply increases the computation and processing Pw wane w and required by the algorithm  Ye _ Another way to eliminate the unequal spacing shown in  is to cal  culate points along the circular boundary using polar coordinates r and  Fig  12  Expressing the circle equation in parametric polar form yields the pair of equations  x x rcos  26 y y rsind When a display is generated with these equations using a fixed angular step size a circle is plotted with equally spaced points along the circumference The step size chosen for  depends on the application and the display device Larger an gular separations along the circumference can be connected with straight line segments to approximate the circular path For a more continuous boundary ona taster display we can set the step size at r This plots pixel positions that are approximately one unit apart  Computation can be reduced by considering the symmetry of circles The shape of the circle is similar in each quadrant We can generate the circle section in the second quadrant of the xy plane by noting that the two circle sections are symmetric with respect to the y axis And circle sections in the third and fourth quadrants can be obtained from sections in the first and second quadrants by   Symmetry of a circle Calculation of a circle point x y in one octant yields the circle points shown for the other seven octants   considering symmetry about the x axis We can take this one step further and note that there is also symmetry between octants Circle sections in adjacent oc tants within one quadrant are symmetric with respect to the line dividing the two octants These symmetry conditions are illustrated in Fig 14 where a point at position x y on a one eighth circle sector is mapped into the seven circle points in the other octants of the xy plane Taking advantage of the circle symme try in this way we can generate all pixel positions around a circle by calculating only the points within the sector from x  to x  y  Determining pixel positions along a circle circumference using either Eq 24 or Eq 26 still requires a good deal of computation time The Cartesian equation 24 involves multiplications and square root calculations while the parametric equations contain multiplications and trigonometric calculations More efficient circle algorithms are based on incremental calculation of decision  parameters as in the Bresenham line algorithm which involves only simple inte ger operations  Bresenham s line algorithm for raster displays is adapted to circle genera tion by setting up decision parameters for finding the closest pixel to the circum ference at each sampling step The circle equation 24 however is nonlinear so that square root evaluations would be required to compute pixel distances from a circular path Bresenham s circle algorithm avoids these square root calculations by comparing the squares of the pixel separation distances  A method for direct distance comparison is to test the halfway position be tween two pixels to determine if this midpoint is inside or outside the circle boundary This method is more easily applied to other conics and for an integer circle radius the midpoint approach generates the same pixel positions as the Bresenham circle algorithm Also the error involved in locating pixel positions along any conic section using the midpoint test is limited to one half the pixel separation  Midpoint Circle Algorithm As in the raster line algorithm we sample at unit intervals and determine the closest pixel position to the specified circle path at each step For a given radius r and screen center position x  y  we can first set up our algorithm to calculate pixel positions around a circle path centered at the coordinate origin   Then each calculated position x y is moved to its proper screen position by adding x to x and y to y Along the circle section from x  tox  yin the first quadrant the slope of the curve varies from to  Therefore we can take unit steps in the positive x direction over this octant and use a decision parameter to deter mine which of the two possible y positions is closer to the circle path at each step Positions if the other seven octants are then obtained by symmetry To apply the midpoint method we define a circle function  fort   tyr 27  Any point x y on the boundary of the circke with radius r satisfies the equation feirte x    If the point is in the interior of the circle the circle function is nega tive And if the point is outside the circle the circle function is positive To sum  marize the relative position of any point x v can be determined by checking the sign of the circle function   if x y is inside the circle boundary ferte X y    if x y is on the circle boundary 28  if x y is outside the circle boundary The circle function tests in 28 are performed for the midpositions between pix els near the circle path at each sampling step Thus the circle function is the deci sion parameter in the midpoint algorithm and we can set up incremental calcu lations for this function as we did in the line algorithm   shows the midpoint between the two candidate pixels at sam pling position x   Assuming we have just plotted the pixel at    we next need to determine whether the pixel at position x   y  or the one at position     is closer to the circle Our decision parameter is the circle function 27 evaluated at the midpoint between these two pixels  Pi  fae  Lye  29    y p e If p   this midpoit t is inside the circle and the pixel on scan line y is closer to the circle boundary Otherwise the midposition is outside or on the circle bound ary and we select the pixel on scanline y   Successive decision parameters are obtained using incremental calculations We obtain a recursive expression for the next decision parameter by evaluating the circle function at sampling position x     x   Pray  fae a0 tL Yee     y  1P  ve  shoe or Pest  Pe t  D Wha  D  Yer  ed  30  where y  is either y or y_  depending on the sign of py  Increments for obtaining p  are either 2x    if p is negative or 2x     Evaluation of the terms 2x  and   can also be done incremen tally as 2X41  2x  2ye  2y  At the start position  r  these two terms have the values and 2r respectively  Each successive value is obtained by adding to the previous value of 2x and subtracting from the previous value of 2y  The initial decision parameter is obtained by evaluating the arcle function at the start position xq  g     m xieyr reed  Yr t   rT  My Mt ty  Midpoint between candidate pixels at sampling position x along a circular path  Po fans         f or po   r 30 If the radius r is specified as an integer we can simply round py to Po zl r for r an integer  since all increments are integers  As in Bresenham s line algorithm the midpoint method calculates pixel po sitions along the circumference of a circle using integer additions and subtrac tions assuming that the circle parameters are specified in integer screen coordi nates We can summarize the steps in the midpoint circle algorithm as follows  Midpoint Circle Algorithm  Input radius r and circle center x y  and obtain the first point on the circumference of a circle centered on the origin as  Xo Yo   r  Calculate the initial value of the decision parameter as  PoT gr  At each x position starting at k  perform the following test lf Px  the next point along the circle centered on  is  y and Pri  Pr t dys t1 Otherwise the next point along the circle is x   y   and Pro  Pet Deyo  1D 2Yeos where 2x   2x  and 2y1  2y   Determine symmetry points in the other seven octants  Move each calculated pixel position x y onto the circular path cen  tered on x  y  and plot the coordinate values  FaxtX yrytye  Repeat steps through until x  y   Section  y   Circle Generating Algorithms  8B      _  Selected pixel positions solid  circles along a circle path with radius r  centered on the origin using the midpoint circle algorithm Open circles show the symmetry oi 6 10 positions in the first quadrant   Example Midpoint Circle Drawing Given a circle radius r  we demonstrate the midpoint circle algorithm by determining positions along the circle octant in the first quadrant from x  to x  y The initial value of the decision parameter is Po l r   For the circle centered on the coordinate origin the initial point is x9 yo    and initial increment terms for calculating the decision parameters are 2xy yy  Successive decision parameter values and positions along the circle path are cal culated using the midpoint method as  Ky Xkeate Mert 2Xta Yu   20    20   20  18   18  16  14  A plot of the generated pixel positions in the first quadrant is shown in   The following procedure displays a raster circle on a bilevel monitor using the midpoint algorithm Input to the procedure are the coordinates for the circle center and the radius Intensities for pixel positions along the circle circumfer ence are loaded into the frame buffer array with calls to the set Pixel routine  y Px x yl x E lipse generated about foci F and F   include device h  void circleMidpoint int xCenter int yCenter int radius   int x   int y  radius  int p  radius  void circlePlotPoints int int int int    Plot first set of points  circlePlotPoints xCenter yCenter x y i while x  y   Xtt if p   pts x Ll else  Yori pte  x  y    circlePlotPoints xCenter yCenter x y     void circlePlotPoints int xCenter int yCenter int x int y  setPixel xCenter setPixel xCenter x yCenter  y  setPixel xCenter x yCenter  y   x yoenter  y   setPixel xCenter  x yCenter     ha setPixel xCenter y yOenter  x  setPixel xCenter y yCenter  x  setPixel xCenter y yOenter  x  setPixel xCenter y yCenter  x    Loosely stated an ellipse is an elongated circle Therefore elliptical curves can be generated by modifying circle drawing procedures to take into account the dif ferent dimensions of an ellipse along the major and minor axes  Properties of Ellipses An ellipse is defined as the set of points such that the sum of the distances from two fixed positions foci is the same for all points   Lf the distances to the two foci from any point P  x y on the ellipse are labeled d and d  then the general equation of an ellipse can be stated as d  d  constant 32  Expressing distances d and d in terms of the focal coordinates F   x y  and F   x y2  we have  Vx  x   y  yy  VG    Y ys  constant 39 By squaring this equation isolating the remaining radical and then squaring again we can rewrite the genera ellipse equation in the form Ax  By  Cxy  Dx  Ey  F 34  where the coefficients A B C D E and F are evaluated in terms of the focal coor dinates and the dimensions of the major and minor axes of the ellipse The major axis is the straight line segment extending from one side of the ellipse to the other through the foci The minor axis spans the shorter dimension of the ellipse bisecting the major axis at the halfway position ellipse center between the two foci  An interactive method for specifying an ellipse in an arbitrary orientation is to input the two foci and a point on the ellipse boundary With these three coordi nate positions we can evaluate the constant in Eg 33 Then the coefficients in Eq 34 can be evaluated and used to generate pixels along the elliptical path  Ellipse equations are greatly simplified if the major and minor axes are ori ented to align with the coordinate axes In  we show an ellipse in stan dard position with major and minor axes oriented parallel to the x and y axes Parameter r for this example labels the semimajor axis and parameter r labels the semiminor axis The equation of the ellipse shown in  can be written in terms cf the ellipse center coordinates and parameters r and r as  fu C    doa   35 J WY ry Using polar coordinates r and we can also describe the ellipse in standard posi tion with the parametric equations  y x tr cos  36 Yrytry sind Symmetry considerations can be used to further reduce computations An ellipse in standard position is symmetric between quadrants but unlike a circle it is not symmetric between the two octants of a quadrant Thus we must calculate pixel positions along the elliptical arc throughout one quadrant then we obtain posi tions in the remaining three quadrants by symmetry Fig 19  Midpoint Ellipse A gorithm Our approach here is similar to that used in displaying a raster circle Given pa rameters    and   y  we determine points x  for an ellipse in standard position centered on the origin and then we shift the points so the ellipse is cen tered at x  y  It we wish also to display the ellipse in nonstandard position we could then rotate the ellipse about its center coordinates to reorient the major and minor axes For the present we consider only the display of ellipses in standard position We discuss general methods for transforming object orientations and positions in Chapter  The midpoint ellipse method is applied throughout the first quadrant in two parts  shows the division of the first quadrant according to the slope of an ellipse with r   We process this quadrant by taking unit steps in the v direction where the slope of the curve has a magnitude less than and tak ing unit steps in the y direction where the slope has a magnitude greater than  Regions and   can be processed in various ways We can start at position  r and step clockwise along the elliptical path in the first quadrant  Ellipse centered at   y  with semimajor axis r and semiminor axis    Symmetry or an ellipse Calculation of a point x y in one quadrant yields the ellipse points shown for the other three quadrants  RegaM Slope     r    Ragin     Ellipse processing regions Over region the magnitude of the ellipse slope is less than over region the magnitude of the slope is greater than t  shifting from unit steps in x to unit steps in y when the slope becomes less than  Alternatively we could start at r  and select points in a counterclockwise order shifting from unit steps in y to unit steps in x when the slope becomes greater than  With parallel processors we could calculate pixel positions in the two regions simultaneously As an example of a sequential implementation of the midpoint algorithm we take the start position at  r  and step along the el lipse path in clockwise order throughout the first quadrant We define an ellipse function from Eq 35 with  y    as fattpae  y  12x  By  r2r  37 which has the following properties  if x y is inside the ellipse boundary feripselX    if x y is on the ellipse boundary 38  o if x y is outside the ellipse boundary Thus the ellipse function fetipee X   serves as the decision parameter in the mid point algorithm At each sampling position we select the next pixel along the el lipse path according to the sign of the ellipse function evaluated at the midpoint between the two candidate pixels  Starting at  r  we take unit steps in the x direction until we reach the boundary between region and region   Then we switch to unit steps in the y direction over the remainder of the curve in the first quadrant At each step we need to test the value of the slope of the curve The ellipse slope is calcu lated from Eq 37 as    39 ary dy dx At the boundary between region and region dy dx  and 2rx  2rdy Therefore we move out of region whenever 2rjx  2rdy 40   shows the midpoint between the two candidate pixels at sam pling position x  in the first region Assuming position  y  has been se lected at the previous step we determine the next position along the ellipse path by evaluating the decision parameter that is the ellipse function 37 at this midpoint  an  fp 1y      iy t rly   in if pl   the midpoint is inside the ellipse and the pixel on scan line y is closer to the ellipse boundary Otherwise the midposition is outside or on the ellipse boundary and we select the pixel on scan line y   At the next sampling position     x   the decision parameter rex  62y   for region is evaluated as  ve Te Plier  fps Year  se y midpoint  NK  Gy t ep  rly  s i  Xe Xt    or    Midpoint between candidate ve   u  x  42 pixels at sampling position  x along an elliptical path  Plysy  ple  2p t D ry E  where y is either y or y   depending on the sign of p1  Decision parameters are incremented by the following amounts 2rexeey  n if pl  increment   rex tPF Ye ifp1   As in the circle algorithm increments for ihe decision parameters can be calcu lated using only addition and subtraction since values for the terms x and 2r y can also be obtained incrementally At the initial position  r  the two terms evaluate to arix  43  arzy  2rir  44 As x and y are incremented updated values are obtained by adding 2r to 43 and subtracting 2r from 44 The updated values are compared at each step and we move from region to region when condition 40 is satisfied  In region the initial value of the decision parameter is obtained by evalu ating the ellipse function at the start position xq yo   r   Plo  feng Ty       r2p2 ny  rly  ny or ply r  rir  qn 45  Over region we sample at unit steps in the negative y direction and the midpoint is now taken between horizontal pixels at each step   For this region the decision parameter is evaluated as pay  feu  Yu  46    ry    yy   Chapter Output Primitives ly ie rox sr dy   O  Midpoint between candidate pixels at sampling position y   along an elliptical path   f p2   the midposition is outside the ellipse boundary and we select the pixel at x If p2   the midpoint is inside or on the ellipse boundary and we select pixel position X   To determine the relationship between successive decision parameters in region we evaluate the ellipse function at the next sampling step y    YM  pei  fouped  Yer i 47  ila  BY etl DIP  rie or with x   set either to x or to x  depending on the sign of p2  When we enter region the initial position x y  is taken as the last posi tion selected in region  and the initial decision parameter in region is then j pay  fata  Yo   49  p iV pd p  rilxg  ry  WP  ri ry Te simplify the calculation of p2y we could select pixel positions in counterclock wise order starting at r   Unit steps would then be taken in the positive y di rection up to the last pesition selected in region  The midpoint algorithm can be adapted to generate an ellipse in nonstan dard position using the ellipse function Eq 34 and calculating pixel positions over the entire elliptical path Alternatively we could reorient the ellipse axes to standard position using transformation methods discussed in Chapter apply the midpoint algorithm to determine curve positions then convert calculated pixel positions to path positions along the original ellipse orientation  Assuming r  vy and the ellipse center are given in integer screen coordi nates we only need incremental integer calculations to determine values for the decision parameters in the midpoint ellipse algorithm The increments r  r  2r  and 2rj are evaluated once at the beginning of the procedure A summary of the midpoint ellipse algorithm is listed in the following steps   Midpoint Ellipse Algorithm  Input r  ry and ellipse center x y  and obtain the first point on an ellipse centered on the origin as  Xo We   ry  Calculate the initial value of the decision parameter in region as  At each x position in region starting at k  G perform the follow ing test If pl   the next point along the ellipse centered on  is  y  y and Ply  Ply t Wart ry Otherwise the next point along the circle is     and Ply  plat 2a  Wp  with Wx  Wx  Ay  2y  Bre and continue until 2rgx  Ory  Calculate the initial value of the decision parameter in region using the last point   Yo calculated in region as po ri z0    DF  Br  At each y position in region starting at k  perform the follow ing test If p2  the next point along the ellipse centered on  is tu  and Py   pa  tT Otherwise the next point along the circle is x    and Pave  p2yt Irie  Why br  using the same incremental calculations for and y as in region  Determine symmetry points in the other three quadrants  Move each calculated pixel position x y onto the elliptical path cen tered on x  y  and plot the coordinate values  x x XxX yoy ty  Repeat the steps for region until 2r3x  2r y   Section Flipse Cenerating Algorithms   Example Midpoint Ellipse Drawing Given input ellipse parameters r  and r   we illustrate the steps in the midpoint ellipse algorithm by determining raster positions along the ellipse path in the first quadrant Initial values and increments for the decision parameter cal culations are arjx  with increment 2r7   arty  2riry with increment  2r    For region J The initial point for the ellipse centered on the origin is xp  o    and the initial decision parameter value is ply  ry  r2r  a   Successive decision parameter values and positions along the ellipse path are cal culated using the midpoint method as  k ply UXtete Veer Xa 2B Yk  qd  768 224  768 44  768 208  640   640 288  512 244  384  We now move out of region since 2rfx  2rzy  For region the initial point is xp yo    and the initial decision parameter is py     The remaining positions along the ellipse path in the first quadrant are then cal culated as  k Pr Xeers Yeor Xara 15t  256 233  128 745  _   A plot of the selected positions around the ellipse boundary within the first quadrant is shown in   In the following procedure the midpoint algorithm is used to display an el lipse with input parameters Rx Ry xCenter and yCenter Positions along the Section Ellipse Generating Algorithms   Positions along an elliptical path centered on the origin with r  and r  using the midpoint algorithm to calculate pixel o0t123 67 addresses in the first quadrant  Rw  oO  curve in the first quadrant are generated and then shifted to their proper screen positions Intensities for these positions and the symmetry positions in the other three quadrants are loaded into the frame buffer using the set Pixel routine   include device h  define ROUND a   int  a  void ellipseMidpoint int xCenter int yCenter int Rx int Ry   int Rx2  Rx Rx  int Ry2  Ry Ry  int twoRx2  Rx2  int twoRy2  Ry2  int p  int x  int y  Ry int px   int py  twoRx2  y void ellipsePlotPoints int int int int    Plot the first set of points  ellipsePlotPoints xCenter yCenter x y    Region  P  ROUND Ry2  Rxz  Ry   25  RxZ  while px  py  X  px  twoRy2 if p  P  Ry2  px else  Yyr i py  twoRx2 p  Ry2  px  py  ellipsePlotPaints xCenter yCenter x y     Region  Pp  ROUND Ry2  x  x  Rx2 y  y  Rx2 Ry2  while y    r G py  twoRx2 if p  p t Rx2  py else  K e Px  twoRy2 p  Rx2  py  PX     ell psePlotPoincs xCenter yCenter x y    void ellipsePiotPo nts int xCenter int yCenter int x int y    setPixel xCenter  x yCenter  y   setPixel xCenter  x yCenzer  y  setPixel xCenter  x yCenter  y  setPixel xCenter  x yCenter  y    OTHER CURVES Various curve functions are useful in object modeling animation path specifica tions data and function graphing and other graphics applications Commonly encountered curves include conics trigonometric and exponential functions probability distributions general polynomials and spline functions Displays of these curves can be generated with methods similar to those discussed for the circle and ellipse functions We can obtain positions along curve paths directly from explicit representations y  f x or from parametric forms Alternatively we could apply the incremental midpoint method to plot curves described with im plicit functions f x y    A straightforward method for displaying a specified curve function is to ap proximate it with stra ght line segments Parametric representations are useful in this case for obtaining equally spaced line endpoint positions along the curve path We can also generate equally spaced positions from an explicit representa tion by choosing the independent variable according to the slupe of the curve Where the slope of y  x has a magnitude less than we choose x as the inde pendent variable and calculate y values at equal x increments To obtain equal spacing where the slope has a magnitude greater than we use the inverse func tion x  f  y  and calculate values of x at equal y steps  Straight line or curve approximations are used to graph a data set of dis crete coordinate points We could join the discrete points with straight line seg ments or we could use linear regression least squares to approximate the data set with a single straight line A nonlinear least squares approach is used to dis play the data set with some approximating function usually a polynomial  As with circles and ellipses many functions possess symmetries that can be exploited to reduce the computation of coordinate positions along curve paths For example the normal probability distribution function is symmetric about a center position the mean  and all points along one cycle of a sine curve can be generated from the points in a interval  Conic Sections In general we can describe a conic section or conic with the second degree equation  Ax  By  Cxy  Dx  Ey  F 50 where values for parameters A B C D E and F determine the kind of curve we are to display Given this set of coefficients we can determine the particular conic that will be generated by evaluating the discriminant B  4AC   generates an ellipse or circle B  4AC   generates a parabola 31  generates a hyperbola For example we get the circle equation 24 when A  B  C  D  2x F  2y and F  x  y  r  Equation 50 also describes the degenerate conics points and straight lines  Ellipses hyperbolas and parabolas are particularly useful in certain aninta tion applications These curves describe orbital and other motions for objects subjected to gravitational electromagnetic or nuclear forces Planetary orbits in the solar system for example are ellipses and an object projected into a uniform gravitational field travels along a parabolic trajectory  shows a para bolic path in standard position for a gravitational field acting in the negative y di rection The explicit equation for the parabolic trajectory of the object shown can be written as Y Wy  ax  x9   B x  x  52 with constants  and b determined by the initial velocity vp cf the object and the acceleration  due to the uniform gravitational force We can also describe such parabolic motions with parametric equations using a time parameter  measured in seconds from the initial projection point  X  Xq t Byol Y  Yo  Yyol  af Here v9 and v are the initial velocity components and the value of g near the surface of the earth is approximately 980cm sec  Object positions along the par abolic path are then calculated at selected time steps  Hyperbolic motions   occur in connection with the collision of charged particles and in certain gravitational problems For example comets or meteorites moving around the sun may travel along hyperbolic paths and escape to outer space never to return The particular branch left or right in  describing the motion of an object depends on the forces involved in the prob lem We can write the standard equation for the hyperbola centered on the origin in  as fy xy oo   Le oUF with x  r for the left branch and x r for the right branch Since this equa tion differs from the standard ellipse equation 35 only in the sign between the x and y terms we can generate points along a hyperbolic path with a slightly modified ellipse algorithm We will return to the discussion of animation applica tions and methods in more detail in Chapter And in Chapter we discuss applications of computer graphics in scientific visualization  G  Section Other Curves  Etgnre 24 Parabolic path of an object tossed into a downward gravitational field at the initial position x9 Yo  Left Branch    Left and right branches of a hy perbola in standard position with symmetry axis along the x axis  Chapter Output Primitives   A spline curve formed with individual cubic polynomial sections between specified coordinate points  Parabolas and hyperbolas possess a symmetry axis For example the parabola described by Eq 53 is symmetric about the axis  X  Xq  oPya The methods used in the midpoint ellipse algorithm can be directly applied to obtain points along one side of the symmetry axis of hyperbolic and parabolic paths in the two regions  where the magnitude of the curve slope is less than and  where the magnitude of the slope is greater than To do this we first select the appropriate form of Eq 50 and then use the selected function to set up expressions for the decision parameters in the two regions  Polynomials and Spline Curves A polynomial function of nth degree in x is defined as y  ax ko 55  ay  ax    ay yx  ar  where is a nonnegative integer and the a are constants with a  We get a quadratic when  a cubic polynomial when n  a quartic when n  and so forth And we have a straight line when  Polynomials are useful in a number of graphics applications including the design of object shapes the speci fication of animation paths and the graphing of data trends in a discrete set of data points  Designing object shapes or motion paths is typically done by specifying a few points to define the general curve contour then fitting the selected points with a polynomial One way to accomplish the curve fitting is to construct a cubic polynomial curve section between each pair of specified points Each curve section is then described in parametric form as  ay t Qgu tag  au  56  Y Fay  ayy  ayu  aut 57  where parameter u varies over the interval to Values for the coefficients of u in the parametric equations are determined from boundary conditions on the curve sections One boundary condition is that two adjacent curve sections have the same coordinate position at the boundary and a second condition is to match the two curve slopes at the boundary so that we obtain one continuous smooth curve   Continuous curves that are formed with polynomial pieces are called spline curves or simply splines There are other ways to set up spline curves and the various spline generating methods are explored in Chapter   Methods for exploiting parallelism in curve generation are similar to those used in displaying straight line segments We can either adapt a sequential algorithm by allocating processors according to curve partitions or we could devise other methods and assign processors to screen partitions  A parallel midpoint method for displaying citcles is to divide the circular arc from to into equal subarcs and assign a separate processor to each subarc As in the parallel Bresenham line algorithm we then need to set up com putations to determine the beginning y value and decisicn parameter p value for each processor Pixel positions are then calculated throughout each subarc and positions in the other circle octants are then obtained by symmetry Similarly a parallel ellipse midpoint method divides the elliptical arc over the first quadrant into equal subarcs and parcels these out to separate processors Pixel positions in the other quadrants are determined by symmetry A screen partitioning scheme for circles and ellipses is to assign each scan line crossing the curve to a separate processor In this case each processor uses the circle or ellipse equation to calcu late curve intersection coordinates  For the display of elliptical arcs or other curves we can simply use the scan line partitioning method Each processor uses the curve equation to locate the in tersection positions along its assigned scan line With processors assigned to indi vidual pixels each processor would calculate the distance or distance squared from the curve to its assigned pixel If the calculated distance is less than a prede fined value the pixel is plotted   Routines for circles splines and other commonly used curves are included in many graphics packages The PHIGS standard does not provide explicit func tions for these curves but it does include the following general curve function  generalizedDrawingPrimitive n wcPoints id datalist  where wcPoints isa list of n coordinate positions datalist contains noncoor dinate data values and parameter id selects the desired function At a particular installation a circle might be referenced with id  an ellipse with id  and so on  As an example of the definition of curves through this PHIGS function a circle id  say could be specified by assigning the two center coordinate val ues to wcpoints and assigning the radius value to datalist The generalized drawing primitive would then reference the appropriate algorithm such as the midpoint method to generate the circle With interactive input a circle could be defined with two coordinate points the center position and a point on the cir cumference Similarly interactive specification of an ellipse can be done with three points the two foci and a point on the ellipse boundary all stored in wo points For an ellipse in standard position wcpoints could be assigned only the center coordinates with daialist assigned the values for r and r  Splines defined with control points would be generated by assigning the control point coordi nates towcpoints  Functions to generate circles and ellipses often include the capability of drawing curve sections by specifying parameters for the line endpoints Expand ing the parameter list allows specification of the beginning and ending angular values for an arc as illustrated in  Another method for designating a cir  Section Curve Functions   Circular arc specified by beginning and ending angles Circle center is at the coordinate origin  Chapter  Output Primitives  cular or elliptical arc is to input the beginning and ending coordinate positions of the arc  10         Lower left section of the screen grid referencing integer coordinate positions     Lt     4 Line path for a series of connected line segments between screen grid coordinate positions   INuminated pixel at raster position   PIXEL ADDRESSING AND OBJECT GEOMETRY So far we have assumed that all input positions were given in terms of scan line number and pixel position number across the scan line As we saw in Chapter there are in general several coordinate references associated with the specifica tion and generation of a picture Object descriptions are given in a world reference frame chosen to suit a particular application and input world coordi nates are ultimately converted to screen display positions World descriptions of objects are given in terms of precise coordinate positions which are infinitesi mally small mathematical points Pixel coordinates however reference finite screen areas If we want to preserve the specified geometry of world objects we need to compensate for the mapping of mathematical input points to finite pixel areas One way to do this is simply to adjust the dimensions of displayed objects to account for the amount of overlap of pixel areas with the object boundaries Another approach is to map world coordinates onto screen positions between pixels so that we align object boundaries with pixel boundaries instead of pixel centers  Screen Grid Coordinates An alternative to addressing display positions in terms of pixel centers is to refer ence screen coordinates with respect to the grid of horizontal and vertical pixel boundary lines spaced one unit apart   A screen coordinate position is then the pair of integer values identifying a grid intersection position between two pixels For example the mathematical line path for a polyline with screen endpoints     and  is shown in   With the coordinate origin at the lower left of the screen each pixel area can be referenced by the integer grid coordinates of its lower left corner  illustrates this convention for an by section of a raster with a single illumi nated pixe at screen coordinate position   In geneial we identify the area occupied by a pixel with screen coordinates x y as the unit square with diago nally opposite corners at x y and x  y   This pixel addressing scheme has several advantages It avoids half integer pixel boundaries it facilitates pre cise object representations and it simplifies the processing involved in many scan conversion algorithms and in other raster procedures  The algorithms for line drawing and curve generation discussed in the pre ceding sections are still valid when applied to input positions expressed as screen grid coordinates Decision parameters in these algorithms are now simply a mea sure of screen grid separation differences rather than separation differences from pixel centers  Maintaining Geometric Properties of Displayed Objects When we convert geometric descriptions of objects into pixel representations we transform mathematical points and lines into finite screen areas If we are to maintain the original geometric measurerrents specified by the input coordinates   Line path and corresponding pixel display for input screen grid   endpoint coordinates  and 21 23 25 27 29    for an object we need to account for the finite size of pixels when we transform the object definition to a screen display   shows the line plotted in the Bresenham line algorithm example of Section Interpreting the line endpoints  and  as precise grid crossing positions we see that the line should not extend past screen grid posi tion   If we were to plot the pixel with screen coordinates   as in the example given in Section we would display a line that spans horizontal units and vertical units For the mathematical line however Ax  and dy  If we are addressing pixels by their center positions we can adjust the length of the displayed line by omitting one of the endpoint pixels If we think of screen coordinates as addressing pixel boundaries as shown in  we plot a line using only those pixels that are interior to the line path that is only those pix els that are between the line endpoints For our example we would plot the left most pixel at  and the rightmost pixel at   This displays a line that           Conversion of rectangle a with verti es at screen coordinates       and  into display b that includes the right and top boundaries and into display c that maintains geometric magnitudes  Section 10 Pixel Addressing and Object Geometry Chapter Qutput Primitives  has the same geometric magnitudes as the mathematical line from  to   For an enclosed area inpul geometric properties are maintained by display ing the area only with those pixels that are interior to the object boundaries The rectangle defined with the screen coordinate vertices shown in  a  for example is larger when we display it filled with pixels up to and including the border pixel lines joing the specified vertices As defined the area of the rectangle is units but as displayed in  b  it has an area of units In  c  the original rectangle measurements are maintained by displaying  xe   y       Circle path and midpoint circle algorithm plot of a circle with radius in screen coordinates  YyW x  y x dx  y   x y  ty xb  f y 4x   Modification of the circle plot in  to maintain the specified circle diameter of only the interna pixels The right boundary of the mput rectangle is at x  To maintain this boundary in the display we set the rightmost pixel grid coordinate at x  The pixels in this vertical column then span the interval from x  to x  Similarly the mathematical top boundary of the rectangle is at y  so we set the top pixel row for the displayed rectangle at y   These compensations for finite pixel width along object boundaries can be applied to other polygons and to curved figures so that the raster display main tains the input object specifications A circle of radius and center position   for instance would be displayed as in Fig by the midpoint circle algo rithm using screen grid coordinate positions But the plotted circle has a diameter of To plot the circle with the defined diameter of we can modify the circle algorithm to shorten each pixel scan line and each pixel column as in  One way to do this is to generate points clockwise along the circular arc in the third quadrant starting at screen coordinates   For each generated point the other seven circle symmetry points are generated by decreasing the x coordi nate values by along scan lines and decreasing the y coordinate values by along pixel columns Similar methods are applied in ellipse algorithms to main tain the specified proportions in the display of an ellipse  A standard output primitive in general graphics packages is a solid color or pat terned polygon area Other kinds of area primitives are sometimes available but poly gons are easier to process since they have linear boundaries There are two basic approaches to area filling on raster systems One way to fill an area is to determine the overlap intervals for scan lines that cross the area Another method for area filling is to start from a given interior position and paint outward from this point until we encounter the specified boundary conditions The scan line approach is typically used in general graphics packages to fill poly gons circles ellipses and other simple curves Fill methods starting from an inte rior point are useful with more complex boundaries and in interactive painting systems In the following sections we consider methods for solid fill of specified areas Other fill options are cliscussed in Chapter  Scan Line Polygon Fill Algorithm  illustrates the scan line procedure for sond tilling of polygon areas For each scan line crossing a polygon the area fill algorithm locates the intersec tion points of the scan line with the polygon edges These intersection points are then sorted from left to right and the corresponding frame buffer positions be tween each intersection pair are set to the specified fill color In the example of  the four pixel intersection positions with the polygon boundaries define two stretches of interior pixels from x  to x  l4and from x  tox  Some scan line intersections at polygon vertices require special handling A scan line passing through a vertex intersects two polygon edges at that position adding two points to the list of intersections for the scan line  shows two scan lines at positions y and y that intersect edge endpoints Scan line y in tersects five polygon edges Scan line y  however intersects an even number of edges although it also passes through a vertex Intersection points along scan line Section 11 Filled Area Primitives 1t7 Chapter Output Primitives  Interior pixels along a scan line passing through a polygon area  y correctly identify the interior pixel spans But with scan line y we need to do some additional processing to determine the correct interior points  The topological difference between scan line y and scan line y in  is identified by noting the position of the intersecting edges relative to the scan line For scan line y the two intersecting edges sharing a vertex are on opposite sides of the scan line But for scan line y  the two intersecting edges are both above the scan line Thus the vertices that require additional processing are those that have connecting edges on opposite sides of the scan line We can identify these vertices by tracing around the polygon boundary either in clockwise or counterclockwise order and observing the relative changes in vertex y coordinates as we move from one edge to the next If the endpoint y values of two consecutive edges mo notonically increase or decrease we need to count the middle vertex as a single intersection point for any scan line passing through that vertex Otherwise the shared vertex represents a local extremum minimum or maximum on the polv gon boundary and the two edge intersections with the scan line passing through that vertex can be added to the intersection list  Scan Line y  Sean Line y     Intersection points along scan lines that intersect polygon vertices Scan line y generates an odd number of intersections but scan line y generates an even number of intersections that can be paired to identify correctly the interior pixel spans One way to resolve the question as to whether we should count a vertex as one intersection or two is to shorten some polygon edges to split those vertices that should be counted as one intersection We can process nonhorizontal edges around the polygon boundary in the order specified either clockwise or counter clockwise As we process each edge we can check to determine whether that edge and the next nonhorizontal edge have either monotonically increasing or decreasing endpoint y values If so the lower edge can be shortened to ensure that only one intersection point is generated for the scan line going through the common vertex joining the two edges  illustrates shortening of an edge When the endpoint y coordinates of the two edges are increasing the y value of the upper endpoint for the current edge 1s decreased by as in Fig 37 a  When the endpoint y values are monotonically decreasing as in Fig 37 b  we decrease the y coordinate of the upper endpoint of the edge following the current edge  Calculations performed in scan conversion and other graphics algorithms typically take advantage of various coherence properties of a scene that is to be displayed What we mean by coherence is simply that the properties of one part of a scene are related in some way to other parts of the scene so that the relation ship can be used to reduce processing Coherence methods often involve incre mental calculations applied along a single scan line or between successive scan lines In determining edge intersections we can set up incremental coordinate calculations along any edge by exploiting the fact that the slope of the edge is constant from one scan line to the next  shows two successive scan lines crossing a left edge of a polygon The slope of this polygon boundary line can be expressed in terms of the scan line intersection coordinates    m  et Yi 58 Xper  Xk Since the change in y coordinates between the two scan lines is simply Ves Ye  59 P P  f     Scan Line y  Lt  Scan Line y P Scan Line y       a  b   Adjusting endpoint y values for a pulygon as we process edges in order around the polygon perimeter The edge currently being processed is inuicated as a solid line In a  the y coordinate of the upper endpoint of the current edge is decreased by In b  the y coordinate of the upper endpoint of the next edge is decreased by  Section 11 Filled Area Primitives  Scan Line y   Scan Line y   Two successive scan lines intersecting a polygon boundary  the x intersection value x  on the upper scan line can be determined from the x intersection value x on the preceding sean line as I  Xper  MHF 60 m Each successive x intercept can thus be calculated by adding the inverse of the slope and rounding to the nearest integer  An obvious parallel implementation of the fill algorithm is to assign each scan line crossing the polygon area to a separate processor Edge intersection cal culations are then performed independently Along an edge with slope m the in tersection x value for scan line k above the initial scan line can be calculated as Xp  Xt  67 mn tn a sequential fill algorithm the increment of x values by the amount m along an edge can be accomplished with integer operations by recalling that the slope is the ratio of two integers  Pie  Ke where Ax and Ay are the differences between the edge endpoint x and y coordi nate values Thus incremental calculations of x intercepts along an edge for suc cessive scar lines can be expressed as Ax Xie  MF 62 del k Ay Using this equation we can perform integer evaluation of the x intercepts by ini tializing a counter to then incrementing the counter by the value of Ax each time we move up to a new scan line Whenever the counter value becomes equal to or greater than Ay we increment the current x intersection value by and de crease the counter by the value Ay This procedure is equivalent to maintaining integer and fractional parts for x intercepts and incrementing the fractional part until we reach the next integer value As an example of integer incrementing suppose we have an edge with slope m  At the initial scan line we set the counter to and the counter in Sean Line y  Scan Line yp   o  Scan Line v4   A polygon and its sorted edge table with edge DC shortened by one unit in the y direction  crement to As we move up to the next three scan lines along this edge the counter is successively assigned the values  and On the third scan line above the initial scan line the counter now has a value greater than So we in crement the x intersection coordinate by and reset the counter to the value   We continue determining the scan line intersections in this way until we reach the upper endpoint of the edge Similar calculations are carried out to obtain intersections for edges with negative slopes We can round to the nearest pixel x intersection value instead of truncating to obtain integer positions by modifying the edge intersection algorithm so that the increment is compared to Ay This can be done with integer arithmetic by incrementing the counter with the value 24x at each step and comparing the in crement to Ay When the increment is greater than or equal to Ay we increase the x value by and decrement the counter by the value of 2Ay In our previous ex ample with m  the counter values for the first few scan lines above the ini tial scan line on this edge would now be  reduced to   reduced to   reduced to   and reduced to  Now x would be incre mented on scan lines   etc  abuve the initial scan line for this edge The extra calculations required for each edge are 2Ax  Ax  Ax and 2Ay  Ay  Ay  To efficiently perform a polygon fill we can first store the polygon bound ary in a sorted edge table that contains all the information necessary to process the sean lines efficiently Proceeding around the edges in either a clockwise or a counterclockwise order we can use a bucket surt to stare the edges sorted on the smallest y value of cach edge in the correct scan line positions Only nonhorizon tal edges are entered into the sorted edge table As the edges are processed we can also shorten certain edges to resolve the vertex intersection question Each entry in the table for a particular scan line contains the maximum y value for that edge the x intercept value at the lower vertex for the edge and the inverse slope of the edge For each scan line the edges are in sorted order from left to night  shows a polygon and the assuciated sorted edge table   Crapter Qutpul Primitives Next we process the scan lines from the bottem of the polygon to its top producing an active edge list for each scan line crossing the polygon boundaries The active edge list for a scan line contains all edges crossed by that scan line with iterative coherence calculations used to obtain the edge intersections  Implementation of edge intersection calculations can also be facilitated by storing Ax and Ay values in the sorted edge table Also to ensure that we cor rectly fill the interior of specified polygons we can apply the considerations dis cussed in Section 10 For each scan line we fill in the pixel spans for each pair of x intercepts starting trom the leftmost x intercept value and ending at one po sition before the rightmost  intercept And each polygon edge can be shortened by one unit in the y direction at the top endpoint These measures also guarantee that pixels in adjacent polygons will not overlap each other  The following procedure performs a solid fill scan conversion for an input set of polygon vertices For each scan line within the vertical extents of the poly gon an active edge list is set up and edge intersections are calculated Across each scan line the interior fill is then applied between successive pairs of edge intersections processed from left to right  finclude device h  typedef struct tEdge  int yUpper  float xIntersect dxPerScan  struct tRdge  next Edge   Inserts edge into list in order of increasing xIntersect field   void insertEdge Edce  Edge  p  g  list  Pp  q next   edge xIntersect  p sxIntersect  while p  NULL  p  NULL else  Q P  p  p next    edge next  g next  g next  edge   int  if  k l   ent  j  else jeke i while pts k y  pts j y Lf  j  fent  j  else  For an index return y coordinate of next nonhorizontal line  int yNext int k int cnt dePz  pts  list Edge  edge    jee return pts j y    Store lower y coordinate and inverse slepe for each edge Adjust and store upper y coordinate for edges that are the lower member of a monotonicallw increasing or decreasing pair of edges   void makeEdgeRec if   dcPt lower dcPt upper int yComp Edge  edge Edge  edges   edge dxPerScan  float  upper x  lower x   upper y  lower y  edge xIntersect  lower x if upper y  yComp edge yUpper  upper y  else edge yUpper  upper y insertEdge edges lower y  edge  void buildEdgeList int cnt dePt  pts Edge  edges      Edge  edge dcPt vl v2 int i yPrev  pts ent  y  vl x  pts cnt  x vl y  pts ent  y for i i cnt i   v2  pts ij  if vl y  v2 y    nonhorizontal line  edge  Edge  malloc sizeof Edge  if vl y  v2 y   up going edge  makeEdgeRec vl v2 yNext i cnt pts  edge edges  else  down going edge   InakeEdgeRec v2 vl yPrev edge edges   yPrev  vi y vi  v2  void buildActiveList int scan Edge  active Edge  edges      Edge  p  q  p  edges scan  next while p  q  p next insertEdge active p  P    void fillScan int scan Edge  active  Edge  pl  p2 int i  pl  active next while pl   p2  pl next  for pl xIntersect p2 xIntersect     seuPixel lint i scani   pl  p2 next      void deleteAfter Edge  q  Edge  p  q next  q rext  p next tree p    Delete completed edges Update xIntersect field for others  void updateActiveList int scan Edge  active   Edge  g  active  p  active next   while p  i if scan  p yUpper  p  p mext deleteAtrer q    else  p xTntersect  p xintersect  p dxPer3can q 3p p  p next  a void rescrtActivelist Edge  active  Edge  g  p  active next  active next  NULL while p  q  popnext insertEdge active p  P    void seanFill int cnt dePr  pts   Edge  edges WINDOW_HEIGHT   active int i scan  for i i WINCOW_HEIGHT i   edges i   Edge  malloc sizeof Edge  edges ij next  NULL buildEdgeList cnt pts edges  active  Edge  malloc sizeof tEdge   active next  NULL  for scan scan WINDOW_HEIGHT scan   buildActiveList scan active edges  if active next  fillScan scan active  updateActiveList scan active  vesortAcctivebist active      Free edge records that have been malloc ed      Inside Outside Tests Area filling algorithms and other graphics processes often need to identify inte rior regions of objects So far we have discussed area filling only in terms of stan dard polygon shapes In elementary geometry a polygon is usually defined as having no self intersections Examples of standard polygons include triangles rectangles octagons and decagons The component edges of these objects are joined only at the vertices and otherwise the edges have no common points in the plane Identifying the interior regions of standard polygons is generally a straightforward process But in most graphics applications we can specify any sequence for the vertices of a fill area including sequences that produce intersect ing edges as in  For such shapes it is not always clear which regions of the xy plane we should call interior and which regions we should designate as exterior to the object Graphics packages normally use either the odd even rule or the nonzero winding number rule to identify interior regions of an object  We apply the odd even rule also called the odd parity rule or the even odd rule by conceptually drawing a line from any position P to a distant point outside the coordinate extents of the object and counting the number of edge crossings along the line If the number of polygon edges crossed by this line is odd then P is an interior point Otherwise P is an exterior point To obtain an ac curate edge count we must be sure that the line path we choose does not inter sect any polygon vertices  a shows the interior and exterior regions obtained from the odd even rule for a self intersecting set of edges The scan line polygon fill algorithm discussed in the previous section is an example of area fill ing using the odd even rule  Another method for defining interior regions is the nonzero winding num ber rule which counts the number of times the polygon edges wind around a particular point in the counterclockwise direction This count is called the wind ing number and the interior points of a two dimensional object are defined to be A  a extenor De extenor  a j  tC  ee     eG interior  menor  Ve  E E  ay a F  Odd Even rule Nonzero Winding Number Rule  a  b   Identifying interior and exterior regions for a self intersecting polygon  Chapter Output Primitives those that have a nonzero value for the winding number We apply the nonzero winding number rule to polygons by initializing the winding number tu C and again imagining a line drawn from any position P to a distant point bu yor the coordinate extents of the object The line we choose must not pass through any vertices As we move along the line from position P to the distant point we count the number of edges that cross the line in each direction We add  to the winding number every time we intersect a polygon edge that crosses the line from right to left and we subtract every time we intersect an edge that crosses from left to right The final value of the winding number after all edge crossings have been counted determines the relative position of P If the winding number is nonzero P is defined to be an interior point Otherwise P is taken to be an exterior point  b shows the interior and exterior regions defined by the nonzero winding number rule for a self intersecting set of edges For standard palygons and other simple shapes the nonzero winding number nule and the odd even tule give the same results But for more complicated shapes the two methods may yield different interior and exterior regions as in the example of   One way to determine directional edge crossings is to take the vector cross product of a vector u along the line from P to a distant point with the edge vector E for each edge that crosses the line If the z component of the cross product u X E fora particular edge is positive that edge crosses from right to left and we add to the winding number Otherwise the edge crosses from left to right and we subtract from the winding number An edge vector is calculated by sub tracting the starting vertex position for that edge from the ending vertex position For example the edge vector for the first edge in the example of  is Ess  Vg  Va where V and V5 represent the point vectors for vertices A and B A somewhat simpler way to compute directional edge crossings is to use vector dot products instead of cross products To do this we set up a vector that is perpendicular to u and that points from right to left as we look along the line from P in the direction of u If the components of u are u  u  then this perpendicular to u has compo nents  u  u  Appendix A  Now if the dot product of the perpendicular and an edge vector is positive that edge crosses the line from right to left and we add to the winding number Otherwise the edge crosses the line from left to right and we subtract from he winding number  Some graphics packages use the nonzero winding number rule to imple ment area filling since it is more versatile than the odd even rule In general ob jects can be defined with multiple unconnected sets of vertices or disjoint sets of closed curves and the direction specified for each set can be used to define the interior regions of objects Examples include characters such as letters of the al phabet and punctuation symbols nested polygons and concentric circles or el lipses For curved lines the odd even rule is applied by determining intersec tions with the curve path instead of finding edge intersections Similarly with the nonzero winding number rule we need to calculate tangent vectors to the curves at the crossover intersection points with the line from position P  Sean Line Fill of Curved Boundary Areas In general scan line fill of regions with curved boundaries requires more work than polygon filling since intersection calculations now involve nonlinear boundaries For simple curves such as circles or ellipses performing a scan line fill is a straightforward process We only need to calculate the two scan line inter sections un opposite sides of the curve This is the same as generating pixel posi tions along the curve boundary and we can do that with the midpoint method Then we simply fill in the horizontal pixel spans between the boundary points on opposite sices of the curve Symmetries between quadrants and between octants for circles are used to reduce the boundary calculations  Similar methods can be used to generate a fill area for a curve section An elliptical arc for example can be filled as in  The interior region is bounded by the ellipse section and a straight line segment that closes the curve by joining the beginning and ending positions of the arc Symmetries and incre mental calculations are exploited whenever possible to reduce computations  Boundary Fill Algorithm Another approach to area filling is to start at a point inside a region and paint the interior outward toward the boundary If the boundary is specified in a single color the fill algorithm proceeds outward pixel by pixel until the boundary color is encountered This method called the boundary fill algorithm is particularly useful in interactive painting packages where interior points are easily selected Using a graphics tablet or other interactive device an artist or designer can sketch a figure outline select a fill color or pattern from a color menu and pick an interior point The system then paints the figure interior To display a solid color region with no border  the designer can choose the fill color to be the same as the boundary color  A boundary fill procedure accepts as input the coordinates of an interior point x y  a fill color and a boundary color Starting from x y  the procedure tests neighboring positions to determine whether they are of the boundary color If not they are painted with the fill color and their neighbors are tested This process continues until all pixels up to the boundary color for the area have been tested Both inner and outer boundaries can be set up to specify an area and some examples of defining regions for boundary fill are shown in    shows two methods for proceeding to neighboring pixels from the current test position In  a  four neighboring points are tested These are the pixel positions that are right left above and below the current pixel Areas filled by this method are called connected The second method shown in  b  is used to fill more complex figures Here the set of neighboring posi tions to be tested includes the four diagonal pixels Fill methods using this ap proach are called connected An connected boundary fill algorithm would correctly fill the interior of the area defined in  but a connected bound ary fill algorithm produces the partial fill shown    Example color boundaries for a boundary fill procedure   Interior fill of an elliptical arc   Fill methods applied toa connected area a and to an connected area b  Open circles represent pixels to be tested from the current test position shown as a solid color  The following procedure illustrates a recursive method ror filling a connected area with an intensity specified in parameter fill up to a boundary color specified with parameter boundary We can extend this procedure to fill an connected region by including four additional statements to test diagonal positions such as x  y    void boundaryFill4 int x int y int fill int boundary   int current  current  getPixel x y  if  current  boundary   current  fill   setColor fill  setPixel x y  boundaryFill4 x l y fill boundary  boundaryFill4 x y fill boundary  boundaryFilld x y l fill boundary  boundaryFill4 x y fill boundary      __ Recursive boundary fill algorithms may not fill regions correctly if some in terior pixels are already displayed in the fill color This occurs because the algo rithm checks next pixels both for boundary color and for fill color Encountering a pixel with the fill color can cause a recursive branch to terminate leaving other interior pixels unfilled To avoid this we can first change the color of any interior pixels that are initially set to the fill color before applying the boundary fill pro cedure  Also since this procedure requires considerable stacking of neighboring points more efficient methods are generally employed These methods fill hori zontal pixel spans across scan lines instead of proceeding to connected or connected neighboring points Then we need only stack a beginning position for each horizontal pixel span instead of stacking all unprocessed neighboring positions around the current position Starting from the initial interior point with this method we first fill in the contiguous span of pixels on this starting scan line Then we locate and stack starting positions for spans on the adjacent scan lines where spans are defined as the contiguous horizontal string of positions  Start Pasition al bi   The area defined within the color boundary a is only partially filled in b using a connected boundary fill algorithm  a  b  c  d  Filled Pixel Spans Stacked Positions  S668       e00ec0e e    e0e  e O86 eeee       2OSOGH FEOHSCO 2eO e S06      S635    ay  ee   8666     SSUSOSsesose ari  we  om    Oe    SBCGS6CCC008  ge q      a        SSeeeeeesoece   Boundary fill across pixel spans for a connected area  a The filled initial pixel span showing the position of the initial point open circle and the stacked positions for pixel spans on adjacent scan lines  b Filled pixel span on the first scan line above the initial scan line and the current contents of the stack  c Filled pixel spans on the first two scan lines above the initial scan line and the current contents of the stack  ad Completed pixel spans for the upper right portion of the defined region and the remaining stacked positions to be processed   An area defined within multiple color boundaries  bounded by pixels displayed in the area border color At each subsequent step we unstack the next start position and repeat the process  An example of how pixel spans could be filled using this approach is illus trated for the connected fill region in  In this example we first process scan lines successively from the start line to the top boundary After all upper scan lines are processed we fill in the pixel spans on the remaining scan lines in order down to the bottom boundary The leftmost pixel position for each hori zontal span is located and stacked in left to right order across successive scan lines as shown in  In a of this figure the initial span has been filled and starting positions and for spans on the next scan lines below and above are stacked In  b  position has been unstacked and processed to pro duce the filled span shown and the starting pixel position for the single span on the next scan line has been stacked After position is processed the filled spans and stacked positions are as shown in  c  And  d shows the filled pixels after processing all spans in the upper right of the specified area Position is next processed and spans are filled in the upper left of the region then position is picked up to continue the processing for the lower scan lines  Flood Fill Algorithm Sometimes we want to fill in or recolor an area that is not defined within a sin gle color boundary  shows an area bordered by several different color regions We can paint such areas by replacing a specified interior color instead of searching for a boundary color valuc This approach is called a flood fill algo rithm We start from a specified interior point x y and reassign all pixel values that are currently set to a given interior color with the desired fill color If the area we want to paint has more than one interior color we can first reassign pixel val ues so that all interior points have the same color Using either a connected or connected approach we then step through pixel positions until all interior points have been repainted The following procedure flood fills a connected re gion recursively starting from the input position  void floodFill4 int x int y int fillcoler int oldcolor  if getPixel x y   oldColor   setColor fillColor  setPixel x y  floodFill4 x y fillColor oldColor  floodFill4d x y fillColor oldcolor  floodFill4d x y l1 fillColor oldColor  floodFill4 x y fillColor oldColor   We can modify procedure f loodFil14 to reduce the storage requirements of the stack by filling horizontal pixel spans as discussed for the boundary fill al gorithm In this approach we stack only the beginning positions for those pixel spans having the value oldColor  The steps in this modified flood fill algo rithm are similar to those illustrated in  for a boundary fill Starting at the first position of each span the pixel values are replaced until a value other than oldColor is encountered  We display a filled polygon in PHIGS and GKS with the function fillArea n weVertices  The displayed polygon area is bounded by a series of n straight line segments connecting the set of vertex positions specified in wcvertices These packages do not provide fill functions for objects with curved boundaries  Implementation of the 111Area function depends on the selected type of interior fill We can display the polygon boundary surrounding a hollow interior or we can choose a solid color or pattern fill with no border for the display of the polygon For solid fill the i11Area function is implemented with the scan line fill algorithm to display a single color area The various attribute options for dis playing polygon fill areas in PHIGS are discussed in the next chapter  Another polygon primitive available in PHIGS is 111AreaSet This func tion allows a series of polygons to be displayed by specifying the list of vertices for each polygon Also in other graphics packages functions are often provided for displaying a variety of commonly used fill areas besides general polygons Some examples are fillkectangle fillCire e fillCirclearc fill Ellipse and fill llipsearc  The cell array is a primitive that allows users to display an arbitrary shape de fined as a two dimensional grid pattern A predefined matrix of color values is mapped by this function onto a specified rectangular coordinate region The PHIGS version of this function is cellArray wwePoints n m colorArray where colorArray is the n by m matrix of integer color values and wePoints lists the limits of the rectangular coordinate region  min Yaun ANd Ona Ymar   shows the distribution of the elements of the color matrix over the co ordinate rectangle Each coordinate cell in  has width max  Xgun n and height Umax  Yeun m Pixel color values are assigned according to the relative positions of the pixel center coordinates If the center of a pixel lies within one of the n by m coordinate cells that pixel is assigned the color of the corresponding element in the matrix colorArray  Letters numbers and other characters can be displaved in a variety of sizes and stvles The overall design style for a set or family of characters is called a type  Section 12 Fill Area Functions    y  View  a rows Ymn m columns  Xemun ine x  Mapping an n by m cell array into a rectangular coordinate region  face Today there are hundreds of typefaces available for computer applications Examples of a few common typefaces are Courier Helvetica New York Palatino and Zapf Chancery Originally the term font referred to a set of cast metal char acter forms in a particular size and format such as point Courier Italic or point Palatino Bold Now the terms font and typeface are often used inter changeably since printing is no longer done with cast metal forms  Typefaces or fonts can be divided into two broad groups serif and sans serif Serif type has small lines or accents at the ends of the main character strokes while sans serif type does not have accents For example the text in this book is set in a serif font Palatino  But this sentence is printed in a sans serif font Optima  Serif type is generally more readable that is it is easier to read in longer blocks of text On the other hand the individual characters in sans serif type are easier to recognize For this reason sans serif type is said to be more legible Since sans serif characters can be quickly recognized this typeface is good for labeling and short headings  Two different representations are used for storing computer fonts A simple method for representing the character shapes in a particular typeface is to use rectangular grid patterns The set of characters are then referred to as a bitmap font or bitmapped font  Another more flexible scheme is to describe character shapes using straight line and curve sections as in PostScript for example In this case the set of characters is called an outline font  illustrates the two methods for character representation When the pattern in  a is copied to an area of the frame buffer the bits designate which pixel positions are to be displayed on the monitor To display the character shape in  b  the interior of the character outline must be filled using the scan line fill proce dure Section 11  Bitmap fonts are the simplest to define and display The character grid only needs to be mapped to a frame buffer position In general however bitmap fonts require more space because each variation size and format must be stored in a font cache It is possible to generate different sizes and other variations such as bold and italic from one set but this usually does not produce good results  In contrast to bitmap fonts outline fonts require less storage since each vari ation does not require a distinct font cache We can produce boldface italic or different sizes by manipulating the curve definitions for the character outlines But it does take more time to process the outline fonts because they must be scan converted into the frame buffer  A character string is displayed in PHIGS with the following function  text woePoint string  Parameter string is assigned a character sequence which is then displayed at coordinate position wcPoint  x y  For example the statement text wcPoint  Population Distribution   along with the coordinate specification for wcPoint could be used as a label on a distribution graph  Just how the string is positioned relative to coordinates x y is a user op tion The default is that x y sets the coordinate location for the lower left corner of the first character of the horizontal string to be displayed Other string orienta tions such as vertical horizontal or slanting are set as attribute options and will be discussed in the next chapter  Another convenient character function in PHIGS is one that places a desig nated character called a marker symbol at one or more selected positions This function is defined with the same parameter list as in the line function  polymarker n wePoints  A predefined character is then centered at each of the n coordinate positions in the list wcPoints The default symbol displayed by polymarker depends on the      TP VT Tyr rpayoa Oy  TyoOy a 1tija OFT 1TPO O  T4110 OQ1rryyyrpryrpojo O7TFrPOy o tyraAlse Oo  t oyoj rss a Ti Ty yPr1p aqa O o o oysalo ola       a  b    The letter B represented in a with an by bilevel bitmap pattem and in b with an outline shape defined with straight line and curve segments  Section 14 Character Generation     x Sequence of data values plotted oF 100 with the pol ymarker function  particular implementation but we assume for now that an asterisk is to be used  illustrates plotting of a data set with the statement polymarker  wcPoints  SUMMARY The output primitives discussed in this chapter provide the basic tools for con structing pictures with straight lines curves filled areas cell array patterns and text Examples of pictures generated with these primitives are given in Figs 50 and 51  Three methods that can be used to plot pixel positions along a straight line path are the DDA algorithm Bresenham s algorithm and the midpoint method For straight lines Bresenham s algorithm and the midpoint method are identical and are the most efficient Frame buffer access in these methods can also be per formed efficiently by incrementally calculating memory addresses Any of the line generating algorithms can be adapted to a parallel implementation by parti tioning line segments  Circles and ellipses can be efficiently and accurately scan converted using midpoint methods and taking curve symmetry into account Other conic sec tions parabolas and hyperbolas can be plotted with similar methods Spline curves which are piecewise continuous polynomials are widely used in design applications Parallel implementation of curve generation can be accomplished by partitioning the curve paths  To account for the fact that displayed lines and curves have finite widths we must adjust the pixel dimensions of objects to coincide to the specified geo metric dimensions This can be done with an addressing scheme that references pixel positions at their lower left corner or by adjusting line lengths  Filled area primitives in many graphics packages refer to filled polygons A common method for providing polygon fill on raster systems is the scan line fill algorithm which determines interior pixel spans across scan lines that intersect the polygon The scan line algorithm can also be used to fill the interior of objects with curved boundaries Two other methods for filling the interior regions of ob jects are the boundary fill algorithm and the flood fill algorithm These two fill procedures paint the interior one pixel at a time outward from a specified inte rior point  The scan line fill algorithm is an example of fillirg object interiors using the odd even rule ta locate the interior regions Other methods for defining object in teriors are also useful particularly with unusual self intersecting objects A com mon example is the nonzero winding number rule This rule is more flexible than the odd even rule for handling objects defined with multiple boundaries    A data plot generated with straight line segments a curve circles or markers  and text  Couftesy of  Wolfram Research Inc  The Maker of An electrical diagram drawn  Mathematica  with straight line sections  circles filled rectangles and text  Courtesy of Wolfram Research Inc  The Maker of Mathematica  Additional primitives available in graphics packages include cell arrays character strings and marker symbols Cell arrays are used to define and store color patterns Character strings are used to provide picture and graph labeling And marker symbols are useful for plotting the position of data points  Table lists implementations for some of the output primitives discussed in this chapter  TABLE OUTPUT PRIMITIVE IMPLEMENTATIONS  typedef struct  float x y  wePt2 Defines a location in dimensionai world coordinates  pPolyline int n wePt2  pts Draw a connected sequence of n line segments specified in pts  pCircle wePt2 center float r Draw a circle of radius x at center  pFillarea int n wcPt2  pts Draw a filled polygon with n vertices specified in pts  pCellArray wePt2  pts int n int m  int colors Map an n by m array of colors onto a rectangular area defined by pts  pText wePt2 position char  txt Draw the character string txt at position  pPolymarker int n wePt2  pts Draw a collection of n marker symbols at pts  Chapter Applications  Output Primitives Here we present a few example programs illustrating applications of output primitives Functions listed in Table are defined in the header file graph ics h along with the routines openGraphics closeGraphics setColor and setBackground  The first program produces a line graph for monthly data over a period of one year Output of this procedure is drawn in  This data set is also used by the second program to produce the bar graph in     include stdio h  include graphics h  define WINDOW_WIDTH  define WINDOW_HEIGHT   Amount of space to leave on each side of the chart   define MARGIN_WIDTH 05  WINDOW_WIDTH  define N_DATA typedef enum  Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec  Months  char  monthNames N_DATA    Jan  Feb  Mar  Apr  May  Jun  Jul  Bug  Sep  Oct  Now  Dec   int readData char  inPile float  data   int fileError  FALSE  FILE  fp  Months month  if  fp  fopen inFile  r    NULL fileError  TRUE else  for month  Jan month  Dec month  fscanf fp  tf  dataf month  fclose fp    return  fileError    void lineChart float  data  wePt2 dataPos N_DATA  labelPos Months m float mWidth  WINDOW_WIDTH   MARGIN_WIDTH  N_DATA int charcBottom   WINDOW_HEIGHT int offset  05  WINDOW_HEIGHT   Space between data and labels  int labelLength    Assuming fixed width pixel characters   labelPos y  chartBettom  for m  Jan m  Dec m     Calculate x and y positions for data markers  dataPos m x  MARGIN_WIDTH  m  mWidth   mWidth dataPos m y  chartBottom  offset  data m   Shift the label to the left by one half its length  labelPos x  dataPos m x   labelLength pText lahelPos monthNames m    pPolyline N_DATA dataPos  pPolymarker N_DATA dataPos    Summary Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dac  A line plot of data points output by the lineChart procedure   void main int argc char  argv   t float data N_DATA  int dataError  FALSE long windeowID  if arge   fprintf stderr  Usage  dataFileName n  argv  exit     dataError  readData argv  data  if dataError  fprintf stderr  s error Can t read file s n  argv  exit     windewID  openGraphics  argv WINDOW_WIDTH WINDOW_HEIGHT   setEBackground WHITE  setColor BLACK  lineChart data  sleep   closeGraphics windowID   void barChart float  data   wePt2 dataPos  labelPos Months m  float x mWidth  WINDOW_WIDTH   MARGIN_WIDTH  N_DATA  int chartBottom   WINDOW _HEIGHT  int offset  05  WINDOW_HEIGHT   Space between data and labels   int labelLength    Assuming fixed width B pixel characters    labelPos y  chartBottom  for m  Jan m  Dec m      Find the center of this month s bar    MARGIN_WIDTH  m  mWidth   mwWidth   Shift the label to the left by one half its assumed length  labelPos x  x   labelLength Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec  A bar chart plot output by the barChart procedure  pText labelPos monthNames m   Get the coordinates for this month s bar   dataFos 0i x  dataPos x  x   labelLength dataFos l x  dataPos x  x   laseiLength dataFos Ql y  dataPos  y  chartBottom  offset dataFos y  dataPos  chartBottom  offset  datal m  prillArea  dataPos    KK KM  Pie charts are used to show the percentage contribution of individual parts to the whole The next procedure constructs a pie chart with the number and rel ative size of the slices determined by input A sample output from this procedure appears in    Rdefine TWO_PT 28 void pieChart Moat  data  wePt2 pts  center float radius  WINDOW_HETGHT  float newSlice total  lastSlice  Months month  center x  WINDOW_WIDTH   center y  WINDOW_HEIGHT  pCircle center radius  for month  Jan month  Dec month  total  datafmonth  pts ii x  center x pts y  center y  for month  Jan month  Dec montht   newSlice  TWOLPL  datal month  total  jlastSlice ptsi l x  center x  radius  cosf newSlice  pts l y  center y  radius  sinf newSlice  pPolyline  pts  lastSlice  newSlice   Lo  Some variations on the circle equations are output by this next procedure The shapes shown in  are generated by varying the radius r of a circle Depending on how we vary r we can produce a spiral cardioid limacon or other similar figure  include stdio h  include math a  include graphics h  define TWO_PI 28  Limacon equacion is r  a  ecs theta  b Cardioid is the same with a  b sor a   cos theta  a typedet enum  spiral cardioid threeLeat tourLeaf limacon  Fig void drawCurlyFig Fig figure wcPt2 pos int  4p  float r theta  dtheta   float pl0l int nPoints  int ceilf TWO_PI  p  i wePt2  pe if  pt  iwept2  malloc nPeints  sizeof wePt2   NULL  fprintf stderr  Couldn t ailocate pcints n i recur   Set first point for figure  pt O y  pos y switch figure  ease spiral prl O x  pos x break case limacon pt O x  pos x  plO  E 1l  break case Cardicid prlO x  pos x  pio   break case threeLeaf pt  x  pos x  pl0  break case fourLeaf pt 0j x  pos x  pl0  break  nPoints  i while theta  TWO_PI  switch figure  case spival r p  theta break case limacon r p  cosf theta  pil  break case cardioid xr  p    cosf treta  break case threeLeaf r  p  cosf  theta  break case fourLeaf xr  p  cosf  theta  break   pt nPoints x  pos x  r  cosf thetui pt nPoints y  pos y  r  sinf theta nPointst  theta  dtheta  pPolyline nkoints pt  free pt   void main int arge char  argv    Output generated from the pieChart procedure  OOP   Curved figures produced with the drawShape procedure  long windowID  openGraphics  argv   Fig   Center positions for each figure   wePt2 center            Parameters to define each figure First four need one parameter Fifth figure lLimacon needs two   int p      setBackground WHITE  setColor BLACK  for f spiral f limacon        f   drawCurlyFig f center f  plf   Sleep  closeGraphics windowID   REFERENCES Information on Bresenham s algorithms can be found in Bresenham   For mid point methods see Kappel  Parallel methods for generating lines and circles are discussed in Pang  and in Wright   Additional programming examples and information on PHIGS primitives can be found in Howard et al   Hopgood and Duce  Gaskins  and Blake  For information on GKS output primitive functions see Hopgood et al  and Enderle Kansy and Pfaff   EXERCISES   Implement the poly ine function using the DDA algorithm given any number n of input points A single point is to be plotted when n   Extend Bresenham s line algorithm to generate lines with any slope taking symmetry between quadrants into account Implement the poly ine function using this algorithm as a routine that displays the set of straight lines connecting the n input points For n  the routine displays a single point       10  11  14 15  16  17  18  19  Devise a consistent scheme for implementing the polyline function for any set of input line endpoints using a modified Bresenham line algorithm so that geometric magnitudes are maintained Section 10  Use the midpoint method to derive decision parameters for generating points along a straight line path with slope in the range  m  Show that the midpoint decision parameters are the same as those in the Bresenham Iine algorithm  Use the midpoint method to derive decision parameters that can be used to generate straight line segments with any slope  Set up a parallel version of Bresenham s line algorithm for slopes in the range  m  Set up a parallel version of Bresenham s algorithm for straight lines of any slope Suppose you have a system with an inch by inch video monitor that can display pixels per inch If memory is organized in one byte words the starting frame buffer address is and each pixel is assigned one byte of storage what is the frame buffer address of the pixel with screen coordinates  yl  Suppose you have a system with an inch by inch video monitor that can display pixels per inch If memory is organized in one byte words the starting frame buffer address is and each pixel is assigned bits of storage what is the frame buffer address ior addresses of the pixel with screen coordinates x y  implement the set Pixel routine in Bresenham s ine algorithm using iterative tech niques for calculating frame buffer addresses Section  Revise the midpoint circle algorithm to display sv that geometric magnitudes are maintained Section 10  Set up a procedure for a parallel implementation of the midpoint circle algorithm  Derive decision parameters for the midpoint ellipse algorithm assuming the start posi tion is r  and points are to be generated along the curve path in counterclockwise order  Set up a procedure for a parallel implementation of the midpoint ellipse algorithm  Devise an efficient algorithm that takes advantage of symmetry properties to display a sine function  Devise an efficient algorithm taking function symmetry into account to display a plot of damped harmonic motion  y Ae sin wx  where w is the angular frequency and is the phase of the sine function Plot y as a function of x for several cycles of the sine function or until the maximum amplitude is reduced to A  Using the midpoint method and taking symmetry into account develop an efficient algorithm for scan conversion of the following curve over the interval  x   i yeax 12 Use the midpoint method and symmetry considerations to scan convert the parabola y  x  over the interval  x  Use the midpoint method and symmetry considerations to scan convert the parabola xe y for the interval  y  Exercises   20  21  22 26 27  28 29  30 31  34 Set up a midpoint algoritiim taking symmetry considerations into account to scan convert any paraboia of the form y axt b with input values for parameters a b and the range of x Write a program to scan convert the interior of a specified ellipse into a solid color Devise an algorithm for determining interior regions for any input set of vertices using the nonzero winding rumber rule and cross product calculations to identify the direc tion of edge crossings  Devise an algorithm for determining interior regions for any input set of vertices using the nonzero winding number rule and dot product calculatians to identify the direc tion of edge crossings   Write a procedure for filling the interior of any specificd set of polygon vertices using the nonzero winding number rule to identify interior regions   Modify the boundary fill algorithm for a connected region to avoid excessive stack  ing by incorporating scan line methods Write a boundary fill procedure to fill an connected region Explain how an ellipse displayed with the midpoint method could be properly filled with a boundary fill algorithm  Develop and implement a flood fill algorithm to fill the interior of any specified area Write a routine to implement the text function Write a routine to implement the polymarker function Write a program to display a bar graph using the polyline function Input to the program is to include he data points and the labeling required for the x and y axes The data points are to be scaled by the program so that the graph is displayed across the full screen area   Write a program to display a bar graph in any selected screen area Use the poly  line function to draw the bars  Write a grocedure to display a line graph for any input set of data points in any se lected area of the screen with the input data set scaled to fit the selected screen area Data points are to be displayed as asterisks joined with straight line segments and the x and y axes are to be labeled according te input specifications  Instead of asterisks small circles or some other symbols could be used to plot the data points  Using a circle function write a routine to display a pie chart with appropriate label ing Input to the routine is to include a data set giving the distribution of the data over some set of intervals the name of the pie chart and the names of the intervals Each section labet is to be displayed outside the boundary of the pie chart near the corre sponding pie section HAPTER      Attributes of Output Primitives     I n general any parameter that affects the way a primitive is to be displayed is referred to as an attribute parameter Some attribute parameters such as color and size determine the fundamental characteristics of a primitive Others specify how the primitive is to be displayed under special conditions Examples of attributes in this class include depth information for three dimensional view ing and visibility or detectability options for interactive object selection pro grams These special condition attributes will be considered in later chapters Here we consider only those attributes that control the basic display propertics of primitives without regard for special situations For example lines can be dot ted or dashed fat or thin and blue or orange Areas might be filled with one color or with a multicolor pattern Text can appear reading from left to right slanted diagonally across the screen or in vertical columns Individual characters can be displayed in different fonts colors and sizes And we can apply intensity variations at the edges of objects to smooth out the raster stairstep effect  One way to incorporate attribute options into a graphics package is to ex tend the parameter list associated with each output primitive function to include the appropriate attributes A line drawing function for example could contain parameters to set color width and other properties in addition to endpoint coor dinates Another approach is to maintain a system list of current attribute values Separate functions are then included in the graphics package for setting the cur rent values in the attribute list To generate an output primitive the system checks the relevant attributes and invokes the display routine for that primitive using the current attribute settings Some packages provide users with a combi nation of attribute functions and attribute parameters in the output primitive commands With the GKS and PHIGS standards attribute settings are accom plished with separate functions that update a system attribute list  Basic attributes of a straight line segment are its type its width and its color In some graphics packages lines can also be displayed using selected pen or brush options In the following sections we consider how line drawing routines can be modified to accommodate various attribute specifications Line Type Possible selections for the line type attribute include solid lines dashed lines and dotted lines We modify a line drawing algorithm to generate such lines by setting the length and spacing of displayed solid sections along the line path A dashed line could be displayed by generating an interdash spacing that is equal to the length of the solid sections Both the length of the dashes and the interdash spacing are often specified as user options A dotted line can be displayed by generating very short dashes with the spacing equal to or greater than the dash Section size Similar methods are used to produce other line type variations Line Attributes To set line type attributes in a PHIGS application program a user invokes the function setLinetype 1t where parameter is assigned a positive integer value of  or to generate lines that are respectively solid dashed dotted or dash dotted Other values for the line type parameter 1t could be used to display variations in the dot dash patterns Once the line type parameter has been set in a PHIGS application pro gram al subsequent line drawing commands produce lines with this line type The following program segment illustrates use of the linetype command to display the data plots in    include stdio h  include graphics h  define MARGIN_WIDTH 05  WINDOW_WIDTH int readData char  inFile float  data if int fileBrror  FALSE  FILE  fp  int month  if  fp  fopen inFile  r    NULL fileError  TRUE else  for month month month t  fscanf fp  f  data month  fclose fp   return fileError   void chartData float  data pLineType lineType   wePt2 pts i2  float monthWidth  WINDOW_WIDTH   MARGIN_WIDTH   int i  for  i i   pts i x  MARGIN_WIDTH  i  monthWidth   monthWidth pts i y  data ij t pSetLineType lineType  pPolyline  pts   int main int argc char  argv   long windowID  openGraphics  argv WINDOW_WIDTH WINDOW_HEIGHT  float data  setBackground WHITE   setcColor BLUE  readData   data datal960  data  chartData data SOLID  readData   data datal970  data  chartData data DASHED  readData   data datal980  data  chartData data DOTTED  sleep   closeGraphics windowID   a  ees80e b   Unequal length dashes displayed with the same number of pixels    Plotting three data sets with three differen line types as output by the chart Data procedure  Raster line algorithms display line type attributes by plotting pixel spans For the various dashed dotted and dot dashed patterns the line drawing proce dure outputs sections of cuntiguous pixels along the line path skipping over a number of intervening pixels between the solid spans Pixel counts for the span length and interspan spacing can be specified in a pixel mask which is a string containing the digits  and to indicate which positions to plot along the line path The mask for instance could be used to display a dashed line with a dash length of four pixels and an interdash spacing uf three pixels On a bilevel system the mask gives the bit values that should be loaded into the frame buffer along the line path to display the selected line type  Plotting dashes with a fixed number of pixels results in unequal length dashes for different line orientations as illustrated ir  Both dashes shown are plotted with four pixels but the diagonal dash 1s longer by a factor of V2 For precision drawings dash lengths should remain approximately constant for any line orientation To accomplish this we can adjust the pixel counts for the solid spans and interspan spacing according to the line slope In  we can dis play approximately equal length dashes by reducing the diagonal dash to three pixels Another method for maintaining dash length is to treat dashes as individ ual line segments Endpoint coordinates for each dash are located and passed to the line routine which then calculates pixel positions along the dash path  Line Width Implementation of line width options depends on the capabilities of the output device A heavy line on a video monitor could be displaved as adjacent parallel lines while a pen plotter nught require pen changes As with other PHIGS attrib utes a line width command is used to set the current line width value in the at tribute list This value is then used by line drawing algorithms to control the thickness of lines generated with subsequent output primitive commands We set the line width attribute with the command  setLinew  inScaleFactor lw  Line width parameter i is assigned a positive number to indicate the relahve width of the line to be displayed A value of  specifies a standard width line On a pen plotter for instance a user could set lw to a value of to plot a line whose width is half that of the standard line Values greater than produce lines thicker than the standard For raster implementation a standard width line is generated with single pixels at each sample position as in the Bresenham algorithm Other width lines are displayed as positive integer multiples of the standard line by plotting addi tional pixels along adjacent parallel line paths For lines with slope magnitude less than we can modify a line drawing routine to display thick lines by plot ting a vertical span of pixels at each x position along the line The number of pix els in each span is set equal to the integer magnitude of parameter 1w In  we plot a double width line by generating a parallel line above the original line path At each x sampling position we calculate the corresponding y coordinate and plot pixels with screen coordinates x y and x y  We display lines with lw  by alternately plotting pixels above and below the single width line path  For lines with slope magnitude greater than we can plot thick lines with horizontal spans alternately picking up pixels to the right and left of the line path This scheme is demonstrated in  where a line width of is plotted with horizontal pixel spans  Although thick lines are generated quickly by plotting horizontal or vertical pixel spans the displayed width of a line measured perpendicular to the line path is dependent on its slope A line will be displayed thinner by a factor of compared to a horizontal or vertical line plotted with the samelength pixel spans  Another problem with implementing width options using horizontal or vertical pixel spans is that the method produces lines whose ends are horizontal or vertical regardless of the slope of the line This effect is more noticeable with very thick lines We can adjust the shape of the line ends to give them a better ap pearance by adding line caps   One kind of line cap is the butt cap ob tained by adjusting the end positions of the component parallel lines so that the thick line is displayed with square ends that are perpendicular to the line path If the specified line has slope m the square end of the thick line has slope m Another line cap is the round cap obtained by adding a filled semicircle to each butt cap The circular arcs are centered on the line endpoints and have a diameter equal to the line thickness A third type of line cap is the projecting square cap Here we simply extend the line and add butt caps that are positioned one half of the line width beyond the specified endpoints  Other methods for producing thick lines include displaying the line as a filled rectangle or generating the line with a selected pen or brush pattern as dis cussed in the next section To obtain a rectangle representation for the line   Double wide raster line with slope m  generated with vertical pixel spans  Section Line Attributes     x  r x   C   oe      Raster line with slope m   and line width parameter lw  plotted with horizontal pixel spans  boundary we calculate the position of the rectangle vertices along perpendicu Jars to the line path so that vertex coordinates are displaced from the line end points by one half the line width The rectangular line then appears as in Fig a  We could then add round caps to the filled rectangle or extend its length to display projecting square caps  Generating thick polylines requires some additional considerations In gen eral the methods we have considered for displaying a single line segment will not produce a smoothly connected series of line segments Displaying thick lines using horizontal and vertical pixel spans for example leaves pixel gaps at the boundaries between lines of different slopes where there is a shift from horizon tal spans to vertical spans We can generate thick polylines that are smoothly joined at the cost of additional processing at the segment endpoints  shows three possible methods for smoothly joining two line segments A miter join is accomplished by extending the outer boundaries of each of the two lines until they meet A round join is produced by capping the connection between the two segments with a circular boundary whose diameter is equal to the line  a  b ic  Thick lines drawn with a butt caps  b round caps and c projecting square caps  a  b tc   Thick line segments connected with a miter join  b round join and c bevel join  width And a bevel join is generated by displaying the line segments with butt caps and filling in the triangular gap where the segments meet If the angle be tween two connected line segments is very small a miter join can generate a long spike that distorts the appearance of the polyline A graphics package can avoid this effect by switching from a miter join to a bevel join say when any two con secutive segments meet at a small enough angle  Pen and Brush Options With some packages lines can be displayed with pen or brush selections Op tions in this category include shape size and pattern Some possible pen or brush shapes are given in  These shapes can be stored in a pixel mask that identifies the array of pixel positions that are to be set along the line path For example a rectangular pen can be implemented with the mask shown in Fig by moving the center or one corner of the mask along the line path as in  To avoid setting pixels more than once in the frame buffer we can sim ply accumulate the horizontal spans generated at each position of the mask and keep track of the beginning and ending x positions for the spans across each scan line  Lines generated with pen or brush shapes can be displayed in various widths by changing the size of the mask For example the rectangular pen line in  could be narrowed with a rectangular mask or widened with a 4x4 mask Also lines can be displayed with selected patterns by superimposing the pattern values onto the pen or brush mask Some examples of line patterns are shown in  An additional pattern option that can be provided in a paint package is the display of simulated brush strokes  illustrates some patterns that can be displayed by modeling different types of brush strokes  Line Color When a system provides color or intensity options a parameter giving the cur rent color index is included in the list of system attribute values A polyline rou tine displays a line in the current color by setting this color value in the frame buffer at pixel locations along the line path using the set Pixel procedure The number of color choices depends on the number of bits available per pixel in the frame buffer  We set the line color value in PHIGS with the function setPolylineColourIndex lc  Section Line Auributes Custom Document Brushes    ae     Pen and brush shapes for line display  Nonnegative integer values corresponding to allowed color choices are assigned to the line color parameter Lc A line drawn in the background color is invisible and a user can erase a previously displayed line by respecifying it in the back ground color assuming the line does not overlap more than one background color area  An example of the use of the various line attribute commands in an applica  ions program is given by the following sequence of statements  setLinetype  setLinewictnsScaleFactor  setPolylinetolourIndex  polyline nl wcpointsl  setPolylinetclourIndex  polyline n  wepoints   This program segment would display two figures drawn with double wide dashed lines The first is displayed in a color corresponding to code and the second in color     Line    Path a  bi   a A pixel mask for a rectangular pen and b the associated array of pixels displayed by centering the mask over a specified pixel position   X   _ Generating a line with the pen shape of      Curved lines drawn with a paint program using various shapes and patterns From left to right the brush shapes are square round diagonal line dot pattern and faded airbrush     A daruma doll a symbol of good fortune in Japan drawn by computer artist Koichi Kozaki using a paintbrush system Daruma dolls actually come without eyes One eye is painted in when a wish is made and the other is painted in when the wish comes true  Courtesy of Wacom Technology Inc   Parameters for curve attributes are the same as those for line segments We can display curves with varying colors widths dot dash patterns and available pen or brush options Methods for adapting curve drawing algorithms to accommo date attribute selections are similar to those for line drawing  The pixel masks discussed for implementing line type options are also used in raster curve algorithms to generate dashed and dotted patterns For example the mask produces the dashed circle shown in  We can generate the dashes in the various octants using circle symmetry but we must shift the pixel positions to maintain the correct sequence of dashes and spaces as we move from one octant to the next Also as in line algorithms pixel masks display dashes and interdash spaces that vary in length according to the slope of the curve If we want to display constant length dashes we need to adjust the num ber of pixels plotted in each dash as we move around the circle circumference In stead of applying a pixel mask with constant spans we plot pixels along equal angular arcs to produce equal length dashes  Raster curves of various widths can be displayed using the method of hon zontal or vertical pixel spans Where the magnitude of the curve slope is less than we plot vertical spans where the slope magnitude is greater than we plot horizontal spans  demonstrates this method for displaying a circular arc of width in the first quadrant Using circle symmetry we generate the circle path with vertical spans in the octant from x  to x  y and then reflect pixel positions about theline y  x to obtain the remainder of the curve shown Circle sections in the other quadrants are obtained by reflecting pixel positions in the first quadrant about the coordinate axes The thickness of curves displayed with Section this method is again a function of curve slope Circles ellipses and other curves Curve Attributes will appear thinnest where the slope has a magnitude of Another method for displaying thick curves is to fill in the area between two parallel curve paths whose separation distance is equal to the desired width We could do this using the specified curve path as one boundary and setting up the second boundary either inside or outside the original curve path This ap proach however shifts the original curve path either inward or outward de pending on which direction we choose for the second boundary We can maintain the original curve position by setting the two boundary curves at a distance of one half the width on either side of the specified curve path An example of this approach is shown in  for a circle segment with radius and a specified width of The boundary arcs are then set at a separation distance of on either side of the radius of To maintain the proper dimensions of the circular arc as discussed in Section 10 we can set the radii for the concentric boundary arcs at r 14and r  Although this method is accurate for generating thick circles in general it provides only an approximation to the true area of other thick       A dashed circular arc displayed with a dash span of pixels and an interdash spacing of pixels            Circular are of width plotted with pixel spans      A circular arc of width and radius displayed by filling the region between two concentric ares   Circular arc displayed with a rectangular pen  curves For example the inner and outer boundaries of a fat ellipse generated with this method do not have the same foci  Pen or brush displays of curves are generated using the same techniques discussed for straight line segments We replicate a pen shape along the line path as illustrated in  for a circular arc in the first quadrant Here the center of the rectangular pen is moved to successive curve positions to produce the curve shape shown Curves displayed with a rectangular pen in this manner will be thicker where the magnitude of the curve slope is A uniform curve thickness can be displayed by rotating the rectangular pen to align it with the slope direc tion as we move around the curve or by using a circular pen shape Curves drawn with pen and brush shapes can be displayed in different sizes and with superimposed patterns or simulated brush strokes  Various color and intensity level options can be made available to a user de pending on the capabilities and design objectives of a particular system General purpose raster scan systems for example usually provide a wide range of colors while random scan monitors typically offer only a few color choices if any Color options are numerically coded with values ranging from through the positive integers For CRT monitors these color codes are then converted to intensity level settings for the electron beams With color plotters the codes could control ink jet deposits or pen selections  Ina color raster system the number of color choices available depends on the amount of storage provided per pixel in the frame buffer Also color informa tion can be stored in the frame buffer in two ways We can store color codes di rectly in the frame buffer or we can put the color codes in a separate table and use pixel values as an index into this table With the direct storage scheme when ever a particular color code is specified in an application program the corre sponding binary value is placed in the frame buffer for each component pixel in the output primitives to be displayed in that color A minimum number of colors can be provided in this scheme with bits of storage per pixel as shown in Table Each of the three bit positions is used to control the intensity level either on or off of the corresponding electron gun in an RGB monitor The leftmost bit controls the red gun the middle bit controls the green gun and the rightmost bit controls the blue gun Adding more bits per pixel to the frame buffer increases the number of color choices With bits per pixel bits can be used for each gun This allows four different intensity settings for each of the three color guns and a total of color values are available for each screen pixel With a resolution of by a full color bit per pixel RGB system needs megabytes of storage for the frame buffer Color tables are an alternate means for providing ex tended color capabilities to a user without requiring large frame buffers Lower cost personal computer systems in particular often use color tables to reduce frame buffer storage requirements  Color Tables  illustrates possible scheme for storing color values in a color lookup table or video lookup table  where frame buffer values are now used as indices into the color table In this example each pixel can reference any one of the table positions and each entry in the table uses bits to specify an RGB color For the color code a combination green blue color is displayed for pixel location x y  Systems employing this particular lookup table would allow TABLE THE EIGHT COLOR CODES FOR A THREE BIT PER PIXEL FRAME BUFFER  Stored Color Values Displa yed Color in Frame Buffer Color Code RED GREEN BLUE  Black Biue Green Cyan Red Magenta Yellow White WA bw NM H seen Hoo oO  o70 Section Color and Grayscale Levels Chapter 4a user to select any colors for simultaneous display from a palette of nearly Attributes of Output Primitives million colors Compared to a full color system this scheme reduces the num  ber of simultaneous colors that can be displayed but it also reduces the frame buffer storage requirements to megabyte Some graphics systems provide bits per pixel in the frame buffer permitting a user to select colors that could be used in each display  A user can set color table entries in a PHIGS applications program with the function  setColourRepresentation ws ci colorptr  Parameter ws identifies the workstation output device parameter ci specifies the color index which is the color table position number to for the exam ple in   and parameter colorptr points to a trio of RGB color values r g b each specified in the range from to An example of possible table entries for color monitors is given in   There are several advantages in storing color codes in a lookup table Use of a color table can provide a reasonable number of simultaneous colors without requiring large frame buffers For most applications or different colors are sufficient for a single picture Also table entries can be changed at any time allowing a user to be able to experiment easily with different color combinations in a design scene or graph without changing the attribute settings for the graph ics data structure Similarly visualization applications can store values for some physical quantity such as energy in the frame buffer and use a lookup table to try out various color encodings without changing the pixel values And in visual ization and image processing applications color tables are a convenient means for setting color thresholds so that all pixel values above or below a specified threshold can be set to the same color For these reasons some systems provide both capabilities for color code storage so that a user can elect either to use color tables or to store color codes directly in the frame buffer  To Red Gun  Ta Green Gun   To Biue Gun    A color lookup table with bits per entry accessed from a frame buffer with bits per pixel A value of stored at pixel position x y references the location in this table containing the value Each bit segment of this entry controls the intensity level of one of the three electron guns in an RGB monitor        ca Coler Ci Color  OF   Q         03 13 L   Workstation color tables  Grayscale With monitors that have no color capability color functions can be used in an ap plication program to set the shades of gray or grayscale for displayed primi tives Numeric values over the range from to J can be used to specify grayscale levels which are then converted to appropriate binary codes for storage in the raster This allows the intensity settings to be easily adapted to systems with dif tering grayscale capabilities  Table lists the specifications for intensity codes for a four level gray scale system In this example any intensity input value near 33 would be stored as the binary value in the frame buffer and pixels with this value would be displayed as dark gray If additional bits per pixel are available in the frame buffer the value of 33 would be mapped to the nearest level With bits per pixel we can accommodate  gray levels while bits per pixel would give us shades of gray An alternative scheme for storing the intensity information is to convert each intensity code directly to the voltage value that produces this gray scale level on the output device in use  When multiple output devices are available at an installation the same color table interface may be used for all monitors In this case a color table for a monochrome monitor can be set up using a range of RGB values as in  with the display intensity corresponding to a given color index ci calculated as intensity  min r g b  max r g b  TABLE INTENSITY CODES FOR A FOUR LFVEL GRAYSCALE SYSTEM  Intensity Stored Intensity Displayed Codes Values In The Grayscale Frame Buffer Binary Code  Black 33 aly Dark gray 67  Light gray qi1 White  Section Calor and Grayscale Levels Hollow fa  Patterned te  Figure  Polygon fill styles   Options for filling a defined region include a choice between a solid color or a patterned fill and choices for the particular colors and patterns These fill options can be applied to polygon regions or to areas defined with curved boundaries depending on the capabilities of the available package In addition areas can be painted using various brush styles colors and transparency parameters  Fill Styles Areas are displayed with three basic fill styles hollow with a color border filled with a solid color or filled with a specified pattern or design A basic fill style is selected in a PHIGS program with the function setInteriorStyle fs  Values for the fill style parameter fs include hollow solid and pattern   Another value for fill style is hatch which is used to fill an area with selected hatching patterns parallel lines or crossed lines as in  As with line at tributes a selected fill style value is recorded in the list of system attributes and applied to fill the interiors of subsequently specified areas Fill selections for pa rameter fs are normally applied to polygon areas but they can also be imple mented to fill regions with curved boundaries  Hollow areas are displayed using only the boundary outline with the inte rior color the same as the background color A solid fill is displayed in a single color up to and including the borders of the region The color for a solid interior or for a hollow area outline is chosen with setInteriorColourIndex fc  where fill color parameter fc is set to the desired color code A polygon hollow fill is generated with a line drawing routine as a closed polyline Solid fill of a re gion can be accomplished with the scan line procedures discussed in Section 11  Other fill options include specifications for the edge type edge width and edge color of a region These attributes are set independently of the fill style or fill color and they provide for the same options as the line attribute parameters line type line width and line color  That is we can display area edges dotted or dashed fat or thin and in any available color regardless of how we have filled the interior  MN eee EE  Diagonal Diagonal Hatch Fill Cross Hatch Fill  Polygon fill using hatch patterns Pattern Fill We select fill patterns with setInteriorStyleindex pi  where pattern index parameter pi specifies a table position For example the fol lowing set of statements would fill the area defined in the i11Area command with the second pattern type stored in the pattern table  setInteriorStyie pattern  setInteriorStyleIndex  fillArea n points  Separate tables are set up for hatch patterns  f we had selected hatch fill for the interior s yle in this program segment then the value assigned to parameter pi is an index to the stored patterns in the hatch table  For fill style pattern table entries can be created on individual output de vices with setPatternRepresentation iws p nx ny cp  Parameter pi sets the pattern index number for workstation code ws and cp isa two dimensional array of color codes with nx colunins and ny rows The follow ing program segment illustrates how this function could be used to set the first entry in the pattern table for workstation  cpll    ep   epli   cp    setPatternRepresentation   cp  Table shows the first two entries for this color table Color array cp in this ex ample specifies a pattern that produces alternate red and black diagonal pixel lines on an eight color system  When a color array cp is to be applied to fill a region we need to specify the size of the area that is to be covered by each element of the array We do this by setting the rectangular coordinate extents of the pattern  setPatternSize dx dy  where parameters dx and dy give the coordinate width and height of the array mapping An example of the coordinate size associated with a pattern array is given in  If the values for dx and dy in this figure are given in screen co ordinates then each element of the color array would be applied to a by screen grid containing four pixels  A reference position for starting a pattern fill is assigned with the statement setPatrernReferencePoint  positicn Parameter posit on is a pointer to coordinates xp yp that fix the lower left comer of the rectangular pattern From this starting position the pattern is then replicated in the x and y directions until the defined area is covered by nonover  TABLE A WORKSTATION PATTERN TABLE WITH TWO ENTRIES USING THE COLOR CODES OF TABLE Index Pattern p1  cp  nN co Noh hRo Norns __ _1   T b    fer tae  ba tap te 4nd    Fienre 20 A pattern array with columns and rows mapped to an by coordinate rectangle Chapter Attributes of Output Primitives lapping copies of the pattern array The process of filling an area with a rectangu lar pattern is called tiling and rectangular fill patterns are sometimes referred to as tiling patterns  demonstrates tiling of a triangular fill area starting from a pattern reference point  To illustrate the use of the pattern commands the following program exam ple displays a black and white pattern in the interior of a parallelogram fill area   The pattern size in this program is set to map each array element toa single pixel      Start Position     Tiling an area froma designated start position Nonoverlapping adjacent patterns are laid out to cover all sean lines passing through the defined area   define WS void patternFill   wePt2 pts  int bwPattern          pSetPatternRepresentation WS   bwPattern  pes x  pts O y  pts x  pts l y  pts x  pts  y  pts x  pts y   pSetFillAreaInteriorStyle PATTERN  pSetFillAreaPatternIndex  pSetPatternReferencePoint   pFillArea  pts  Pattern fill can be implemented by modifying the scan line procedures dis cussed in Chapter so that a Selected pattern is superimposed onto the scan lines Beginning from a specified start position for a pattern fill the rectangular patterns would be mapped vertically to scan lines between the top and bottom of the fill area and horizontally to interior pixel positions across these scan lines Horizontally the pattern array is repeated at intervals specified by the value of size parameter dx Similarly vertical repeats of the pattern are separated by inter vals set with parameter dy This scan line pattern procedure applies both to poly gons and to areas bounded by curves   te e   at   A pattern array a superimposed on a parallelogram fill area to produce the display b  Hatch fill is applied to regions by displaying sets of parallel lines The fill procedures are implemented to draw either single hatching or cross hatching Spacing and slope for the hatch lines can be set as parameters in the hatch table On raster systems a hatch fill can be specified as a pattern array that sets color values for groups of diagonal pixels  in many systems the pattern reference point xp yp 1s assigned by the sys tem For instance the reference point could be set automatically at a polygon ver tex In general for any fill region the reference point can be chosen as the lower left corner of the bounding rectangle or bounding box determined by the coordi nate extents of the region   To simplify selection of the reference coordi nates some packages always use the screen coordinate origin as the pattern start position and window systems often set the reference point at the coordinate ori gin of the window Always setting xp yp at the coordinate origin also simplifies the tiling operations when each color array clement of a pattern is to be mapped to a single pixel For example if the row positions in the pattern array are refer enced in reverse that is from bottom to top starting at  a pattern value is then assigned to pixel position x y in screen or window coordinates as setPixel  x y  cPpty mod ny x mod nx    where ny and nx specify the number of rows and number of columns in the pat tern array Setting the pattern start position at the coordinate origin however ef fectively attaches the pattern fill to the screen or window background rather than to the fill regions Adjacent or overlapping areas filled with the same pattern would show no apparent boundary between the areas Also repositioning and refilling an object with the same pattern can result in a shift in the assigned pixel values over the object interior A moving object would appear to be transparent against a stationary pattern background instead of moving with a fixed interior pattern  It is also possible te combine a fill pattern with background colors includ ing grayscale in various ways With a bitmap pattern containing only the digits and the values could be used as transparency indicators to let the back ground show through Alternatively the and digits can be used to fill an inte tior with two color patterns In general color fill patterns can be combined in several other ways with background colors The pattern and background colors can be combined using Boolean operations or the pattern colors can simply re place the background colors  demonstrates how the Boolean and re place operations for a by fill pattern would set pixel values on a binary black and white system against a particular background pattern   Ymax   Bounding t Rectangle  Bounding rectangle for a region with coordinate extents Xin Xmax x Varun ANG Ya in the x and y man directions  mio Section Area Fill Attrivutes   eo  Pattern Background   Pixel Values  Combining a fili pattern with a backgrouna pattern using Boolean operations and or and xor exclusive or  and using simple replacement  Soft Fill Modified boundary fill and flood fill procedures that are applied to repaint areas so that the fill color is combined with the background colors are referred to as soft fill or tint fill algorithms One use for these fill methods is to soften the fill colors at object borders that have been blurred to antialias the edges Another is to allow repainting of a color area that was originally filled with a semitranspar ent brush where the current color is then a mixture of the brush color and the background colors behind the area In either case we want the new fill color to have the same variations over the area as the current fill color  As an example of this type of fill the linear soft fill algorithm repaints an area that was originally painted by merging a foreground color F with a single background color B where F  B Assuming we know the values for F and B we can determine how these colors were originally combined by checking the cur rent color contents of the frame buffer The current RGB color P of each pixel within the area to be refilled is some linear combination of F and B  P tF  OB where the transparency factor t has a value between and for each pixel For values of t less than the background color contributes more to the interior color of the region than does the fill color Vector Equation holds for each RGB component of the colors with P  Pp Po Pad F  Fz Fe Fp  B  Bz By Bg   We can thus calculate the value of parameter  using one of the RGB color com ponents as Po By pa eB F By  where k  R G or B and F  B  Theoretically parameter t has the same value for each RGB component but roundoff to integer codes can result in different values of  for different components We can minimize this roundoff error by se lecting the component with the largest difference between F and B This value of tis then used to mix the new fill color NF with the background color using either a modified flood fill or boundary fill procedure  Similar soft fill procedures can be applied to an area whose foreground color is to be merged with multiple background color areas such as a checker board pattern When two background colors B  and B are mixed with fore ground color F the resulting pixel color P is P  F   B    to  t B   where the sum of the coefficients ty t  and  t  t  on the color terms must equal We can set up two simultaneous equations using two of the three RGB color components to solve for the two proportionality parameters tg and  These parameters are then used to mix the new fill color with the two back ground colors to obtain the new pixel color With three background colors and one foreground color or with two background and two foreground colors we need all three RCB equations to obtain the relative amounts of the four colors For some foreground and background color combinations however the system of two or three RGB equations cannot be solved This occurs when the color val ucs are a very similar or when they are all proportional to each other  The appearance of displayed characters is controlled by attributes such as font size color and orientation Attributes can be set both for entire character strings text and for individual characters defined as marker symbols  Tex Attributes There are great many text options that can be made available to graphics pro grammers First of all there is the choice of font or typeface  which is a set of characters with a particular design style such as New York Courier Helvetica London Times Roman and various special symbol groups The characters in a selected font can also be displaved with assorted underlining styles solid ted doubie  in boldface in italics and in outline or shadow styles A particular  Section Character Attributes Chapter Attributes of Output Primitives  font and associated style is selected in a PHIGS program by setting an integer code for the text font parameter t f in the function serTextFou  tf  Font options can be mace available as predetined sets of grid patterns or as char acter sets designed with polylines and spline curves  Color settings for displayed text are stored in the system attribute list and used by the procedures that load character definitions into the frame buffer When a character string is to be displayed the current color is used to set pixel values in the frame buffer corresponding to the character shapes and positions Control of text color or intensity is managed from an application program with setTextCoLourIndex tc  where text color parameter zc specifies an allowable color code  We can adjust text size by scaling the overall dimensions height and width of characters or by scaling only the character width Character size is specified by printers and compositors in points where point is 013837 inch or approxi mately 72 inch  For example the text you are now reading is a point font Point measurements specify the size of the body of a character   but dif ferent fonts with the same point specifications can have different character sizes depending on the design of the typeface The distance between the bottomline and the topline of the character body is the same for all characters in a particular size and typeface but the body width may vary Proportionally spaced fonts assign a smaller body width to narrow characters such as i j  and f compared to broad characters such as W or M Character heighi is defined as the distance between the baseline and the capline of characters Kerned characters such as f and j in Fig 25 typically extend beyond the character body limits and letters with descend ers g j p q y extend below the baseline Fach character is positioned within the character body by a font designer to allow suitable spacing along and be tween print lines when text is displayed with character bodies touching  Text size can be adjusted without changing the width to height ratio of characters with secCharacterHeight ich  _ character  kern _ character pom body e i character height    base   im    i  __ bottom J  kern igure Character body Height  Height Height Parameter ch is assigned a real value greater than to set the coordinate height of capital letters the distance between baseline and capline in user coordinates This setting also affects character body size so that the width and spacing of characters is adjusted to maintain the same text proportions For instance dou bling the height also doubles the character width and the spacing between char acters  shows a character string displayed with three different charac ter heights The width only of text can be set with the function  The effect of different character height settings on displayed text  setCharacterExpansionFactor cw  where the character width parameter cw is set to a positive real value that scales the body width of characters Text height is unaffected by this attribute setting Examples of text displayed with different character expansions is given in Fig 27  Spacing between characters is controlled separately with setCharacterSpacing cs  where the character spacing parameter cs can be assigned any real value The value assigned to cs determines the spacing betwveen character bodies along print lines Negative values for cs overlap character bodies positive values in sert space to spread out the displayed characters Assigning the value to cs causes text to be displayed with no space between character bodies The amount of spacing to be applied is determined by multiplying the value of cs by the character height distance between baseline and capline  In  a character string is displayed with three different settings for the character spacing para meter  The orientation for a displayed character string is set according to the direc tion of the character up vector  setCharacterUpVector upvect  Parameter upvect in this function is assigned two values that specify the x and y vector components Text is then displayed so that the orientation of characters from baseline to capline is in the direction of the up vector For example with upvect    the direction of the up vector is and text would be displayed as shown in  A procedure for orienting text rotates characters so that the sides of character bodies from baseline to capline are aligned with the up vector The rotated character shapes are then scan converted into the frame buffer  Section Character Attributes widthO width width   The effect of different character width settings on displayed text  Spacing Spacing Spacing  The effect of different character spacings on displayed text  Chapter TT ee  Attributes of Output Primitives i  i  i    te a      UZ Direction of the up vector a  Up Vector  _ controls the orientation of  a  oh displayed text b  It is useful in many applications to be able to arrange character strings verti cally or horizontally   An attribute parameter for this option is set with the statement seuTextPath tp     t  Ax ms cro dpme  f A where the text path parameter tp can be assigned the value right left up or down Examples of text displayed with these four options are shown in  A procedure for implementing this option must transform the character patterns into the specified orientation before transferring them to the frame buffer  Character strings can also be oriented using a combination of up vector and text path specifications to produce slanted text  shows the directions  of character strings generated by the various text path settings for a up vec  tor Examples of text generated for text path values down and right with this up ee vector are illustrated in  oe Another handy attribute fer character strings is alignment This attribute  specifies how text is to be positioned with respect to the start coordinates Align Text path attributes can be set Ment attributes are set with to produce horizontai or vertical arrangements of setTextAligumert h vi character strings  _ a HORIZONTAL TEXT where parameters h and  control horizontal and vertical alignment respectively Horizontal alignment is set by assigning h a value of left centre or right Vertical D alignment is set by assigning v a value of top cap half base or bottom The inter pretation of these alignment values depends on the current setting for the text r path  shows the position of the alignment settings when text is to be t displayed horizontally to the right or vertically down Similar interpretations   apply to text path values of left and up The most natural alignment for a par SNMIres string ticular text path is chosen by assigning the value norma to the h and v parame  s ters  illustrates common alignment positions tor horizontal and vertt  i cal text labels  r A precision specification for text display is given with n setTextPree sion tpri K vee where text precision parameter tpr is assigned one of the values string char or  stroke The highest quality text is displayed when the precision parameter is set to Text displayed with the four the value stroke For this precision setting greater detail would be used in defin text path options ing the character shapes and the processing of attribute selections and other string manipulation procedures would be carried out to the highest possible ac curacy The lowest quality precision setting string is used for faster display of character strings At this precision many attribute selections such as text path are ignored and string manipulation procedures are simplified to reduce processing time  Marker Attributes A marker symbol is a single character that can be displayed in different colors and in different sizes Marker attributes are implemented by procedures that load the chosen character into the raster at the defined positions with the specified color and size  We select a particular character to be the marker symbol with setMarkerType mt  where marker type parameter mt is set to an integer code Typical codes for marker type are the integers through specifying respectively a dat   a ver tical cross   an asterisk   a circle o  and a diagonal cross x  Displayed marker types are centered on the marker coordinates  We set the marker size with setMarkerSizeScaleFactor ms  with parameter marker size ms assigned a positive number This scaling parame ter is applied to the nominal size for the particular marker symbol chosen Values greater than produce character enlargement values less than reduce the marker size    STRING left center   top   cap T  w  voooo ees Half ese   Dase G Bottom  left right Alignment attribute values for horizontal and vertical strings   canter Section Charac er Attributes Direction of Character up Vector  a   oy  a Lo g   Text Path Direction b    An up vector spec fication a controls the direction of the text path b    a  b   The up vector in  produces the display a fora down path and the display b for a right path   RAH OL GNMEN CENTER AUIGNMENT Gn tbe aot ALIG  Ligure 35 Character string alignmenss  Marker color is specified with setPolymarkerColourlIndex mc  A selected color cade tor parameter mc is stored in the current attribute list and used to display subsequently specified marker primitives   With the procedures we have considered so far each function references a single attribute that specifies exactly how a primitive is to be displayed with that at tribute setting These specifications are called individual or unbundled attrib utes and they are meant to be used with an output device that is capable of dis playing primitives in the way specified If an application program employing individual attributes is interfaced to several output devices some of the devices may not have the capability to display the intended attributes A program using individual color attributes for example mav have to be modified to produce ac ceptable output on a monochromatic monitor  Individual attribute commands provide a simple and direct method for specifying attributes when a single output device is used When several kinds of output devices are available at a graphics installation it is convenient for a user to be able to say how attributes are to be interpreted on each of the different de vices This is accomplished by setting up tables for each output device that lists sets of attribute values that are to be used on that device to display each primi tive tvpe A particular set of attribute values for a primitive on each output de vice is then chosen by specifying the appropriate table index Attributes specified in this manner are called bundled attributes The table for each primitive hat de fines groups of attribute values to be used when displaying that primitive on a particular output device 1s called a bundle table  Attributes that may be bundled into the workstation table entries are those that do not involve courdinate specifications such as color and line type The choice between a bundled or an unbundled specification is made by setting a switch called the aspect source flag for each of these attributes  set individsalASF fattributeptr flagptr where parameter attic buteptr points to a list of attributes and parameter flagptr points to the corresponding list of aspect source flags Each aspect source flag can be assigned a value of individual or bundled Attributes that may be bundled are listed in the following sections  Bundled lire Attributes Entries in the bundle lable for line attributes on a specitied workstation are set with the function setPolylinetepresentation ws li it lw lc Parameter ws is the workstation identifier and line index parameter 1i defines Section the bundle table position Parameters 1t lw and 1c are then bundled and as Bundled Attributes signed values to set the line type line width and line color specifications respec  tively for the designated table index For example the following statements de  fine groups of line attributes that are to be referenced as index number on two different workstations  setPolyiineRepresentation    setPclylineRepresentaticn    A polyline that is assigned a table index value of would then be displayed using dashed lines at half thickness in a blue color on workstation while on workstation this same index generates solid standard sized white lines  Once the bundle tables have been set up a group of bundled line attributes is chosen for each workstation by specifying the table index value  setPolylineIndex 1i  Subsequent polyline commands would then generate lines on each worksta tion according to the set of bundled attribute values defined at the table position specified by the value of the line index parameter  Bundled Area Fill Attributes Table entries for bundled area fill attributes are set with setiInteriorRepresentation ws fi fs pi fe  which defines the attribute list corresponding to fill index i on workstation ws Parameters fs pi and fc are assigned values for the fill style pattern index and fill color respectively on the designated workstation Similar bundle tables can also be set up for edge attributes of polygon fill areas  A particular attribute bundle is then selected from the table with the func tion setinteriorIndex fi Subsequently defined fill areas are then displayed on each active workstation ac cording to the table entry specified by the fill index parameter fi Other fill area attributes such as pattern reference point and pattern size are independent of the workstation designation and are set with the functions previously described  Bundled Text Attributes The function setTextRepresentation ws ti tf tp te ts tc  bundles values for text font precision expansion factor size and color in a table position for workstation ws that is specified by the value assigned to text index  parameter ci Other text attributes including character up vector text path character height and text alignment are set individually A particular text index value is then chosen with the function setTextIndex ti  Each text function that is then invoked is displayed on each workstation with the set of attributes referenced by this table position  Bundled Marker Attrioutes Table entries for bundled marker attributes are set up with setPolymazkerRepresentation ws mi mt ms mc  This defines the marker type marker scale factor and marker color for index mi on workstation ws Bundle table selections are then made with the function setPolymarkerIndex mi   Current settings for attributes and other parameters such as workstation types and status in the system lists can be retrieved with mquiry functions These functions allow current values to be copied into specified parameters which can then be saved for later reuse or used to check the current state of the system if an error occurs  We check current attribute values by stating the name of the attribute in the inquiry function For example the functions inguirePolvi nelndex lasz1i and inguirelInteriorColourIndex lastfc  copy the current values for line index and fill color into parameters last1i and lastfc The following program segment illustrates reusing the current line type value after a set of lines are drawn with a new line type  inquireLinetype aldlt  setLincetype newlt  setLinetype  oldit  Displayed primitives generated by the raster algorithms discussed in Chapter have a jagged or stairstep appearance because the sampling process digitizes co ordinate points on an object to discrete integer pixel positions This distortion of information due to low frequency sampling undersampling is called aliasing We can improve the appearance of displayed raster lines by applying antialias ing methods that compensate for the undersampling process  An example of the effects of undersampling is shown in  To avoid losing information from such periodic objects we need to set the sampling fre quency to at least twice that of the highest frequency occurring in the object re ferred to as the Nyquist sampling frequency or Nyquist sampling rate f  f 2fonax  Another way to state this is that the sampling interval should be no larger than one half the cycle interval called the Nyquist sampling interval  For x interval sampling the Nyquist sampling interval Ax is Ax  Sats  where Axjyce  fmax In  our sampling interval is one and one half times the cycle interval so the sampling interval is at least three times too big If we want to recover all the object information for this example we need to cut the sampling interva down to one third the size shown in the figure  One way to increase sampling rate with raster systems is simply to display objects at higher resolution But even at the highest resolution possible with cur rent technology the jaggies will be apparent to some extent There is a limit to how big we can make the frame buffer and still maintain the refresh rate at to frames per second And to represent objects accurately with continuous para meters we need arbitrarily small sampling intervals Therefore unless hardware technology is developed to handle arbitrarily large frame buffers increased screen resolution is not a complete solution to the aliasing problem  NVVMA       Sampling Positions ta   Sampling the periodic shape in a at the marked positions produces the aliased lower frequency  b representation in b   Section Antialiasing  I Chapter Attributes of Output Primitives No With raster systems that are capable of displaying more than two intensity levels color or gray scale  we can apply antialiasing methods to modify pixel in tensities By appropriately varying the intensities of pixels along the boundaries of primitives we can sinooth the edges to lessen the jagged appearance  A straightforward antialiasing method is to increase sampling rate by treat ing the screen as if it were covered with a finer grid than is actually available We can then use multiple sample points across this finer grid to determine an appro priate intensity level for each screen pixel This technique of sampling object characteristics at a high resolution and displaying the results at a lower resolu tion is called supersampling or postfiltering since the general method involves computing intensities at subpixe grid positions then combining the results to obtain the pixel intensities  Displayed pixel positions are spots of light covering a finite area of the screen and not infinitesimal mathematical points Yet in the line and fill area algorithms we have discussed the intensity of each pixel is de termined by the location of a single point on the object boundary By supersam pling we obtain intensity information from multiple points that contribute to the overall intensity of a pixel  An alternative to supersampling is to determine pixel intensity by calculat ing the areas of overlap of each pixe with the objects to be displayed Antialias ing by computing overlap areas is referred to as area sampling or prefiltering since the intensity of the pixel as a whole is determined without calculating sub pixel intensities  Pixel overlap areas are obtained by determining where object boundaries intersect individual pixel boundaries  Raster objects can also be antialiased by shifting the display location of pixel areas This technique calied pixel phasing is applied by microposition ing the electron beam in relation to object geometry  Supersampling Straight Line Segments Supersamp ing straight lines can be performed in several ways For the gray scale display of a straight line segment we can divide each pixel into a number of subpixels and count the number of subpixels that are along the line path The intensity level for each pixel is then set to a value that is proportional to this sub pixel count An example of this method is given in Fig 37 Each square pixel area is divided into rine equal sized square subpixels and the shaded regions show the subpixels that would be selected by Bresenham s algorithm This scheme provides for three intensity settings above zero since the maximum nuinber of subpixels that can be selected within any pixel is three For this exam ple the pixel at position  is set to the maximum ntensity evel  pixels at  and  are each set to the next highest intensity level  and pix els at  and  are each set to the lowest intensity above zero evel  Thus the line intensity 1s spread out over a greater number of pixels and the stairstep effect is smoothed by displaying a somewhat blurred line path in the vicinity of the stair steps between horizontal runs  if we want to use more inten sity levels to antialiase the line with this methad we increase the number of sam pling positions across each pixel Sixteen subpixels gives us four intensity levels above zero twenty five subpixels gives us five levels and so on  In the supersampling example of  we considered pixel areas ot fi nite size but we treated the line as a mathematical entity with zero width Actu ally displayed lines have a width approximately equal to tha of a pixel If we take the finite width of the line into account we can perform supersampling by setting each pixel intensity proportional to the number of subpixels inside the   Supersampling subpixel positions along a straight line segment whose left endpoint is at screen coordinates     polygon representing the line area A subpixel can be considered to be inside the line if its lower left corner is inside the polygon boundaries An advantage of this supersampling procedure is that the number of possible intensity levels for each pixel is equal to the total number of subpixels within the pixel area For the ex ample in  we can represent this line with finite width by positioning the polygon boundaries parallel to the line path as in  And each pixel can now be set to one of nine possible brightness levels above zero  Another advantage of supersampling with a finite width line is that the total line intensity is distributed over more pixels In  we now have the pixel at grid position  turned on at intensity level  and we also pick up contributions from pixels immediately below and immediately to the left of posi tion   Also if we have a color display we can extend the method to take background colors into account A particular line might cross several different color areas and we can average subpixel intensities to obtain pixel color settings For instance if five subpixels within a particular pixel area are determined to be inside the boundaries for a red line and the remaining four pixels fall within a blue background area we can calculate the color for this pixel as Pixel gior   red  blue  The trade off for these gains from supersampling a finite width line is that identifying interior subpixels requires more calculations than simply determining which subpixels are along the line path These calculations are also complicated by the positioning of the line boundaries in relation to the line path This posi    Supersampling subpixel positions in relation to the interior of a line of finite width  Section Antialiasing Chapter Attributes of Output Primitives       Relative weights for a grid of by subpixels  tioning depends on the slope of the line For a line the line path is centered on the polygon area but tor either a horizontal or a vertical line we want the line path to be one of the polygon boundaries For instance a horizontal line passing through grid coordinates  would be represented as the polygon bounded by horizontal grid lines y  and y  Similarly the polygon representing a vertical line through   would have vertical boundaries along grid lines x  and x  For lines with slope  mt   the mathematical line path is posi tioned propertionately closer to the lower polygon boundary and for lines with slope m  this line path is placed closer to the upper polygon boundary  Pixel Weighting Masks Supersampling algorithms are often implemented by giving more weight to sub pixels near the center of a pixel area since we would expect these subpixels to be more important in determining the overall intensity of a pixel For the by pixel subdivisions we have considered so far a weighting scheme as in  could be used The center subpixel here is weighted four times that of the corner subpixels and twice that of the remaining subpixels intensities calculated for each grid of nine subpixels would then be averaged so that the center subpixel is weighted by a factor of the top bottom and side subpixels are each weighted by a factor of and the corner subpixels are each weighted by a fac tor of 16 An atray of values specifying the relative importance of subpixels is sometimes referred to as a mask of subpixel weights Similar masks can be set up for larger subpixel grids Also these masks are often extended to include con tributions from subpixels belonging to neighboring pixels su that intensities can be averaged over adjacent pixels  Area Sampling Straight Line Segments We perform area sampling for a straight line by setting each pixel intensity pro portional to the area of overlap of the pixel with the finite width line The line can be treated as a rectangle and the section of the line area between two adja cent vertical or two adjacent horizontal screen grid lines is then a trapezoid Overlap areas for pixels are calculated by determining how much of the trape zoid overlaps each pixel in that vertical column or horizontal row  In  the pixel with screen grid coordinates  is about percent coverec by the line area so its intensity would be set to percent of the maximum ir tensity Similarly the pixel at 21 would be set to an intensity of about percent of maximum A method for estimating pixel overlap areas is illustrated by the su persampling example in  The total number of subpixels within the line boundaries is approximately equal to the overlap area and this estimation is im proved by using finer subpixel grids With color displays the areas of pixel over lap with different color regions is calculated and the final pixel color is taken as the average color of the various overlap areas  Filtering Techniques A more accurate method for antialiasing lines is to use filtering techniques The method is similar to applying a weighted pixel mask but now we imagine a con tinuous weighting surface or filter function covering the pixel  shows examples of rectangular conical and Gaussian filter functions Methods for ap plying the filter function are similar to applying a weighting mask but now we integrate over the pixel surface to obtain the weighted average intensity  lo re duce computation table lookups are commonly used to evaluate the integrals  Pixel Phasing On raster systems that can address subpixel positions within the screen grid pixel phasing can be used to antialias objects Stairsteps along a line path or ob ject boundary are smoothed out by moving micropositioning the electron beam to more nearly approximate positions specified by the object geometry Systems incorporating this technique are designed so that individual pixel positions can be shifted by a fraction of a pixel diameter The electron beam is typically shifted by  or of a pixel diameter to plot points closer to the true path of a line or object edge Some systems also allow the size of individual pixels to be ad justed as an additional means for distributing intensities  illustrates the antialiasing effects of pixel phasing on a variety of line paths  Compensating for Line intensity Differences Antialiasing a line to soften the stairstep effect also compensates for another raster effect illustrated in  Both lines are plotted with the same number of pixels yet the diagonal line is longer than the horizontal line by a factor of V2 The visual effect of this is that the diagonal line appears less bright than the hori zontal line because the diagonal line is displayed with a lower intensity per unit length A line drawing algorithm could be adapted to compensate for this effect by adjusting the intensity of each line according to its slope Horizontal and verti cal lines would be displayed with the lowest intensity while lines would be given the highest intensity But if antialiasing techniques are applied to a display      Common filter functions used to antialias line paths The volume of each filter is normalized to I and the height gives the relative weight at any subpixel position Section Antialiasing  saussian Filter  c   intensities are automatically compensated When the finite width of lines is taken into account pixel intensities are adjusted so that lines display a total intensity proportional to their length  Antialiasing Area Boundaries The antialiasing concepts we have discussed for lines can also be applied to the boundaries of areas to remove their jagged appearance We can incorporate these procedures into a scan line algorithm to smooth the area outline as the area is generated  If system capabilities permit the repositioning of pixels area boundaries can be smoothed by adjusting boundary pixel positions so that they are along the line defining an area boundary Other methods adjust each pixel intensity at a bound ary position according to the percent of pixel area that is inside the boundary In  the pixel at position x y has about half its area inside the polygon boundary Therefore the intensity at that position would be adjusted to one half its assigned value At the next position x  y  along the boundary the in tensity is adjusted to about one third the assigned value for that point Similar adjustments based on the percent of pixel area coverage are applied to the other intensity values aronnd the boundary   ad abi   Jagged lines a  plotted on the Merlin system are smoothed b with an antialiasing technique called pixel phasing This technique increases the number of addressable points on the system from X to X  Courtesy of Megatek Corp     Unequal length lines displayed with the same number of pixels in each line   Supersampling methods can be applied by subdividing the total area and determining the number of subpixels inside the area boundary A pixel partition ing into four subareas is shown in  The original by grid of pixels is turned into an by grid and we now process eight scan lines across this grid instead of four  shows one of the pixel areas in this grid that overlaps an object boundary Along the two scan lines we determine that three of the sub pixel areas are inside the boundary So we set the pixel intensity at percent of its maximum value  Another method for determining the percent of pixel area within a bound ary developed by Pitteway and Watkinson is based on the midpoint line algo tithm This algorithm selects the next pixel along a line by determining which of two pixels is closer to the line by testing the location of the midposition between the two pixels As in the Bresenham algorithm we set up a decision parameter p whose sign tells us which of the next two candidate pixels is closer to the line By slightly modifying the form of p we obtain a quantity that also gives the percent of the current pixel area that is covered by an object  We first consider the method for a line with slope m in the range from to In  a straight line path is shown on a pixel grid Assuming that the pixel at position x y  has been plotted the next pixel nearest the line at x  x is either the pixel at y or the one at y   We can determine which pixel is nearer with the calculation   Ymnia  ry    b  Y   This gives the vertical distance from the actual y coordinate on the line to the halfway point between pixels at position y and y   If this difference calcula tion is negative the pixel at y is closer to the line If the difference is positive the   vee   Adjusting pixel intensities along an x lael area boundary  Section Antialiasing  t  T I I T  Sqr qr at ara ras      A by pixel section of a raster display subdivided inta an by grid   Sean Line Scan Line Subdivided Pixel Area   A subdivided pixel area with three subdivisions inside an object boundary line   Boundary edge of an area passing through a pixel grid section  pixel at y  is closer We can adjust this calculation so that it produces a posi tive number in the range from to by adding the quantity  m  p Imy    bd  y   A  mm  Now the pixel at y is nearer if p m and the pixel at y  is nearer if p il m  Parameter p also measures the amount of the current pixel that is over lapped by the area For the pixel at x  y  in  the interior part of the pixel has an area that can be calculated as area  mx  b y    This expression for the overlap area of the pixel at x  y  is the same as that for parameter p in Eq  Therefore by evaluating p to determine the next pixel po  sition along the polygon boundary we also determine the percent of area cover  age for the current pixel  We can generalize this algorithm to accommodate lines with negative slopes and lines with slopes greater than This calculation for parameter p could then be incorporated into a midpoint line algorithm to locate pixel positions and an object edge and to concurrently adjust pixel intensities along the boundary lines Also we can adjust the calculations to reference pixel coordinates at their lower left coordinates and maintain area proportions as discussed in Section 10  At polygon vertices and for very skinny polygons as shown in  we have more than one boundary edge passing through a pixel area For these cases we need to modify the Pitteway Watkinson algorithm by processing all edges passing through a pixel and determining the correct interior area  Filtering techniques discussed for line antialiasing can also be applied to area edges Also the various antialiasing methods can be applied to polygon areas or to regions with curved boundaries Boundary equations are used to esti mate area overlap of pixel regions with the area to be displayed And coherence techniques are used along and between scan lines to simplify the calculations  SUMMARY In this chapter we have explored the various attributes that control the appear ance of displayed primitives Procedures for displaying primitives use attribute settings to adjust the output of algorithms for line generation area filling and text string displays  The basic line attributes are line type line color and line width Specifica tions for line type include solid dashed and dotted lines Line color specifica tions can be given in terms of RGB components which control the intensity of the three electron guns in an RGB monitor Specifications for line width are given in terms of multiples of a standard one pixel wide line These attributes can be applied to both straight lines and curves  To reduce the size of the frame buffer some raster systems use a separate color lookup table This limits the number of colors that can be displayed to the size of the lookup table Full color systems are those that provide bits per pixel and no separate color lookup table      pounder  Sy mix     Overlap area of a pixel rectangle centered at position x y  with the interior of a polygon area  Fill area attributes include the fill style and the mil color or the fill pattern When the fill style is to be solid the fill color specifies the color for the solid fill of the polygon interior A hollow fill style produces an interior in the background color and a border in the fill color The third type of fill is patterned In this case a selected array pattern is used to fill the polygon interior  An additional fill option provided in some packages is soft fill This fill has applications in antialiasing and in painting packages Soft fill procedures provide a new fill color for a region that has the same variations as the previous fill color One example of this approach is the linear soft fill algorithm that assumes that the previous fill was a linear combination of foreground and background colors This same linear relationship is then determined from the frame buffer settings and used to repaint the area in a new color  Characters defined as pixel grid patterns or as outline fonts can be dis played in different colors sizes and orientations To set the orientation of a char acter string we select a direction for the character up vector and a direction for the text path In addition we can set the alignment of a text string in relation to the start coordinate position Marker symbols can be displayed using selected characters of various sizes and colors  Graphics packages can be devised to handle both unbundled and bundled attribute specifications Unbundled attributes are those that are defined for only one type of output device Bundled attribute specifications allow different sets of attributes to be used on different devices but accessed with the same index num ber in a bundle table Bundle tables may be installation defined user defined or both Functions to set the bundle table values specify workstation type and the attribute list for a given attribute index  To determine current settings for attributes and other parameters we can invoke inquiry functions In addition to retrieving color and other attribute infor mation we can obtain workstation codes and status values with inquiry func tions  Because scan conversion is a digitizing process on raster systems displayed primitives have a jagged appearance This is due to the undersampling of infor mation which rounds coordinate values to pixel positions We can improve the appearance of raster primitives by applying antialiasing procedures that adjust pixel intensities One method for doing this is to supersample That is we con sider each pixel to be composed of subpixels and we calculate the intensity of the     Polygons with more than one boundary line passing through individual pixel regions  Chapter Attributes of Output Primitives  subpixels and average the values of all subpixels Alternatively we can perform area sampling and determine the percentage of area coverage for a screen pixel then set the pixel intensity proportional to this percentage We can also weight the subpixel contributions according fo position giving higher weights to the central subpixels Another method for antialiasing is to build special hardware configurations that can shift pixel positions  Table lists the attributes discussed in this chapter for the output primi tive classifications line fill area text and marker The attribute functions that can be used in graphics packages are listed for each category    TABLE SUMMARY OF ATTRIBUTES Output Bundled Primitive Associated Attribute Setting Attribute Type Attributes Functions Functions Line Type setLinetype setFolylineIndex Width setLineWidthScaleFactor setPolylineRepresentation Color setPolylineColourIndex Fiil Area Fill Style setInteriorStyle setInteriorIndex Fill Color setInteriorColorIndex setInteriorRepresentation Pattern setInteriorStyleIndex setPatternRepresentation setPatternSize setPatternReferencePoint Text Font setTextFont setText Index Color setTextColourIndex setTextRepresentation Size setCharacterHeight setCharacterExpansionFactor Orientation setCharacterUpVector setText Path setTextAlignment Marker Type setMarkerType set PolymarkerIndex Size setMarkerSizeScaleFactor set PolymarkerRepresentation Color set PolymarkerColour Index REFERENCES Calor and grayscale considerations are discussed in Crow  and in Heckbert   Soft fill techniques are given in Fishkin and Barsky   Antialiasing techniques are discussed in Pitteway and Watkinson  Crow  Turkowski  Korein and Badler  and Kirk and Avro Schilling and Wu   Attribute functions in PHIGS are discussed in Howard et al   Hopgood and Duce  Gaskins  and Blake  For information an GKS workstations and attrib utes see Hapgood et al  and Enderle Kansy and Pfaff   EXERCISES  Implement the line type function by modifying Bresenham s line drawing algorithm to display either solid dashed or dotted lines     18  20 21  Implemeni the line type function with a midporn line algcrithm to display either solid dashed or dotted lines  Devise a parallel method for implementing the line type function Devise a parallel method for implementing the line width function  A tine specified by two endpoints and a width can be converted to a rectangular poly gon with four vertices and then displayed using a scan line method Develop an effi cient algorithm for computing the four vertices needed to define such a rectangle using the line endpoints and line width  Implement the line width function in a line drawing arogram so that any one of three line widths can be displayed  Write a program to output a line graph of three data sets defined over the same x coor  dinate range Input to the program is to include the three sets of data values labeling for the axes and the coordinates for the display area on the screen The data sets are to be scaled to iit the specified area each plotted line is to be displayed in a differert line type solid dashed dotted  and the axes are to be labeled  Instead of changing the line type the three data sets can be piotted in different colors   Set up an algorithm for displaying thick tines with either butt caps round caps oF pro jecting square caps These options can be provided in an option menu  Devise an algorithm for displaying thick polylines with either a miter join a round join or a bevel join These options can be provided i an option menu   Implement pen and brush menu options for a line drawing procedure including at leasi two options round and square shapes   Madity a line drawing algorithm so that the intensity of the ourput line is set according to its slope That is by adjusting pixel intensities according to the value of the slope all Ines are displayed with the same intensity per unit length  Define and tmplement a function for controlling the line type tsolid dashed dotted of displayed ellipses   Define and implement a function for setting the width of displayed ellipses Write a routine to display a bar graph in anv spec fied screen area Input is to include the data set labeling for the coordinate axes and ths coordinates for the screen area The data set is to be scaled to fit the designated screen area and the bars are to be dis played in designated colors or patterns   Write a procedure to display two data sets defined cver the same x coordinate range  with the data values scaled to ft a specified region of the display screen The bars for one of the data sets are to be displaced horizontally to produce an overlapping bar pattern foc easy Comparison of the two sets of data iJse a different color or a different fill pattern for the two sets of bars   Devise an algorithm for implementing a color lookup table and the set ColourRep  resentation operation  Suppose you have a system with an inch by inch video screen that can display pixels per inch If a cclor lookup table with positions is used with this system what is the smallest possible size in bytes for the frame buffer  Consider an RGB raster system that has a by frame buffer with a bits per pixel and a color lookup table with bits per pixe  a How many distinct gray lev els can be displayed with this system  b How many distinct colors including gray levels can be displayed  c How many colors cai be displayed at any one time  d What is the total memory size  e Explain two methods for reducing memory size while maintain ng the same color capabilities  Modify the scan line algorithm to apply any specified rectangular fill pattern to a poly  gon interior starting from a designated pattern position Write procedure to fill the interior of a given ellipse with a specified pattern Write a procedure to implement the set Pat terrRepresentat ion function  Exercises  Chapler Atuributes of Output Primitives  22 23  24  27 28  23  32 33  34 35   Define and implement a procedure for changing the size of an existing rectangular till pattern Write a procedure to implement a soft fill algorithm Carefully define what the soft fill algorithm is to accomplish and how colors are to be combined  Devise an algorithm or adjusting the height and width of  haracters defined as rectan gular grid patterns  Implement routines for setting the character up vector and the text path for controlling the display of character strings  Write a program to align text as specified by input values for the alignment parame  ters  Develop procedures jor implementing the marker attribute functions Compare attribute implementation procedures needed by systems that employ bun  dled attributes to those needed by systems using unbundicd attributes  Develop procedures or storing and accessing attributes in unbundled system attribute tables The procedures are to be designed to store designated attribute values in the system tables to pass attributes to the appropriate output routines and to pass attrrb  utes to memory locations specified in inquiry Commands  Set up the same procedures described in he previous exercise for bundled system at tribute tables   Implement an antialiasing procedure by extending Bresenham s line algorithm to ad  just pixel intensities in the vicinity of a line path  Implement an antialiasing procedure far the midpoint line algorithm  Develop an algorithm for antialiasing elliptical boundaries  Modify the scan line algorithm for area fill ta incorporate antialiasing Use coherence techniques to reduce calculations on successive scan lines Write a program to implement the Pitteway Watkinson artialiasing algorithm as a scan line procedure to fill a polygon interior Use the routine setPixel x y intensity to load the intensity value into the frame buffer at location x y   CHAPTER   Two Dimensional  Geometric Transformations  Final   Position ew  a rf  ra      ee      W ith the procedures for displaying output primitives and their attributes we can create a variety of pictures and graphs In many applications there is also a need for altering or manipulating displays Design applications and facility layouts are created by arranging the orientations and sizes of the component parts of the scene And animations are produced by moving the camera or the objects in a scene along animation paths Changes in orientation size and shape are accomplished with geometric transformations that alter the coordinate descriptions of objects The basic geometric transformations are trans lation rotation and scaling Other transformations that are often applied to ob jects include reflection and shear We first discuss methods for performing geo metric transformations and then consider how transformation functions can be incorporated into graphics packages  Here we first discuss general procedures for applying translation rotation and scaling parameters to reposition and resize two dimensional objects Then in Section we consider how transformation equations can be expressed in a more convenient matrix formulation that allows efficient combination of object transformations  Translation A translation is applied to an object by repositioning it aiong a straight line path from one coordinate location to another We translate a two dimensional point by adding translation distances t and t  to the original coordinate position x y to move the point to a new position x  y     xo axtt v ytt   The translation distance pair t  t  is called a translation vector or shift vector  We can express the translation equations as a single matrix equation by using column vectors tc represent coordinate positions and the translation vec tor p  p  et T   xX Xx ty This allows us to write the two dimensional translation equations in the matrix form  P P T  Sometimes matrix transformation equations are expressed in terms of coordinate row vectors instead of column vectors In this case we would write the matrix representations as P  x y and T  i t  Since the column vector representa tion for a point is standard mathematical notation and since many graphics packages for example GKS and PHIGS also use the column vector representa tion we will follow this convention  Translation is a rigid body transformation that inoves objects without defor mation That is every point on the object is translated by the same amount A straight line segment is translated by applying the transformation equation to each of the line endpoints and redrawing the line between the new endpoint po sitions Polygons are translated by adding the translation vector to the coordinate position of each vertex and regenerating the polygon using the new set of vertex coordinates and the current attribute settings  illustrates the applica tion of a specified translation vector to move an object from one position to an other  Similar methods are used to translate curved objects To change the position of a circle or ellipse we translate the center coordinates and redraw the figure in the new location We translate other curves for example splines by displacing the coordinate positions defining the objects then we reconstruct the curve paths using the translated coordinate points  Ve yo      att H H  Moving a polygon from position a 10 20   to position b with the translation ib vector  50 75   Translating a point from position P to position P with translation vector T    Rotation of an object through angle  about the pivot point x y    Rotation of a point from position x y to position x  y through an angle  telative to the coordinate origin The original angular displacement of the point from the x axis is   Rotation A two dimensional rotation is applied to an object by repositioning it along a cir cular path in the xy plane To generate a rotation we specify a rotation angle  and the position x  y  of the rotation point or pivot point about which the ob ject is to be rotated   Positive values for the rotation angle define coun terclockwise rotations about the pivot point as in  and negative values rotate objects in the clockwise direction This transformation can alsa be de scribed as a rotation about a rotation axis that is perpendicular to the xy plane and passes through the pivot point  We first determine the transformation equations for rotation of a point posi tion P when the pivot point is at the coordinate origin The angular and coordi nate relationships of the original and transformed point positions are shown in  In this figure r is the constant distance of the point from the origin angle  is the original angular position of the point from the horizontal and  is the ro tation angle Using standard trigonometric identities we can express the trans formed coordinates in terms of angles and  as   x rcos    rcos cos rsindsin d   y  rsin   rcos sin   rsin dos The original coordinates of the point in polar coordinates are x rcos  versing  Substituting expressions into we obtain the transformation equations for rotating a point at position x y through an angle about the origin  x xcos  ysin  y xsind ycos   With the column vector representations for coordinate positions we can write the rotation equations in the matrix form  P R P  where the rotation matrix is cos  sin    sin  cos  When coordinate positions are represented as row vectors instead af col umn vectors the matrix product in rotation equation is transposed go that the transformed row coordinate vector x y  is calculated as PT  R P T  pr R where P   x y  and the transpose R of matrix R is obtained by interchanging rows and columns For a rotation matrix the transpose is obtained by simply changing the sign of the sine terms Rotation of a point about an arbitrary pivot position is illustrated in  Using the trigonometric relationships in this figure we can generalize Eqs to obtain the transformation equations for rotation of a point about any specified ro tation position x  y   x  x   v x cos  y y  sin d wiry t G v sin  yy cos   These general rotation equations differ from Eqs by the inclusion of additive terms as well as the multiplicative factors on the coordinate values Thus the matrix expression could be modified to include pivot coordinates by matrix addition of a column vector whose elements contain the additive translational terms in Eqs  There are better ways however to formulate such matrix equa tions and we discuss in Section a more consistent scheme for representing the transformation equations  As with translations rotations are rigid body transformations that move objects without deformation Every point on an object is rotated through the same angle A straight line segment is rotated by applying the rotation equations to each of the line endpuints and redrawing the line between the new end point positions Polygons are rotated by displacing each vertex through the speci fied rotation angle and regenerating the polygon using the new vertices Curved lines are rotated by repositioning the defining points and redrawing the curves A circle or an cllipse for instance can be rotated about a noncentral axis by mov ing the center position through the are that subtenas the specified rotation angle An ellipse can be rotated about its center coordinates by rotating the major and minor axes  Sealing A scaling transformation alters the size of an object This operation can be car ried out for polygons by multiplying the coordinate values x y of each vertex by scaling factors s and s to produce the transformed coordinates x  y   NOS xX S yorysy 10  Scaling factor s scales objects in the x direction while s scales in the y direction The transformation equations 10 can also be written in the matrix form  iy   e 11 or P S P 12  where S is the by scaling matrix in Eq 11  Any positive numeric values can be assigned to the scaling factors s and s Values less than reduce the size of objects values greater than produce an en largement Specifying a value of for both s and s leaves the size of objects un changed When s and s are assigned the same value a uniform scaling is pro  Section Basic Transformations  S igure Rotating a point from position x y to position ix y  through an angle  about rotation point x  y   Chapter Two Dimensional Geometric Transformations  a  b  Turning a square a into a rectangle b with scaling factors s  2ands     A line scaled with Eq 12 using  s  is reduced in size and moved closer to the coordinate ongin  Pp    Scaling relative to a chosen fixed point  y  Distances from each polygon vertex to the fixed point are scaled by transformation equations 13  duced that maintains relative object proportions Unequai values for s and s re sult in a differential scaling that is often used in design applications where pic tures are constructed fram a few basic shapes that can be adjusted by scaling and positioning transformetions    Objects transformed with Eq 11 are both scaled and repositioned Scaling factors with values less than move objects closer ta the coordinate origin while values greater than move coordinate positions farther from the origin Figure illustrates scaling a line by assigning the value toa both s and sy in Eq 11 Both the line length and the distance from the origin are reduced by a factor of  We can control the location of a scaled object by choosing a position called the fixed point that is to remain unchanged after the scaling transformation Co ordinates for the fixed point x y can be chosen as one of the vertices the object centroid or any other position   A polygon is then scaled relative to the fixed point by scaling the distance from each vertex to the fixed point For a ver tex with coordinates x y  the scaled coordinates x  y  are calculated as xi say t x  xpsy y yy  ly  ups  13  We can rewrite these scaling transformations to separate the multiplicative and additive terms  xo x s t a1 s  14  yo yes  yl  sy where the additive terms x  s  and y  s  are constant for all points in the object  including coordinates for a fixed point in the scaliny equations is similar to including coordinates for a pivot point in the rotation equations We can set up a column vector whose elements are the constant terms in Eqs 14 then we add this column vector to the product S  P in Eq 12 In the next section we discuss a matrix formulation for the transformation equations that involves only matrix multiplication  Polygons are scaled by applying transformations 14 to each vertex and then regenerating the polygon using the transformed vertices Other objects are scaled by applying the scaling transformation equations to the parameters defin ing the objects An ellipse in standard position is resized by scaling the semima jor and semiminor axes and redrawing the ellipse about the designated center co ordinates Uniform scaling of a circle is done by simply adjusting the radius Then we redisplay the circle about the center coordinates using the transformed radius  COORDINATES  Many graphics applications involve sequences of geometric transformations An animation for example might require an object to be translated and rotated at each increment of the motion In design and picture construction applications we perform translations rotations and scalings to fit the picture components into their proper posihons Here we consider how the matrix representations dis cussed in the previous sections can be reformulated so that such transformation sequences can be efficiently processed  We have seen in Section that each of the basic transformations can be ex pressed in the general matrix form P M P  M  15  with coordinate positions P and P represented as column vectors Matrix M isa by array containing multiplicative factors and M is a two element column Matrix containing translational terms For translation Mj is the identity matrix For rotation or scaling M contains the translational terms associated with the pivot point or sealing fixed point To produce a sequence of transformations with these equations such as scaling followed by rotation then translation we must calculate the transformed coordinates one step at a time First coordinate posi tions are scaled then these scaled coordinates are rotated and finally the rotated coordinates are translated A more efficient approach would be to combine the transformations so that the final coordinate positions are obtained directly from the initial coordinates thereby eliminating the calculation of intermediate coordi nate values To be able to do this we need to reformulate Eq 15 to eliminate the matrix addition associated with the translation terms in Mp  We can combine the multiplicative and translational terms for two dimen sional geometric transformations into a sing e matrix representation by expand ing the by matrix representations to by matrices This allows us to express all transformation equations as matrix multiplications providing that we also ex pand the matrix representations for coordinate positions To express any two di mensional transformation as a matrix multiplication we represent each Cartesian coordinate position Gr y with the homogeneous coordinate triple x  yy fi  where yooh ya  hus a gencral homogeneous coordinate representation can also be written as h x Ay h  For two dimensional geometric transformations we can choose the ho mogeneous parameter h to be any nonzero value Thus there is an infinite num ber of equivalent homogeneous representations for each coordinate point x y  A convenient choice is simply to set h  Each two dimensional position is then represented with homogeneous coordinates x y  Other values for parameter are needed for example in matrix formulations of three dimensioral viewing transformations  The term omogeneous courdmatrs is used in mathematics to refer to the ef tect of this representation on Cartesian equations When a Cartesian point x y is converted to a homogeneous representation x  yy  equations containing x and y such as ffx y   become homogeneous equations in the three parame lers x  y  and h This just means that if each of the three parameters is replaced by any value v times that parameter the value  can be factored out of the equa tions  Expressing positions in homogeneous coordinates allows us to represent all geometric transformation equations as matrix multiplications Coordinates are Section Matrix Representations and 4omageneous Coordinates Chapter Two Dimensional Geometric Transformations represented with three element column vectors and transformation operations are written as by matrices For translation we have x f  x yypaTo y y 17 00 which we can write in the abbreviated form P TCt t  P 18  with T t  t  as the by translation matrix in Eq 17 The inverse of the trans lation matrix is obtained by replacing the translation parameters t and t with their negatives  t and t  Similarly rotation transformation equations about the coordinate origin are now written as  x cos  sind x y J  sing cos Of  y 19 1yqi or as P  R P 20  The rotation transformation operator R 1s the by matrix in Eq 19 with rotation parameter We get the inverse rotation matrix when  is replaced with   Finally a scaling transformation relative to the coordinate ongin is now ex pressed as the matrix multiplication x s 90 yi d O s O  y 21  oc or P  Sis s  P 22  where S s  s  is the by matrix in Eq 21 with parameters s and s  Replac ing these parameters with their multiplicative inverses s and s  yields the inverse scaling matrix  Matrix representations are slandard methods for implementing transforma tions in graphics systems In many systems rotation and scaling functions pro duce transformations with respect to the coordinate origin as in Eqs 19 and 21 Rotations and scalings relative to other reference positions are then handled as a succession of transformation operations An alternate approach in a graphics package is to provide parameters in the transformation functions for the scaling fixed point coordinates and the pivot point coordinates General rotation and scaling matrices that include the pivot or fixed point are then set up directly without the need to invoke a succession of transformation functions  With the matrix representations of the previous section we can set up a matrix for any sequence of transformations as a composite transformation matrix by calculating the matrix product of the individual transformations Forming prod ucts of transformation matrices is often referred to as a concatenation or compo sition of matrices For column matrix representation of coordinate positions we form composite transformations by multiplying matrices in order from right to left That is each successive transformation matrix premultiplies the product of the preceding transformation matrices  Translations If two successive translation vectors f ty  and   are applied to a coordi nate position P the final transformed location P is calculated as Phe Tila ta ATC ty  PI  AT ty ta Tay tat P where P and P are represented as homogeneous coordinate column vectors We can verify this resuJt by calculating the matrix product for the two associative groupings Also the composite transformation matrix for this sequence of trans lations is te ty 00 ty  te ty Of ty f fO tatty  o C0 OT T x ty Tet fed  Thy  te fi  fy  25 which demonstrates that two successive translations are additive Rotations Two successive rotations applied to point P produce the transformed position P  R    R    P   R   ROBE P 20  By multiplying the two rotation matrices we can verify that two successive rota tions are additive  R A   R G   RCA  ey  275 so that the final rotated coordinates can be calculated with the composite rotation matrix as P R   P  28  Section Composite Transformations Chapter Two Dimensional Geometric Transformations  x    a  Scalings Concatenating transformation matrices for two successive scaling operations pro duces the following composite scaling matrix  s2 O Sx Sx sg Of  sy O  Sy Sp 29 ee  or S2 Sy2  Sey Sp  S Sx1  Sens Sy1  Sya  30  The resulting matrix in this case indicates that successive scaling operations are multiplicative That is if we were to triple the size of an object twice in succes sion the fina size would be nine times that of the original  General Pivot Point Rotation With a graphics package that only provides a rotate function for revolving objects about the coordinate origin we can generate rotations about any selected pivot point x  y  by performing the following sequence of translate rotate translate operations  Translate the object so that the pivot point position is moved to the coordi nate origin  Rotate the object about the coordinate origin  Translate the object so that the pivot point is returned to its original posi tion  This transformation sequence is illustrated in  The composite transforma  ate Yel   fb c   id  Original Powinwn Tranelateon of Rotaton Transition or Object and Object so that about Object so that Pivot Point Pivot Point Origin the Pivot Point Ox y b ts at a Retumed Origin to Position x y   A transformation sequence for rotating an object about a specified pivot point using the rotation matrix R  of transformation 19  tion matrix for tlus sequence is obtained with the concatenation Section Composite Transformations x cos sin 0O x yy   sine cos OFF CO y  90 1 06 l cosf sin x1  cos  y sin  sind cos vil cos  x sin  3h   which can be expressed in the form Tix uy RCO  T x  y   RG y  32  where T  x  y   T  x  y  In general a rotate function can be set up to ac cept parameters for pivot point coordinates as well as the rotation angle and to generate automatically the rotation matrix of Eq 31  General Fixed Point Scaling  illustrates a transformation sequence tc produce scaling with respect ta a selected fixed position a y  using a scaling function that can only scale rela  ive to the coordinate origin  Translate object so that the fixed point coincides with the coordinate origin  Seale the object with respect to the coordinate origin  Use the inverse translation of step to return the object to its original posi tion  Concatenating the matrices for these three operations produces the required scal  ing matrix x s O x s O x  ool y  Sy Q Y  Sy yl  sy  331 1 oc 1 or Thy  p Sls   Tx  Yy  SC Up  Sy  This transiormation is automatically generated on systems that provide a scale function that accepts coordinates for the fixed point  General Scaling Directions Parameters s and s scale objects along the x and y directions We can scale an ob ject in other directions by rotating the object to align the desired scaling direc tions with the coordinate axes before applying the scaling transformation Suppose we want to apply scaling factors with values specified by parame ters s and s in the directions shown in  Ta accomplish the scaling with  ix vy   tat ibe 1c ids Ongmal Poston Transiate Object Scale Object Translate Object of Object and that Fixed Point with Respect so thot the Fixed Point Fixed Point x y  te at Origin to Origin ta Returned to Pasition x  y   A transformation sequence for scaling an object with respect to a specified fixed position using the scaling matrix S s  s  of transformation 21    Scaling parameters s and  are to be applied in orthogonal directions defined by the angular displacement  out changing the orientation of the object we first perform a rotation so that the directions for s and s coincide with the x and y axes respectively Then the scal ing transformation is applied followed by an opposite rotation to return points to their original orientations The composite matrix resulting from the product of these three transformations is R    S s    RCA   cos  s sin  s  s cos 6sin   Ss  cos sin  s sin  s cos  35  As an example of this scaling transformation we turn a unit square into a parallelogram   by stretching it along the diagonal from   to  D We rotate the diagonal onto the y axis and double its length with the transforma tion parameters    s   and   In Eq 35 we assumed that scaling was to be performed relative to the ori gin We could take this scaling operation one step further and concatenate the matrix with translation operators so that the composite matrix would include parameters for the specification of a scaling fixed position  Concatenation Properties Matrix multiplication is associative For any three matrices A B and C the ma trix product A  B  C can be performed by first multiplying A and B or by first multiplying B and C  A B C A B C A B  36  Therefore we can evaluate matrix products using either a left to right or a right to left associative grouping  On the other hand transformation products may not be commutative The matrix product A  B is not equal to B  A in general This means that if we want    Wu    e ool   fal tbl   A square a is converted to a parallelogram b using the composite transformation matrix 35 with s    and    to translate and rotate an object we must be careful about the order in which the composite matrix is evaluated   For some special cases such as a se quence of transformations all of the same kind the multiplication of transforma tion matrices is commutative As an example two successive rotations could be performed in either order and the final position would be the same This commu   tative property holds also for two successive translations or two successive scal ings Another commutative pair of operations is rotation and uniform scaling s  s   General Composite Transformations and Computational Efficiency A general two dimensional transformation representing a combination of trans lations rotations and scalings can be expressed as x 1S TSyy ETS x T  nn Sy ts   y 37 1 The four elements rs are the multiplicative rotation scaling terms in the transfor mation that involve only rotation angles and scaling factors Elements frs and irs are the translational terms containing combinations of translation distances pivot point and fixed point coordinates and rotation angles and scaling parame ters For example if an object is to be scaled and rotated about its centroid coordi nates x  y  and then translated the values for the elements of the composite transformation matrix are Tey fy  ROC Yor  SOG Yer Ser Sy  s cos6 s sin x  s cos   ys sin t   s sin sycos y s cos   x s sin O ty 38  Although matrix equation 37 requires nine multiplications and six addi tions the explicit calculations for the transformed coordinates are Section Composite Transformations Chapter  Two Dimensional Geometric Transformations   cA a Se v Final  Final   Position Position oN a oN res cy oo poet    tii4  eee a  b  Reversing the order in which a sequence of transformations 1s performed may affect the transformed position of an object In  an object is first translated then rotated In b  the object is rctated first then translated XS XS AW PS yt ETSY YoFXy TY TS  Ersy iS i Thus we actually only need to perform four multiplications and four additions to transform coordinate positions This is the maximum uumber of computations required for any transformation sequence once the individual matrices have been concatenated and the elements of the composite matrix evaluated Without concatenation the indiv dual transformations would be applied one at a time and the number of calculations could be significantly increased An efficent im plementation for the transformation operations therefore is tv formulate trans formation matrices concatenate any transformation sequence and calculate transformed coordinates using Eq 39 On parallel systems direct matrix multr plications w th the composite transformation matrix of Ec 37 can be equally ef ficient  A general rigid body transformation matrix involving only translations and rotations can be expressed in the form Pe Vy ir   4EN aly thy  Oo where the four elements r are the multiplicative rotation terms and elements tr and tr are the translational terms A rigid body change in coordinate position is also sometimes referred to as a rigid motion transformation All angles and dis tances between coordinate positions are unchanged by the transformation In ad dition matrix 40 has the property that its upper left bv submatrix is an or thogonal matrix This means that if we consider each row of the submatrix as a vector then the two vectors     and   ry form an orthogonal set of unit y vectors Each vector has unit length Boa pk pk ra try eri tea   a u Hy te and the vectors are perpendicular their dot product is  Kalyx  Taylry  G Therefore if these unit vectors are transformed by the rotation submatrix  r  Psy is converted to a unit vector along the x axis and r  ry is transformed into a unit vector along the y axis of the coordinate system  Te Ty Nyy Ty ty Of try   43 1 hy Ty Ty yz tw Ol ry Pall 44 1 As an example the following rigid body transformation first rotates an object through an angle  about a pivot point x  y  and then translates  T t    Rex y  cos  sin x  cos   y sin  ty  sin cos y cos6 x sin  ty 45  Here orthogonal unit vectors in the upper left by submatrix are cos  sin and sin cos  and cos  sin  cos sin cos O   sn    46 1 Similarly unit vector sin cos  is converted by the transformation matrix in Eq 46 to the unit vector  in the y direction  The orthogonal property of rotation matrices is useful for constructing a ro tation matrix when we know the final orientation of an object rather than the amount of angular rotation necessary to put the object into that position Direc tions for the desired orientation of an object could be determined by the align ment of certain objects in a scene or by selected positions in the scene  shows an object that is to be aligned with the unit direction vectors u and v  As suming that the original object orientation as shown in  a  is aligned with the coordinate axes we construct the desired transformation by assigning the elements of u to the first row of the rotation matrix and the elements of v to the second row This can be a convenient method for obtaining the transforma tion matrix for rotation within a local or object  coordinate system when we know the final orientation vectors A similar transformation is the conversion of object descriptions from one coordinate system to another and in Section we consider how to set up transformations to accomplish this coordinate conversion  Since rotation calculations require trignometric evaluations and several multiplications for each transformed point computational efficiency can become an important consideration in rotation transformations In animations and other applications that involve many repeated transformations and small rotation an gles we can use approximations and iterative calculations to reduce computa  Section Composite Transformations  Chapter Two Dimensional Geometric Transformations   The rotation matrix for revolving an object from position a ta position b can be constructed with the values cf the unit orientation vectors u and v relative to the original orientation  tions in the composite transformation equations When the rotation angle is small the trigonometric functions can be replaced with approximation values based on the first few terms of their power series expansions For small enough angles less than  cos  is approximately and sin  has a value very close to the value of  in radians  f we are rotating in small angular steps about the ori gin for instance we can set cos  to and reduce transformation calculations at each step to two multiplications and two additions for each set of coordinates to be rotated  xo x ysing y xsin y 47  where sin is evaluated once for all steps assuming the rotation angle does nat change The error introduced by this approximation at each step decreases as the rotation angle decreases But even with small rotation angles the accumulated error over many steps can become quite large We can control the accumulated error by estimating the error in x and y at each step and resetting object posi tions when the error accumulation becomes too great  Composite transformations often involve inverse matrix calculations Trans formation sequences for general scaling directions and for reflections and shears Section  for example can be described with inverse rotation components As we have noted the inverse matrix representations for the basic geometric frans formations can be generated with simple procedures An inverse translation ma trix is obtained by changing the signs of the translation distances and an inverse rotation matrix is obtained by performing a matrix transpose or changing the sign of the sine terms  These operations are much simpler than direct inverse matrix calculations  An implementation of composite transformations is given in the following procedure Matrix M is initialized to the identity matrix As each individual transformation is specified it is concatenated with the total transformation ma trix M When all transformations have been specified this composite transforma tion is applied to a given object For this example a polygon is scaled and rotated about a given reference point Then the object is translated  shows the original and final positions of the polygon transformed by this sequence Section Composite Transformations 100 200 x 100 200 x  a  oj   polygon a is transformed into b by the composite operations in the following procedure   include math h  include graphics h  typedef float Matrix3x3   Matrix3x3 theMatrix  void matrix3x3SetIdentity Matrix3x3 m  int  for i i i  for j J j  m il j  Gi     Multiplies matrix a times b putting result in b  void matrix3x3PreMultiply Matrix3x3 a Matrix3x3 b   int r c  Matrix3x3 tmp  for ry  r  r  for c  c  c Ht tmp r ic  afr  O bl O c  afr  i bll fic  alr  bl2 fel  for r  x  r  for c     c  b rife  tmp r cl   void translate2 int tx int ty  Matrix3x3 m  matrix3x3Settdentity m  m Oj  tx  mf1   ty  Matrix3x3PreMultiply m theMatrix    void scale2  loat sx oat sy wePt refpt   Matrix3x3 m  matrix3x3SecIdentity m  m O   sx m O    sx  refpt x m l ll  sy  m     sy  refpt y Matrix3x3PreMultiply m theMatrix   void rotate2 float a wcePt2 refPr   Matrix3x3 m  matrix3x3SetIdentity m  a  pToRadians a  m Q  O01  cosf a  m  sint a  m  refPpt x   cosf a   refPt y  sinf a  m   sinf a  m   cost a  m z  refpt y   cosf a   refPt x  sinf a  Matrix3x3PreMu tiply m theMatrix    void transfurmPoints2 int npts wePt2 pts  int k float tmp for k  k  npts k   tmp  cthoeMatrix   pts k x  theMatrix   pts k y  theMatrix   prs k y  theMatrix   pts x x  theMatrix  pts k  y  UheMatrix  pts k x  tmp    void main int arge char  argv  C wePt2 pts       wePt2 refPt    long windowIN  openGraphics  azgv   setBacxground WHITE  setColor BLUE  pFillArea  pts  Matrix3x3SetIidentity theMatrix  scalez  refPt  rotate2  refPt  translate2   transformPoints2  pts  pFillaArea pts  sleep   CloseGraphics windowID     Basic transformations such as translation rotation and scaling are included in most graphics packages Some packages provide a few additional transforma tions that are useful in certain applications Two such transformations are reflec tion and shear  Reflection A reflection is a transformation that produces a mirror image of an object The mirror image for a two dimensional reflection is generated relative to an axis of reflection by rotating the object about the reflection axis We can choose an axis of reflection in the xy plane or perpendicular to the xy plane When the re flection axis is a line in the xy plane the rotation path about this axis is in a plane perpendicular to the xy plane For reflection axes that are perpendicular to the xy plane the rotation path is in the xy plane Following are examples of some com mon reflections  Reflection about the line y  the x axis is accomplished with the transfor mation matrix 0  48  This transformation keeps x values the same but flips the y values of coordi nate positions The resulting orientation of an object after it has been reflected about the x axis is shown in  To envision the rotation transformation path for this reflection we can think of the flat object moving out of the xy plane and rotating through three dimensional space about the x axis and back into the xy plane on the other side of the x axis  A reflection about the y axis flips x coordinates while keeping y coordinates the same The matrix for this transformation is  01 49   illustrates the change in position of an object that has been reflected about the line x  The equivalent rotation in this case is through three di mensional space about the y axis  We flip both the x and y coordinates of a point by reflecting relative to an axis that is perpendicular to the xy plane and that passes through the coordinate origin This transformation referred to as a reflection relative to the coordinate origin has the matrix representation   50  Section Other Transformations  Original Position Reflected Position   Reflection of an object about the x axis  Original Reflected Position Pasition Py   Reflection of an object about the y axis   y Reflected Position v ed  x   XN   Md Original Position  Reflection of an object relative to an axis perpendicular to the ry plane and passing through the coordinate origin  you  Original  Position ro   a  PY  Sea  Reflected Pos tion f      Reflection of an object with respect to the line y  x   y y Pra aa ween eee o71 JZ     Ve e  ee Xap x  Reflection of an object relative to an axis perpendicular to the xy plane and passing through point P   An example of reflection about the origin is shown in  The reflection ma trix 50 is the rotation matrix R  with   We are simply rotating the ob ject in the ry plane half a revolution about the origin  Reflection 50 can be generalized to any reflection point in the xy plane   This reflection is the same as a rotation in the xy plane using the reflection point as the pivot point  If we chase the reflection axis as the diagonal line y  x   the re flection matrix is ors eou  oO a ey a We can derive this matrix by concatenating a sequence of rotation and coordi nate axis reflection matrices One possible sequence is shown in  Here we first perform a clockwise rotation through a angle which rotates the line y  x onto the x axis Next we perform a reflection with respect to the x axis The final step is to rotate the line y  x back to its original position with a counter clockwise rotation through  An equivalent sequence of transformations is first to reflect the object about the x axis and then to rotate counterclockwise  To obtain a transformation matrix for reflection about the diagonal y  x we could concatenate matrices for the transformation sequence  clockwise ro tation by   reflection about the y axis and  counterclockwise rotation by  The resulting transformation matrix is Qo 0 32  shows the original and final positions for an object transformed with Section this reflection matrix Other Transformations Reflections about any line y  mx  in the xy plane can be accomplished with a combination of translate rotate reflect transformations In general we first  translate the line so that it passes through the origin Then we can rotate the line ne onto one of the coordinate axes and reflect about that axis Finally we restore the line to its original position with the inverse rotation and translation transforma  tions We can implement reflections with respect to the coordinate axes or coordi  nate origin as scaling transformations with negative scaling factors Also ele ments of the reflection matrix can be set to values other than  Values whose magnitudes are greater than shift the mirror image farther from the reflection fa axis and values with magnitudes less than bring the mirror image closer to the reflection axis y Shear  I A transformation that distorts the shape of an object such that the transformed __   shape appears as if the object were composed of internal layers that had been caused to slide over each other is called a shear Two common shearing transfor Gg mations are those that shift coordinate x values and those that shift y values  An x direction shear relative to the x axis is produced with the transforma tion matrix 10 53 ov which transforms coordinate positions as   x x sh y yay 54  c Any real number can be assigned to the shear parameter sh A coordinate posi poe tion x y is then shifted horizontally by an amount proportional to its distance y Sequence of transformations value from the x axis y   Setting sh to for example changes the square in to produce reflection about  into a parallelogram Negative values for sh shift coordinate positions  the line y  x  a cleckwise   to the left rotation of  b reflection We can generate x direction shears relative to other reference lines with about the x axisrand c counterclockwise rotation sh  Shy Yet by  55 60  with coordinate positions transformed as x x  SHY  Yres  yoy 56  An example of this shearing transformation is given in  for a shear para meter value of relative to the line y      Reflected Position  Original Position  Reflection with respect to the line y  x       a  bd  OF   A unit square a is converted to a parallelogram b using the x direction shear matrix 53 with sh   A y direction shear relative to the line x  formation matrix Xi is generated with the trans  sh  shy Xyep 57  which generates transformed coordinate positions wos x yl sh kx  Xe  y 58  This transformation shifts a coordinate position vertically by an amount propor tional to its distance from the reference line x  x   illustrates the conversion of a square into a parallelogram with sh  and x    Shearing operations can be expressed as sequences of basic transformations The x direction shear matrix 53 for example can be written as a composite transformation involving a series of rotation and scaling matrices that would scale the unit square of  along its diagonal while maintaining the origi nal lengths and orientations of edges parallel to the x axis Shifts in the positions of objects relative to shearing reference lines are equivalent to translations    D       Yar  Yer    a  b   A unit square a is transformed toa shifted parallelogram b with sh  and y q  1in the shear matrix 55   y      a1   xy   x Ng ea x a  b  A unit square a is turned into a shifted parallelogram b with parameter values sh  and x   in the y direction using shearing transformation 57   Graphics applications often require the transformation of object descriptions from one coordinate system to another Sometimes objects are described in non Cartesian reference frames that take advantage of object symmetries Coordinate descriptions in these systems must then be converted to Cartesian device coordi nates for display Some examples of two dimensional non Cartesian systems are polar coordinates elliptical coordinates and parabolic coordinates In other cases we need to transform between two Cartesian systems For modeling and design applications individual objects may be defined in their own local Carte sian references and the local coordinates must then be transformed to position the objects within the overall scene coordinate system A facility management program tor office layouts for instance has individual coordinate reference de scriptions for chairs and tables and other furniture that can be placed into a floor plan with multiple copies of the chairs and other iterns in different positions In other applications we may simply want to reorient the coordinate reference for displaying a scene Relationships between Cartesian reference systems and some eSmmon non Cartesian systems are given in Appendix A Here we consider transformations between two Cartesian frames of reference   shows two Cartesian systems with the coordinate origins at  and xo yo and with an orientation angle  between the x and x axes To trans form object descriptions from xy coordinates to x y coordinates we need to set up a transformation that superimposes the x y axes onto the xy axes This is done in two steps  Translate so that the origin  p yg of the x y system is moved to the origin of the xy system  Rotate the x axis onto the x axis  Translation of the coordinate origin is expressed with the matrix operation QO  X T  o  o  Yo 59 061 Section Transformations between Coordinate Systems  Chapter Two Dimensional Geometric Transformations   A Cartesian x y system positioned at xp yp with orientation  in an xy Cartesian system  and the orientation of the two systems after the translation operation would ap  pear as in  To get the axes of the two systems into coincidence we then perform the clockwise rotation cos sin R     sin cos  60  Concatinating these two transformations matrices gives us the complete compos ite matrix for transforming object descriptions from the ry system to the x y sys tem  Myyry  R  T Xo  Yo  61  An alternate method for giving the orientation of the second coordinate sys tem is to specify a vector V that indicates the direction for the positive y axis as shown in  Vector V is specified as a point in the xy reference frame rela tive to the origin of the xy system A unit vector in the y direction can then be obtained as  al y   v  Uy en A a Ny And we obtain the unit vector u along the x axis by rotating v clockwise  u  yy  v    u My  63   Position of the reference frames shown in  after translating the origin of the x y system to the coordinate origin of the xy system  x axis   Cartesian system x y with origin at Py  xa Yo and y axis parallel to vector V  In Section we noted that the elements of any rotation matrix could be ex pressed as elements of a set of orthogonal unit vectors Therefore the matrix to rotate the xy system into coincidence with the xy system can be written as uy uy O R  vy 64  As an example suppose we choose the orientation for the y axis as V    then the x axis is in the positive y direction and the rotation transformation ma trix is  oro oo  i  Equivalently we can obtain this rotation matrix from 60 by setting the orienta tion angle as   In an interactive application it may be more convenient to choose the direc tion for V relative to position Py than it is to specifv it relative to the ry coordi nate origin Unit vectors u and v would then be oriented as shown in  The components of v are now calculated as Pi  Po v  lp  Pol  65  and u is obtained as the perpendicular to v that forms a right handed Cartesian system  y axis   A Cartesian x y system defined with two coordinate positions Py and P  within an sy reference frame   Section Transformations between Coordinate Systems Chapter  Two Dimensional Geometric Transformations  A coordinate transformation of the form x  ax  ayy  b  y ay x  ayy  by 66  is called a two dimensional affine transformation Each of the transformed coor dinates x and y is a linear function of the original coordinates x and y and para meters a and  are constants determined by the transformation type Affine transformations have the general properties that parallel lines are transformed into parallel lines and finite points map to finite points  Translation rotation scaling reflection and shear are examples of two di mensional affine transformations Any general two dimensional affine transfor mation can always be expressed as a composition of these five transformations Another affine transformation is the conversion of coordinate descriptions from one reference system to another which can be described as a combination of translation and rotation An affine transformation involving only rotation trans lation and reflection preserves angles and lengths as well as parallel lines For these three transformations the lengths and angle between two lines remains the same after the transformation  Graphics packages can be structured so that separate commands are provided to a user for each of the basic transformation operations as in procedure trans  formObject A composite transformation is then set up by referencing individ ual functions in the order required for the transformation sequence An alternate formulation is to provide users with a single transformation function that in cludes parameters for each of the basic transformations The output of this func tion is the composite transformation matrix for the specified parameter values Both options are useful Separate functions are convenient for simple transforma tion operations and a composite function can provide an expedient method for specifying complex transformatian sequences  The PHIGS library provides users with both options Individual commands for generating the basic transformation matrices are translate trans ateVector matrixTrans ate rotate theta matrixRotate scale scaleVector matrixScale  Hach of these functions produces a by transformation matrix that can then be used to transform coordinate positions expressed as homogeneous column vec tors Parameter translateVector is a pointer to the pair of translation dis tances  and t  Similarly parameter scaleVector specifies the pair of scaling values s and s  Rotate and scale matrices matrixTranslate and matrix Scale transform with respect to the coordinate origin We concatenate transformation matrices that have been previously set up with the function composeMatrix matrix2 matrixl matrixOut  where elements of the composite output matrix are calculated by postmultiply ing matrix2 by matrix A composite transformation matrix to perform a com bination scaling rotation and translation is produced with the function buildfransformationMatrix referencePoint translateVector theta scaleVector matrix  Rotation and scaling are carried out with respect to the coordinate position speci fied by parameter referencePoint The order for the transformation sequence is assumed to be  scale  rotate and  translate with the elements for the composite transformation stored in parameter matrix We can use this function to generate a single transformation matrix or a composite matrix for two or three transformations in the order stated  We could generate a translation matrix by setting scaleVector    theta  and assigning x and y shift values to parameter translatevector Any coordinate values could be assigned to pa rameter referencePoint since the transformation calculations are unaffected by this parameter when no scaling or rotation takes place But if we only want to set up a translation matrix we can use function translate and simply specify the translation vector A rotation or scaling transformation matrix is specified by setting translatevector   and assigning appropriate values to parame ters referencePoint theta and scaleVector To obtain a rotation matrix we set scalevector    and for scaling only we set theta  If we want to rotate or scale with respect to the coordinate origin it is simpler to set up the matrix using either the rotate or scale function  Since the function buildTransformationMatrix always generates the transformation sequence in the order  scale  rotate and  translate the fol lowing function is provided to allow specification of other sequences  composeTransformationMatrix matrixIn referencePoint translateVector theta scaleVector matrixOut  We can use this function in combination with the buildTransformationMa trix function or with any of the other matrix construction functions to compose any transformation sequence For example we could set up a scale matrix about a fixed point with the bui ldTransformat ionMatrix function then we could use the composeTransformationMatrix function to concatenate this scale matrix with a rotation about a specified pivot point The composite rotate scale sequence is then stored in matrixOut  After we have set up a transformation matrix we can apply the matrix to individual coordinate positions of an object with the function transformPoint inPoint matrix outPoint  where parameter inPoint gives the initial xy coordinate position of an object point and parameter out Point contains the corresponding transformed coordi nates Additional functions discussed in Chapter are available for performing two dimensional modeling transformations  Section Transformation Functions Chapter Two Dimensional Geometric Transformations  a    Translating an object from screen position a to position b by moving a rectangular block of pixel values Coordinate positions P   and P   specify the limits of the rectangular block to be moved and P  is the destination reference posiion    The particular capabilities of raster systems suggest an alternate method for transforming objects Raster systems store picture information as pixel patterns in the frame buffer Therefore some simple transformations can be carried out rapidly by simply moving rectangular arrays of stored pixel values from one lo cation to another within the frame buffer Few arithmetic operations are needed so the pixel transformations are particularly efficient  Raster functions that manipulate rectangular pixel arrays are generally re ferred to as raster ops Moving a block of pixels from one location to another is also called a block transfer of pixel values On a bilevel svstem this operation is called a bitBlt bit block transfer  particularly when the function is hardware implemented The term pixBit is sometimes used for block transfers on multi level systems multiple bits per pixel   illustrates translation performed as a block transfer of a raster area All bit settings in the rectangular area shown are copied as a block into an other part of the raster We accomplish this translation bv first reading pixel in tensities from a specified rectangular area of a raster into an array then we copy the array back into the raster at the new location The original object could be erased by filling its rectangular area with the background intensity assuming the object does not overlap other objects in the scene  Typical raster functions often provided in graphics packages are   copy  movea pixel block from one raster area to another  read  save a pixel block in a designated array  write  transfer a pixel array to a position in the frame buffer Some implementations provide options for combining pixel values In replace made pixel values are simply transfered to the destination positions Other op tions for combining pixel values include Boolean operations aid or and exclte sive or and binary arithmetic operations With the exclusive or mode two succes sive copies of a block to the same raster area restores the values that were originally present in that area This technique can be used to move an object across a scene without destroying the background Another option for adjusting pixel values is to combine the source pixels with a specified mask This allows only selected positions within a block to be transferred or shaded by the patterns defined in the mask   12014 3 93 7  6 1 10  a  b  c   Rotating an array of pixel values The original array orientation 1s shown in a  the array orientation after a counterclockwise rotation is shown in b  and the array orientation after a rotation is shown in   __ Destination Rotated Pixel Areas Pixel Array  Destination Pixel Array   A raster rotation for a rectangular block of pixels is accomplished by mapping the destination pixel areas onto the rotated block  Rotations in degree increments are easily accomplished with black trans fers We can rotate an object counterclockwise by first reversing the pixel val ues in each row of the array then we interchange rows and columns A rota tion is obtained by reversing the order of the elements in each row of the array then reversing the order of the rows  demonstrates the array manipu lations necessary to rotate a pixel block by and by  For array rotations that are not multiples of  we must perform more computations The general procedure is illustrated in  Each destination pixel area is mapped onto the rotated array and the amount of overlap with the rotated pixel areas is calculated An intensity for the destination pixel is then computed by averaging the intensities of the overlapped source pixels weighted by their percentage of area overlap  Raster scaling of a block of pixels is analogous to the cell array mapping discussed in Section 13 We scale the pixel areas in the original block using specified values for s and s and map the scaled rectangle onto a set of destina tion pixels The intensity of each destination pixel is then assigned according to its area of overlap with the scaled pixel areas     POTEET I Ty 1 i       b  4___ Destination    i t  i Pixel Array b JU 74 bbb trot ETT I  t   t   Scaled Array  eta I t x    Mapping destination pixel areas onto a scaled array of pixel values Scaling factors s  s  are applied relative to fixed point x y  Section Raster Methods for Transformations Chapter Two Dimensional Geometric Transformations  SUMMARY The basic geometric transformations are translation rotation and scaling Trans lation moves an object in a straight line path from one position to another Rota tion moves an object from one position to another in a circular path around a specified pivot paint ratation point  Scaling changes the dimensions of an object relative toa specified fixed point  We can express two dimensional geometric transformations as by ma trix operators so that sequences of transformations can be concatenated into a single composite matrix This is an efficient formulation since it allows us to re duce computations by applying the composite matrix to the initial coordinate po sitions of an object to obtain the final transformed positions To do this we also need to express two dimensional coordinate positions as three element column or row matrices We choose a column matrix representation for coordinate points because this is the standard mathematical convention and because many graph ics packages also follow this convention For two dimensional transformations coordinate positions are then represented with three element nomogeneous coor dinates with the third homogeneous coordinate assigned the value  Composite transformations are formed as multiplications of any combina tion of translation rotation and scaling matrices We can use combinations of translation and rotation for animation applications and we can use combinations of rotation and scaling to scale objects in any specified direction In general ma trix multiplications are not commutative We obtain different results for exam ple if we change the order of a translate rotate sequence A transformation se quence involving only translations and rotations is a rigid body transformation since angles and distances are unchanged Also the upper left submatrix of a rigid body transformation is an orthogonal matrix Thus rotation matrices can be formed by setting the upper left by submatrix equal to the elements of two orthogonal unit vectors Computations in rotationg transformations can be re duced by using approximations for the sine and cosine functions when the rata tion angle is small Over many rotational steps however the approximation error can accumulate to a significant value  Other transformations include reflections and shears Reflections are trans formations that rotate an object about a reflection axis This produces a mir ror image of the object with respect to that axis When the reflection axis is per pendicular to the xy plane the reflection is obtained as a rotation in the xy plane When the reflection axis is in the xy plane the reflection is obtained as a rotation ina plane that is perpendicular to the xy plane Shear transformations distort the shape of an object by shifting x or y coordinate values by an amount proportional to the coordinate distance from a shear reference line  Transformations between Cartesian coordinate systems are accomplished with a sequence of translate rotate transformations One way to specify a new co ordinate reference frame is to give the position of the new coordinate origin and the direction of the new y axis The direction of the new x axis is then obtained by rotating the y direction vector clockwise Coordinate descriptions of objects in the old reference frame are transferred to the new reference with the transforma tion matrix that superimposes the new coordinate axes onto the old coordinate axes This transformation matrix can be calculated as the concatentation of a translation that moves the new origin to the old coordinate origin and a rotation to align the two sets of axes The rotation matrix is obtained from unit vectors in the x and y directions for the new system Two dimensional geometric transformations are affine transformations That is they can be expressed as a linear function of coordinates x and y Affine transformations transform parallel lines to parallel lines and transform finite points to finite points Geometric transformations that do not involve scaling or shear also preserve angles and lengths  Transformation functions in graphics packages are usually provided only for translation rotation and scaling These functions include individual proce dures for creating a translate rotate or scale matrix and functions for generating a composite matrix given the parameters for a transformation sequence  Fast raster transformations can be performed by moving blocks of pixels This avoids calculating transformed coordinates for an object and applying scan conversion routines to display the object at the new position Three common raster operations bitBlts or pixBlts are copy read and write When a block of pixels is moved to a new position in the frame buffer we can simply replace the old pixel values or we can combine the pixel values using Boolean or arithmetic operations Raster translations are carried out by copying a pixel block to a new location in the frame buffer Raster rotations in multiples of are obtained by manipulating row and column positions of the pixel values in a block Other rotations are performed by first mapping rotated pixel areas onto destination po sitions in the frame buffer then calculating overlap areas Scaling in raster trans formations is also accomplished by mapping transformed pixel areas to the frame buffer destination positions  REFERENCES For additional information on homogeneous coordinates in computer graphics see Blinn and  Transformation functions in PHIGS are discussed in Hopgood and Duce  t oward et al   Gaskins  and Blake  For information on GKS transformation func tions see Hopgood et al  and Enderle Kansy and Pfaff   EXERCISES Write a program to continuously rotate an object about a pivot point Small angles are to be used for each successive rotation and approximations to the sine and cosine functions are to be used to speed up the calculations The rotation angle for each step is to be chosen so that the object makes one complete revolution in less than sec onds To avoid accumulation of coordinate errors reset the original coordinate values for the object at the start of each new revolution  Show that the composition of two retations is additive by concatinating the matrix representations for R  and R   to obtain R   R G   RG    we Write a set of procedures to implement the buildTrans format ionMatrix and the composeTransformationMatrix functions to produce a composite transforma tion matrix for any set of input transformation parameters  Write a program that applies any specified sequence of transformations to a displayed object The program is to be designed so that a user selects the transformation se quence and associated parameters from displayed menus and the composite transfor   a Exercises Chapter Two Dimensional Geometric Transforma ons       an NI oo Oo  I  Eva  mation is then calculated and used to transform the object Display the original object and the transformed object in different colars or different fill patterns  Modily the transformation matrix 35  for scaling in an arbitrary direction to in clude coordinates for any specified scaling fixed point x y  Prove that the multiplication of transformation matrices tor each of the following se quence of operations is commutative  a Two successive rotations  b Two successive trans ations  c Two successive scatings  Prove that a uniform scaling s  s  and a rotatior form a commutative pair of opera tions but that in general scaling and rotation are not commutative operations Multiply the individual scale rotate and translate matrices in Eq 38 to verify the el ements in the composite transformation matrix  Show that transformation matrix 51  fora reflection about the line y  x is equiva lent to a reflection relative to the x axis followed by a counterclockwise rotation of  Show that transformation matrix 52  for a reflection about the line y  x is equivalent to a reflection relative to the y axis followed by a counterclockwise rotation of  Show that two successive reflections about either of the coordinate axes is equivalent to a single rotation about the caordinate origin  Determine the form of the transformation matrix for a reflection about an arbitrary line with equation y  mx  b  Show that two successive reflections about any line passing through the coordinate origin 1s equivalent to a single rotation about the origin Determine a sequence of basic transformations that are equivalent to the x direction shearing matrix 53  Determine a sequence of basic transformations that are equivalent to the y direction shearing matrix 571 Set up a shearing procedure to display italic characters given a vector font definitior  That is afl character shapes in this font are defined with straight line segments and italic characters are formed with shearing transformations Determine an appropriate value for the shear parameter by comparing italics and plain text in some available font Define a simple vectar fon for input to your routine  Derive the following equations for transforming a coordinate point P  x yi in one Cartesian system ta the coordinate values x  yin another Cartesian system thal is ro tated by an angle  as in  Project point P onto each of the four axes and analyse the resulting right triangles  x xcos  ysin  y   xsin  vcos Write a procedure to compute the elements of the matrix for transforming object de scriptions from one Cartesian coordinate system to another The second coordinate system is to be defined with an origin point Py and a vectar V that gives the directian for the positive y axis of this system  Set up procedures for implementing a block transfer of a rectangular area of a frame buffer using onc function to read the area into an array and another function to cop the array into the designated transfer area  Determine the results of performing two successive block transfers into the same area of a frame buffer using the various Boolean operations  What are the results of performing two successive block transfers into the same area a frame buffer using the binary arithmetic operations  Implement a routine to perform block transfers in a trame bulier using any specified Boolean operation or a replacement copy operation 23 Write a routine to implement rotations in increments of in frame buffer block transfers  Exercises 24 Write a routine to implement rotations by any specified angle in a frame buffer block transier  25 Write a routine to implement scaling as a raster transformation of a pixel block   CHAPTER Two Dimensional Viewing  Viewing Coordinate Window  Normalized Space     Viewport L ws2 A  re me NY Window  Zt A IN  ws ws1 Viewport Window ws2 Viewport    Monitor Monitor    W e now consider the formal mechanism for displaying views of a picture on an output device Typically a graphics package allows a user to specify which part of a defined picture is to be displayed and where that part is to be placed on the display device Any convenient Cartesian coordinate system referred to as the world coordinate reference frame can be used to define the pic ture For a two dimensional picture a view is selected by specifying a subarea of the total picture area A user can select a single area for display or several areas could be selected for simultaneous display or for an animated panning sequence across a scene The picture parts within the selected areas are then mapped onto specified areas of the device coordinates When multiple view areas are selected these areas can be placed in separate display locations or some areas could be in serted into other larger display areas Transformations from world to device co ordinates involve translation rotation and scaling operations as well as proce dures for deleting those parts of the picture that are outside the limits of a selected display area  A world coordinate area selected for display is called a window An area ona display device to which a window is mapped is called a viewport The window defines what is to be viewed the viewport defines where it is to be displayed Often windows and viewports are rectangles in standard position with the rec tangle edges parallel to the coordinate axes Other window or viewport geome tries such as general polygon shapes and circles are used in some applications but these shapes take longer to process In general the mapping of a part of a world coordinate scene to device coordinates is referred to as a viewing transfor mation Sometimes the two dimensional viewing transformation is simply re ferred to as the window to viewport transformation or the windowing transformation But in general viewing involves more than just the transformation from the win dow to the viewport  illustrates the mapping of a picture section that falls within a rectangular window onto a designated rectangular viewport  In computer graphics terminology the term window originally referred to an area of a picture that is selected for viewing as defined at the beginning of this section Unfortunately the same term is now used in window manager systems to refer to any rectangular screen area that can be moved about resized and made active or inactive In this chapter we will only use the term window to Window YWinas r Viewpoint Yinax me  L  XW  erin aw  max Venues XVnax World Coordinates Device Coordinates  A viewing transformation using standard rectangies for the window and viewport  refer to an area of a world coordinate scene that has been selected for displav When we consider graphical user interfaces in Chapter  we will discuss screen windows and window manager systems  Some graphics packages that provide window and viewport operations allow only standard rectangles but a more general approach is to allow the rec tangular window to have any orientation In this case we carry out the viewing transformation in several steps as indicated in  First we construct the scene in world coordinates using the output primitives and attributes discussed in Chapters and Nest to obtain a particular orientation for the window we can set up a two dimensional viewing coordinate system in the world coordi nate plane and define a window in the viewing coordinate system The viewing coordinate reference frame is used to provide a method for setting up arbitrary orientations for rectangular windows Once the viewing reference frame is estab lished we can transform desenptions in world coordinates to viewing coordi nates We then define a viewport in normalized coordinates in the range from to and map the viewing coordinate description of the scene to normalized co ordinates At the final step all parts of the picture that he outside the viewport are clipped and the contents of the viewport are transferred to device coordi nates  illustrates a rotated viewing coordinate reference frame and the mapping to normalized coordinates  By changing the position of the viewport we can view objects at different positions on the display area of an output device Also by varying the size of viewports we can change the size and proportions of displayed objects We achieve zooming effects by successively mapping different sized windows on a  Construct  Convert Map Viewing Map Normalized World Coordinate World Coordinates to Viewport to  MC Scene Using wc Coordinates VC Normalized NVC Device       Modaling Coordinate  to    Viewing Coordinates   Coordinates  Transformations Viewing using Window Viewport    Coordinates Specifications    The two dimensional viewing transformation pipeline Device Coordinates   Setting up a rotated world window in viewing coordinates and the corresponding normalized coordinate viewport  fixed size viewport As the windows are made smaller we zoom in on some part of a scene to view details that are not shown with larger windows Similarly more overview is obtained by zooming out from a section of a scene with succes sively larger windows Panning effects are produced by moving a fixed size win dow across the various objects in a scene  Viewports are typically defined within the unit square normalized coordi nates  This provides a means for separating the viewing and other transforma tions from specific output device requirements so that the graphics package is largely device independent Once the scene has been transferred to normalized coordinates the unit square is simply mapped to the display area for the particu lar output device in use at that time Different output devices can be used by pro viding the appropriate device drivers  When all coordinate transformations are completed viewport clipping can be performed in normalized coordinates or in device coordinates This allows us to reduce computations by concatenating the various transformation matrices Clipping procedures are of fundamental importance in computer graphics They are used not only in viewing transformations but also in window manager sys tems in painting and drawing packages to eliminate parts of a picture inside or outside of a designated screen area and in many other applications  This coordinate system provides the reference frame for specifying the world coordinate window We set up the viewing coordinate system using the proce dures discussed in Section First a viewing coordinate origin is selected at some world position Po  xo Yo  Then we need to establish the orientation or rotation of this reference frame One way to do this is to specify a world vector V that defines the viewing y direction Vector V is called the view up vector  Given V we can calculate the components of unit vectors v  v  vy and u  u  u  for the viewing y and x axes respectively These unit vectors are used to form the first and second rows of the rotation matrix R that aligns the viewing x y axes with the world y  axes  Section  Viewing Coordinate Reference Frame  x world   ys ey  a  b    A viewing coordinate frame is moved into coincidence with the world frame in two steps  a translate the viewing origin to the world origin then b rotate to align the axes of the two systems  We obtain the matrix for converting world coordinate positions to viewing coordinates as a two step composite transformation First we translate the view ing origin to the world origin then we rotate to align the two coordinate refer ence frames The composite two dimensional transformation to convert world coordinates to viewing coordinates is where T is the translation matrix that takes the viewing origin point P to the world origin and R is the rotation matrix that aligns the axes of the two reference frames  illustrates the steps in this coordinate transformation  Once object descriptions have been transferred to the viewing reference frame we choose the window extents in viewing coordinates and select the viewport limits in normalized coordinates   Object descriptions are then trans ferred to normalized device coordinates We do this using a transformation that maintains the same relative placement of objects in normalized space as they had in viewing coordinates If a coordinate position is at the center of the viewing window for instance it will be displayed at the center of the viewport   illustrates the window to viewport mapping A point at position xw yw in the window 1s mapped into position xv yv in the associated view port To maintain the same relative placement in the viewport as in the window we require that XU HVeup XW XWyn Weax  XUmin XWax  XWoyn   YO  Yun _ YW  YW in Ymax  YU min YO max  YRoin A point at position rw yw in a designated window is mapped to viewport coordinates xv yu so that relative positions in the two areas are the same  Solving these expressions for the viewport position xv yv  we have XV  AVeug  KW  XW SX  yO  Ymin  yw  Ym SY where the scaling factors are x  Vmax  XUnun XWmax  Wn  ay  Pes  Hain YO max  YWmin Equations can also be derived with a set of transformtions that converts the window area into the viewport area This conversion is performed with the fol lowing sequence of transformations  Perform a scaling transformation using a fixed point position of xu pin YWymin that scales the window area to the size of the viewport  Translate the scaled window area to the position of the viewport  Relative proportions of objects are maintained if the scaling factors are the same sx  sy  Otherwise world objects will be stretched or contracted in either the x or y direction when displayed on the output device  Character strings can be handled in two ways when they are mapped to a viewport The simplest mapping maintains a constant character size even though the viewport area may be enlarged or reduced relative to the window This method would be employed when text is formed with standard character fonts that cannot be changed In systems that allow for changes in character size string definitions can be windowed the same as other primitives For characters formed with line segments the mapping to the viewport can be carried out as a sequence of line transformations  From normalized coordinates object descriptions are mapped to the vari ous display devices Any number of output devices can be open in a part cular application and another window to viewport transformation can be performed for each open output device This mapping called the workstation transforma  Section Window ta Viewpart Coordinate Transiormation Chapter  Two Dimensional Viewing Viewing Coordinate Window  Normalized Space T Viewpon  ws2 Windowws Window Monitor Monitor  Mapping selected parts of a scene in normalized coordinates to different video monitors with workstation transformations  tion is accomplished by selecting a window area in normalized space and a viewport area in the coordinates of the display device With the workstation transformation we gain some additional control over the positioning of parts of a scene on individual output devices As illustrated in  we can use work station transformations to partition a view so that different parts of normalized space can be displayed on different output devices  We define a viewing reference system in a PHIGS application program with the following function  evaluateViewOrientationMatrix xv yO xv yV error viewMatrix  where parameters x0 and y0 are the coordinates of the viewing origin and para meters xV and yV are the world coordinate positions for the view up vector An integer error code is generated if the input parameters are in error otherwise the viewMat rix for the world to viewing transformation is calculated Any number of viewing transformation matrices can be defined in an application  To set up the elements of a window to viewport mapping matrix we in voke the function avaluateViewMappingMatrix xwmin xwmax ywmin ymax xvmin xvmax yvmin yvmax error viewMappingMatrix  Here the window limits in viewing coordinates are chosen with parameters xwmin xwmax ywmin and ywmax and the viewport limits are set with the nor malized coordinate positions xvmin xvmax yvmin yvmax As with the viewing transformation matrix we can construct several window viewport pairs and use them for projecting various parts of the scene to different areas of the unit square  Next we can store combinations of viewing and window viewport map pings for various workstations in a viewing table with setViewRepresentation ws viewIndex viewMatrix viewMappingMatrix xclipmin xclipmax yclipmin yclipmax clipxy  where parameter ws designates the output device workstation  and parameter viewIndex sets an integer identifier for this particular window viewport pair The matrices viewMatrix and viewMappingMatrix can be concatenated and referenced by the viewIndex Additional clipping limits can also be specified here but they are usually set to cvincide with the viewport boundaries And pa rameter cl ipxy is assigned either the value moclip or the value clip This allows us to turn off clipping if we want to view the parts of the scene outside the view port We can also select noclip to speed up processing when we know that all of the scene is included within the viewport limits The function setvViewiIndex viewIndex  selects a particular set of options from the viewing table This view index selec tion is then applied to subsequently specified output primitives and associated attributes and generates a display on each of the active workstations  At the final stage we apply a workstation transformation by selecting a workstation window viewport pair  setWorkstationWindow ws xwsWindmir xwsWindmax ywsWindmin ywsWindmax  setWorkstationViewport ws xwsVPortmin xwsVPortmax ywsVPortmin ywsVPortmax  where parameter ws gives the workstation number Window coordinate extents are specified in the range from to normalized space  and viewport limits are in integer device coordinates  Ifa workstation viewport is not specified the unit square of the normalized reference frame is mapped onto the largest square area possible on an output de vice The coordinate origin of normalized space is mapped to the origin of device coordinates and the aspect ratio is retained by transforming the unit square onto a square area on the output device  Example Two Dimensional Viewing Example As an example of the use of viewing functions the following sequence of state ments sets up a rotated window in world coordinates and maps its contents to the upper right corner of workstation We keep the viewing coordinate origin at the world origin and we choose the view up direction for the window as   This gives us a viewing coordinate system that is rotated clockwise in the world coordinate reference frame The view index is set to the value  Section Two Dimensional Viewing Functions Chapter Two Dimensional Viewing evaluateViewOrlentationMatrix   viewError viewMat  evaluateViewMappingMatrix  24  75   C  viewMapError viewMapMat  setViewRepresentation  viewMat viewMapMat    clip  setViewIndex   Similarly we could set up an additiona transformation with view index that would map a specified window into a viewport at the lower left of the screen Two graphs for example could then be displayed at opposite screen comers with the following statements  setViewIndex  polyline  axes  polyline  datal  setVviewIndex  polyline  axes  polyline  data2  View index selects a viewport in the upper right of the screen display and view index selects a viewport in the lower left corner The function polyline  axes produces the horizontal and vertical coordinate reference for the data plot in each graph   Generally any procedure that identifies those portions of a picture that are either inside or outside of a specified region of space is referred to as a clipping algo rithm or simply clipping The region against which an object is to clipped is called a clip window  Applications of clipping include extracting part of a defined scene for view ing identifying visible surfaces in three dimensional views antialiasing line seg ments or object boundaries creating objects using sclid modeling procedures displaying a multiwindow environment and drawing and painting operations that allow paris of a picture to be selected for copying moving erasing or duphi cating Depending on the application the clip window can be a general polygon or it can even have curved boundaries We first consider clipping methods using rectangular clip regions then we discuss methods for other clip region shapes  For the viewing transformation we want to display only those picture parts that are within the window area assuming that the clipping flags have not been set to noclip  Everything outside the window is discarded Clipping algorithms can be applied in world coordinates so that only the contents of the window in terior are mapped to device coordinates Alternatively the complete world coor dinate picture can be mapped first to device coordinates or normalized device coordinates then clipped against the viewport boundaries World coordinate clipping removes those primitives outside the window from further considera tion thus eliminating the processing necessary to transform those primitives to device space Viewport clipping on the other hand can reduce calculations by al lowing concatenation of viewing and geometric transformation matrices But viewport clipping does require that the transformation to device coordinates be Section performed for all objects including those outside the window area On raster _ Line Clipping systems clipping algorithms are often combined with scan conversion  In the following sections we consider algorithms for dipping the following primitive types  Point Clipping  Line Clipping straight line segments  Area Clipping polygons   Curve Clipping  Text Clipping Line and polygon clipping routines are standard components of graphics pack ages but many packages accommodate curved objects particularly spline curves and conics such as circles and ellipses Another way to handle curved objects is to approximate them with straight line segments and apply the line or polygon clipping procedure  Assuming that the clip window is a rectangle in standard position we save a point P  x y for display if the following inequalities are satisfied XWeyin  XFS AWersy   Yin  YS YUrx where the edges of the clip window pin XWmax Lmine Wmax CaN be either the world coordinate window boundaries or viewport boundaries If any one of these four inequalities is not satisfied the point is clipped not saved for display  Although point clipping is applied Jess often than line or polygon clipping some applications may require a point lipping procedure For example point clipping can be applied to scenes involving explosions or sea foam that are mod eled with particles points distributed in some region of the scene    illustrates possible relationships between line positions and a standard rectangular clipping region A line clipping procedure involves several parts First we can test a given line segment to determine whether it lies completely in side the clipping window If it does not we try to determine whether it lies com pletely outside the window Finally if we cannot identify a line as completely in side or completely outside we must perform intersection calculations with one or more clipping boundaries We process lines through the inside outside tests by checking the line endpoints A line with both endpoints inside all clipping boundaries such as the line from P to P  is saved A line with both endpoints outside any one of the clip boundaries line P P in  is outside the win   Window    Before Clipping After Clipping  a tb  Line clipping against  rectangular clip window  dow All other lines cross one or more clipping boundaries and may require cal culation of multiple intersection points To minimize calculations we try to de vise clipping algorithms that can efficiently identify outside lines and reduce in tersection calculations  For a line segment with endpoints x  y  and   yy and one or both end points outside the clipping rectangle the parametric representation  aeox Fey XD   wey tuys  yh Peusl could be used to determine values of parameter i for intersections with the chp ping boundary coordinates If the value of u for an intersection with a rectangle boundary edge is outside the range to the line does not enter the interiar of the window at that boundary If the value of u is within the range from to the line segment dues indeed cross into the clipping area This method can be ap phed to each chpping boundary edge in turn to determine whether any part of the line segment is to be displayed Line segments that are parallel to window edges can be handled as special cases  Clipping line segments with these parametric tesis requires a good deal of computation and faster approaches to clipping are possible A number of effi cient Jine clippers have been developed and we survey the major algorithms in the next sections Some algorithms are designed explicitly for two dimensional pictures and some are easily adapted to three dimensional applications  Cohen Sutherland Line CHpping This is one of the oldest and most popular kne clipping procedures Generally the method speeds up the processing of Hne segments by performing initial tests that reduce the number of intersections that must be calculated Every line end point in a picture is assigned a four digit binary code called a region code that identifies the location of the point relative to the boundaries of the clipping rec tangle Regions are set up in reference to the boundaries as shown in  Each bit position in the region code is used to indicate one of the four relative co ordinate positions of the point with respect to the clip window to the left right top or bottom By numbering the bit positions in the region code as through from right to left the coordinate regions can be correlated with the bit posi tions as bit left bit nght bit below bit above A value of in any bit position indicates that the point is in that relative position otherwise the bit position is set to If a point is within the clipping rectangle the region code is A point that is below and to the left of the rectangle has a region code of  Bit values in the region code are determined by comparing endpoint coordi nate values x y to the clip boundaries Bit is set to if x  xWgin The other three bit values can be determined using similar comparisons For languages in which bit manipulation is possible region code bit values can be determined with the following two steps  Calculate differences between endpoint coordi nates and clipping boundaries  Use the resultant sign bit of each difference calculation to set the corresponding value in the region code Bit is the sign bit Of X  XW pin Dit is the sign bit of xw    x bit is the sign bit of y  ywain and bit is the sign bit of yn  Y  Once we have established region codes for all line endpoints we can quickly determine which lines are completely inside the clip window and which are clearly outside Any lines that are completely contained within the window boundaries have a region code of for both endpoints and we trivially accept these lines Any lines that have a in the same bit position in the region codes for each endpoint are completely outside the clipping rectangle and we trivially re ject these lines We would discard the line that has a region code of for one endpoint and a code of for the other endpoint Both endpoints of this line are left of the clipping rectangle as indicated by the in the first bit position of each region code A method that can be used to test lines for total clipping is to perform the logical and operation with both region codes If the result is not the line is completely outside the clipping region  Lines that cannot be identified as completely inside or completely outside a clip window by these tests are checked for intersection with the window bound aries As shown in  such lines may or may not cross into the window in terior We begin the clipping process for a line by comparing an outside endpoint to a clipping boundary to determine how much of the line can be discarded Then the remaining part of the line is checked against the other boundaries and we continue until either the line is totally discarded or a section is found inside the window We set up our algorithm to check line endpoints against clipping boundaries in the order left right bottom top  To illustrate the specific steps in clipping lines against rectangular bound aries using the Cohen Sutherland algorithm we show how the lines in  could be processed Starting with the bottom endpoint of the line from P to P2  Section Line Clipping 1000 0000 oo1c ___ LWinctow _ 0100 O11c  Binary region codes assigned to line endpoints according to relative position with respect to the clipping rectangle   define ROUND a  Py   Window   Lines extending from one Pp coordinate region to another may Py pass through the clip window p P or they may intersect clipping   boundaries without entering the window  we check P against the left right and bottom boundaries in turn and find that this point is below the clipping rectangle We then find the intersection point P with the bottom boundary and discard the line section from P to P  The line now has been reduced to the section from P to P2 Since P is outside the clip window we check this endpoint against the boundaries and find that it is to the left of the window Intersection point P is calculated but this point is above the window So the final intersection calculation yields P  and the line from P to P is saved This completes processing for this line so we save this part and go on to the next line Point P in the next line is to the left of the clipping rectangle so we determine the intersection P and eliminate the line section from P3 to P  By checking region codes for the line section from P to P  we find that the remain der of the line is below the clip window and can be discarded also  Intersection points with a clipping boundary can be calculated using the slope intercept form of the line equation For a line with endpoint coordinates x y  and Xx y  the y coordinate of the intersection point with a vertical boundary can be obtained with the calculation y y tm  x   where the x value is set either to xWypin OF to XWmax and the slope of the line is cal culated as m  y  y    x  Similarly if we are looking for the intersection with a horizontal boundary the x coordinate can be calculated as x x  in  with y set either to yw OF tO YWrax  The following procedure demonstrates the Cohen Sutherland line clipping algorithm Codes for each endpoint are stored as bytes and processed using bit manipulations  ifint  as0    Bat masks encode a point s position relative to the clip edges A point s status is encoded by OR ing together appropriate bit masks    Fdefine LEFT_EDGE  Oxl  caefne RIGHT EDGE 0x2 gdefine BUTTOM_EDGE 0x4 define TOP EDGE Ox8   Points encoded as are completely Inside the clip rectangle all others ere outside at least one edge If CR ing two codes is FALSE mo bits are set in either code  the ine can be Accepted If the AND operation between two codes is TRUE the line defined by those endpoints is completely outside the clip region and can be Rejected  ys  define INSIDE a  a  define REJECT a b  akb  define ACCEPT a b   alb  unsigned char encode wePt2 pt dcPt winMin dePt winMax  unsigned char code 0x60  if ipt x  winMin x  code  code  LEFT_EDGE if pt x  winMax x  code  code  RIGHT_EDGE if pt y  winMin y  code  code  BOTTOM_EDGE if pc y  winMax y  code  code  TOP_ENGE return code  void SwapPts wePt2  pl weoPt2  p2   worte tmp  imp  pl  pl  p2  p2  tmp   vo d swapCodes junsigned char  cl unsigne char     unsigned char tmp  tmp  cl  cl  c2  c2  tmp  r void clipbLine dcPt winMin UcPt winMax weFt2 pl wePtz2 p2   unsigned char codel code2  int done  FALSE draw  FALSE  float m  while  done  codel  encode pi winMin winMax  code2  encode p2 winMin winmax  if ACCEPT codel code2   done  TRUE draw  TRUE  else if REJECT codel code2  done  TRUE else   Ensure that pl is outside window  if INSIDE codei    swapPts  pl  p2  sWapCodes  codel  code2 t  Use slope m to find line clipEdge intersections  if p2 x   pl x m  p2 y  pl y   p2 x  pl x  if codel  LEFT_EDGE  pl y   winMin x  pl x  m pl x  winMin x  else if codel  RIGHT_EDGE  pl y   winMax x  pl x  m pl x  winMax x  else if codel  BOTTOM_EDGE    Need to update pl x for non vertical lines only  it tp2 x  pl x pl x   winMin y  pl y  m1 pl y  winMin y  else if codel  TOP_EDGE  if p2 x x  pl x pl x   winMax y  pl y  a pl y  winMax y    if draw  lineDDA ROUND p1 x  ROUND pl y  ROUND p2 x  ROUND p2 y    Liang Barsky Line Clipping  Faster line clippers have been developed that are based on analysis of the para metric equation of a line segment which we can write in the form x X  udx  y y t udy Qsu sl where Ax  x  x and Ay  y  y  Using these parametric equations Cyrus and Beck developed an algorithm that is generally more efficient than the Cohen Sutherland algorithm Later Liang and Barsky independently devised an even faster parametric line clipping algorithm Following the Liang Barsky ap proach we first write the point clipping conditions in the parametric form XWin X  UAx  xw max  10 YW min  y  WAY  YWmax Each of these four inequalities can be expressed as up   k 4 11 where parameters p and q are defined as po  Ax  X AW an Pz  Ax  XWmar  oe 12 P  Ay a YT Wein Ps  Ay  VW Any line that is parallel to one of the clipping boundaries has p  for the value of k corresponding to that boundary k   and correspond to the left right bottom and top boundaries respectively  If for that value of k we also find q  then the line is completely outside the boundary and can be elimi nated from further consideration If q   the line is inside the parallel clipping boundary  When p   the infinite extension of the line proceeds from the outside to the inside of the infinite extension of this particular clipping boundary If p   the line proceeds from the inside to the outside For a nonzero value of p  we can calculate the value of  that corresponds to the point where the infinitely ex tended line intersects the extension of boundary k as we ht 13 Pa For each line we can calculate values for parameters u and u that define that part of the line that lies within the clip rectangle The value of u is deter mined by looking at the rectangle edges for which the line proceeds from the out side to the inside p   For these edges we calculate r  q p  The value of u is taken as the largest of the set consisting of and the various values of r Con versely the value of is determined by examining the boundaries for which the line proceeds from inside to outside p   A value of is calculated for each of these boundaries and the value of u is the minimum of the set consisting of and the calculated r values If u  tu the line is completely outside the clip win dow and it can be rejected Otherwise the end points of the clipped line are calcu lated from the two values of parameter This algorithm is presented in the following procedure Line intersection parameters are initialized to the values v  anc wy  For each clipping boundary the appropriate values for p and q are calculated and used by the func tion clipTest to determine whether the line can be rejected or whether the intersec tion parameters are to be adjusted When p  the parameter r is used to update when p  parameter r is used to update up ff updating u or uy results in u  Uy we reject the line Otherwise we update the appropriate u parameter only if the new value results in a shortening of the hne When p  and q  we can discard the line since it is parallel to and outside of this boundary If the line has not been rejected after all four values of p and g have been tested the endpoints of the clipped line are determined from values of and tp  finclude graphics   define ROUND a i 1int  at9   int clipTest float p float q float  ul fleat   Section Line Clipping float r int retval  TRUE  if p Oo   req p if r  u2s retVal  FALSE else if iv  ul Sul er  else  if p    r qg pPp if ry  ul retVal  FALSE else if r  u2  u2   else  p  so line is parallel to this clipping edge  if q    Line is outside clipping edge retVal  FALSE  return retVal    void clipLine dePe winMin dcPt winMax weFiZ pl wePt2 p2   float ul  u2  dx  p2 x  pl x dy  if clipTest  dx pl x  winMin x  ul  u2  if clipTest dx winMax x  pl x  ul  u2   dy  p2 y  pl y  if clipTest  dy pl y  winMin y  ul    if clipTest dy winMax y  pl y Sui  u2    if u2    p2 x  pl x  u2  dx p2 y  pl y  u2  dy    if ful    pl x  ul  dx  pl y  ul  dy    lineDDA ROUND pl x  ROUND pl y ROUND p2 x  ROUND p2 y    In general the Liang Barsky algorithm is more efficient than the Cohen Sutherland algorithm since intersection calculations are reduced Each update of parameters u and u requires only one division and window intersec tions of the line are computed only once when the final values of u and u have been computed In contrast the Cohen Sutherland algorithm can repeatedly cal culate intersections along a line path even though the line may be completely outside the clip window And each intersection calculation requires both a divi sion and a multiplication Both the Cohen Sutherland and the Liang Barsky al  gorithms can be extended to three dimensional clipping Chapter  Nicholl Lee Nicholl Line Clipping By creating more regions around the clip window the Nicholl Lee Nicholl or NLN algorithm avoids multiple clipping of an individual line segment In the Cohen Sutherland method for example multiple intersections may be calcu lated along the path of a single Line before an intersection on the clipping rectan gle is located or the line is completely rejected These extra intersection calcula tions are eliminated in the NLN algorithm by carrying out more region testing before intersection positions are calculated Compared to both the Cohen Suther land and the Liang Barsky algorithms the Nicholl Lee Nicholl algorithm per forms fewer comparisons and divisions The trade off is that the NLN algorithm can only be applied to two dimensional clipping whereas both the Liang Barsky and the Cohen Sutherland methods are easily extended to three dimensional scenes  For a line with endpoints P and P3 we first determine the position of point P for the nine possible regions relative to the clipping rectangle Only the three regions shown in  need be considered If P lies in any one of the other six regions we can move it to one of the three regions in  using a sym metry transformation For example the region directly above the clip window can be transformed to the region left of the clip window using a reflection about the line y   x or we could use a counterclockwise rotation  Next we determine the position of P relative to P  To do this we create some new regions in the plane depending on the location of P  Boundaries of the new regions are half infinite line segments that start at the position of P and pass through the window corners If P is inside the clip window and P is out side we set up the four regions shown in  The intersection with the ap propriate window boundary is then carried out depending on which one of the four regions L T R or B contains P  Of course if both P and P are inside the clipping rectangle we simply save the entire line  If P is in the region to the left of the window we set up the four regions L LT LR and LB shown in  These four regions determine a unique bound ary for the line segment For instance if P is in region L we clip the line at the left boundary and save the line segment from this intersection point to P  But if P is in region LT we save the line segment from the left window boundary to the top boundary If P is not in any of the four regions L LT LR or LB the entire line is clipped  P in Window P in Edge Region ta b  Section Line Clipping F in Corner Region Ic   Three possibfe positions for a line endpoint P in the NLN line clipping algorithm    The four clipping regions used in the NLN algorithm when P is inside the clip window and Pj is outside    The jour chipping regions used in the NLN algorithm when P is directly left of the clip window  For the third case when P is to the left and above the clip window we use the clipping regions in  In this case we have the two possibilites shown depending on the position of P relative to the top left corner of the window If P is in one of the regions T L TR TB LR or LB this determines a unique clip window edge for the intersection calculations Otherwise the entire line is re jected  To determine the region in which P is located we compare the slope of the line to the slopes of the boundaries of the clip regions For example if P is left of the clipping rectangle   then P is in region LT if  slope P Prp  slope P P  slope P P  14 or Wow BOW 15 Xpo Xp Myo Ky MOY And we clip the entire line if Y YO  x  Or  x yn  Wd o To The coordinate difference and product calculations used in the slope tests are saved and also used in the intersection calculations From the parametric equations X x   x  xu y yit G2  ypu an x intersection position on the left window boundary is x  x  with   x  x    x   that the y intersection position is  The two possible sets of clipping regions used in the NLN algonthin when P 1s above and co the left of the clip window  Line Chaping Using Nonrectangular Clip Windows In some applications it is often necessary to clip lines against arbitrarily shaped polygons Algorithms based on parametric line equations such as the Liang Barsky method and the earlier Cyrus Beck approach can be extended eas ily to convex polygon windows We do this by modifying the algorithm to in clude the parametric equations for the boundaries of the clip region Preliminary screening of line segments can be accomplished by processing lines against the coordinate extents of the clipping polygon For concave polygon clipping re gions we can still apply these parametric clipping procedures if we first split the concave polygon into a set of convex polygons  Circles or other curved boundary clipping regions are also possible but less commonly used Clipping algorithms for these areas are slower because intersec tion calculations involve nonlinear curve equations At the first step lines can be clipped against the bounding rectangle coordinate extents of the curved clip ping region lines that can be identified as completely outside the bounding rec tangle are discarded To identify inside lines we can calculate the distance of line endpoints from the circle center If the square of this distance for both endpoints of a line is less than or equal to the radius squared we can save the entire line The remaining lines are then processed through the intersection calculations which must solve simultaneous circle line equations Splitting Conc ave Polygons We can identify a concave polygon by calculating the cross products of succes sive edge vectors in order around the polygon perimeter If the z component of Chapter Two Dimensional Viewing  Splitting a concave polygon using the vector method  Identifying a concave polygon by calculating cross products of successive pairs of edge vectors  some cross products is positive while others have a negative z component we have a concave polygon Otherwise the polygon is convex This is assuming that no series of three successive vertices are collinear in which case the cross product of the two edge vectors for these vertices is zero If all vertices are collinear we have a degenerate polygon a straight line   illustrates the edge vector cross product method for identifying concave polygons  A vector method for splitting a concave polygon in the xy plane is to calculate the edge vector cross products in a counterclockwise order and to note the sign of the z component of the cross products If any z component tums out to be neg ative as in   the polygon is concave and we can split it along the line of the first edge vector in the cross praduct pair The following example illustrates this method for splitting a concave polygon  Example Vector Method far Splitting Concave Polygons  shows a concave polygon with six edges Edge vectors for this poly gon can be expressed as E    E    E   Ey    E    BE     where the z component is since all edges are in the xy plane The cross product E x E for two successive edge vectors is a vector perpendicular to the xy plane with z component equal to E Ey  EE jy  E x E   D E x E     E x E    E x E    E x E    E x E    Since the cross product E x E has a negative z component we split the polygon along the line of vector E  The line equation for this edge has a slope of and a y intercept of  We then determine the intersection of this line and the other  Splitting a concave polygon using the rotational method After rotating V3 onto the x axis we find that V is below the x axis So we split the polygon along the line of Vy  polygon edges to split the polygon into two pieces No other edge cross products are negative so the two new polygons are both convex  We can also split a concave polygon using a rotational method Proceeding counterclockwise around the polygon edges we translate each polygon vertex V in turn to the coordinate origin We then rotate in a clockwise direction so that the next vertex V  is an the x axis If the next vertex V   is below the x axis the polygon is concave We then split the polygon into two new polygons along the x axis and repeat the concave test for each of the two new polygons Otherwise we continue to rotate vertices on the x axis and to test for negative y vertex values  illustrates the rotational method for splitting a concave polygon  To clip polygons we need to modify the line clipping procedures discussed in the previous section A polygon boundary processed with a line clipper may be displayed as a scries of unconnected line segments   depending on the orientation of the polygon to the clipping window What we really want to dis play is a bounded area after clipping as in  For polygon clipping we re quire an algorithm that will generate one or more closed areas that are then scan converted for the appropriate area fill The output of a polygon clipper should be a sequence of vertices that defines the clipped polygon boundaries    Lo eee  Display of a polygon processed by a Balore Clipping After Clipping line clipping algorithm Section Polygon Clipping  Chapter Two Dimensional Viewing  Original Polygon   wr  Display of a correctly clipped Before Clipping After Clipping polygon  Sutherland Hadgeman Polygon Clipping We can correctly clip a polygon by processing the polygon bound ry as a whole against each window edge This could be accomplished by processing all poly gon vertices against each clip rectangle boundary in turn Beginning with the ini tial set of polygon vertices we could first clip the polygon against the left rectan gle boundary to produce a new sequence of vertices The new set of vertices could then be successively passed to a right boundary clipper a bottom bound ary clipper and a top boundary clipper as in  At each step a new se quence of output vertices is generated and passed to the next window boundary clipper  There are four possible cases when processing vertices in sequence around the perimeter of a polygon As each pair of adjacent polygon vertices is passed to a window boundary clipper we make the following tests  If the first vertex is outside the window boundary and the second vertex is inside both the intersec tion point of the polygon edge with the window boundary and the second vertex are added to the output vertex list  If both input vertices are inside the win dow boundary only the second vertex is added to the output vertex list  If the first vertex is inside the window boundary and the second vertex is outside only the edge intersection with the window boundary is added tu the output vertex list  If both input vertices are outside the window houndary nothing is added to the output list These four cases are illustrated in  for successive pairs of polygon vertices Once all vertices have been processed for one clip window boundary the output hst of vertices is clipped against the next window bound ary  Clip Left Clip Top  Clipping a polygon against successive window boundaries   a yDaras   XeWM 4dOP  UTNM Jdop q eBbpga d z3domM eptsut qut bv Ode N sugep ebpa  do  woqjog 4ySta ajeT  umus zapaddAy  Arepunog Mopurm SIY 10J ISI XOWIA Jndyno ayy ur spnod ayy jaqry pasn o78 siaquinu pout xayaa yim Sunieys Mopulm eyo Arepumog 1a au surese uoSAjod e Buiddy aand14  MODULAR  Arepunog yea ysuteZe paddy syutod yse pue ysiy ayy Aq pauyap seury sdip autynor Busop e passadoid uaaq aaey saotien uo8Ajod Tle tay Arepunog yey ysuteSe paddy yurod ys1y ayy Arepunog MOPUIM Yea JO saloys JUTOg Aszy Aeue ayy syutod jo Aerie jndyno ay owl pasayua uay st sare punog Mopul je ysuteze Butddips saatains yey yutod Auy a8vys Buiddy jxau ay  passed st yl MopulM ayy apisut st d J  adeys Sutiddyp yxau ay  passed pue payepngyes st uolpasiaqul ay  Arepunog MOpuUIA sty sassop Azepunog s pue d syuiodpua kq pauyap aul ay jj Arepunog mMopulM jsiy ay ysurese Butddips JO AUN JUTOdAT TO ay  d Xayiaa yea sassed autos urew ayy Arepunoq MopulM dip yses 10j paddija sem yey yurod yuaoas jsou ay sprosar s Aeue uy yoeoidde Zutddip aurjadid ay sayexjsuowap ounpasoid ZuIMOT OS ay  ssaddya Arepunog jo auyedid e ySnoryy Ul Santen UOsATOd ayy Jo UCIssarZoid ay a esnyyI 2m  SLY U MOpPULM dip e YM squtod UOKQDeSJa UI SyI pure UOsATOd ev smoys amsi4z auyadid ayy ur anuguoo jou saop yurod ay  asimsayyQ sueddyo Are punog moj Te 4q Arepunog MopulM  ud Jo apIsul aq peuluLiajap uaaq uaaq sey U saye Afuo 3st xaqea Ndno ayy pappe st Quod uoTdessayUI payeMs eo 10 xoyed jndur ue saya yutod y sauynos Zutddip yo auryjedid e pue sossaz0id afsuis e siossadoid jayjered yum auop aq ues siqy aeddro Arepunog yxau ayy uO saoquea paddy ay Butssed pure days yora ye saoysaa enplarpul Buid dyp Ajduns Aq syst xayaa yndyno ayerpaunaqut ayy ayeutums ued aay Arepunoqg MOpUIM Yea ysuTeZe paddy st uoBAjod e se saotyre Jo yst yndyno ue Joy aBeI0 s dn 8unjas samnbar paquosap jsnl aaey am se wyyo08je ay Suyuewarduiy  Azepuncg mopulm 4xau ay oy ssadoid ayy yead a1 pinom am sjutod panes aay ayy Butsy julod uoysasJajUI ayy aAeS pur puy am OS APIS NO ST Xa IaA PUY PUL Y XIS al  paaes are osje Aay pue aprsul aq pauluLiajap are  pue p saorplaa  XaPAA puke yuod uoTpasJayUl ayy YO aves pue VoSasiayt ay ayeNz e aM ApIsUT st YONYM  Xayaa Sucre Suraocpy Are punog ayy Jo apisyno ay UO aq pumoy ale Zz pur  saotwan Arepunog MopuIM Ye ay ysulese  BLY ul eare ayy Bunssadoid Aq poyyaur sy ayeaysnyy ay  Arepunog mopumm yet ay ysureSe saoysaa u08Ajod jo sited Jo Sutssazoid aatssa2ons 24n314  audu BARS  A OARS Ta oape ALA 2aes 1nd  ul ul ul ur ino  A polygon overlapping a rectangular clip window   eS  Top  ee Clipper out vy   v1   y vi  v5 Vy Mg I GN _ Vs  Vy _ V5  Vv   Processing the vertices of the polygon in  through a boundary clipping pipeline After all vertices are processed through the pipeline the vertex list for the clipped polygon is V3 Vz Vx Vi  case Left if p x  wMin x return FALSE  break case Right if p x  wMax x return FALSE  break case Bottom if p y  wMin y return FALSE  break case Top if p y  wMax y return FALSE  break   return TRUE   int cross wcPt2 pl wcPt2 p2 Edge b dcPt wMin dcPt wMax  if inside pl b wMin wMax   inside p2 b wMin wMax  return FALSE  else return TRUE   wePt2 intersect wcPt2 pl wecPt2 p2 Edge b dcPt wMin dcPt wMax   weoPt2 iPt  float m  if pl x  p2 x  m  pl y  p2 y   pl x  p2 x  gwitch b  case Left iPt   wMin x iPt y  p2 y  wMin x  p2 x  m break  case Right iPt x  WMax x    iPt y  p2 y  wMax x  p2 x  m break case Bottom iPt y  wMin y if pl x  p2 x iPt x  p2 x  wMin y  p2 y  m else iPt x  p2 x break case Top iPt y  wMax y if pl x  p2 x iPt x  p2 x  wMax y  p2 y  m else iPt x  p2 x break vt return iPt   void clipPoint wcPt2 p Edge b dePt wMin dcPt wMax wePt2  pOut int  cnt wePt2  first  weoPt2  s  wePt2 iPt   If no previous point exists for this edge save this point   if  first b  first b   p else  Previous point exists If p and previous point cross edge find intersection Clip against next boundary if any If no more edges add intersection to output list   if cross p s b  b wMin wMax   iPt  intersect p s b  b wMin wMax  if b  Top clipPoint iPt b wMin wMax pout ent first s  else  pout ent  iPt   ent     s b  p   Save p as most recent point for this edge    For all if point is inside proceed to next clip edge if any  if inside p b wMin wMax  if b  Top clipPoint p b wMin wMax pOut cnt first  else  pout  cnt  p   ent     void closeClip dcPt wMin dcPt wMax wePt2  pout int  cnt wePt2  first  woPt2  s  wePt2 i Edge b  for b  Left b  Top bre  if cross s b  first b  b wMin wMax   i intersect s b  first b  b wMin wMax  if b  Top clipPoint i b l wMin wMax pOut cnt first s  else  pout ent  i  ent       int clipPolygon dcPt wMin dcPt wMax int n wcPt2  pIn wePt2  pout      first holds pointer toa first point processed against a clip edge  s holds most recent point processed against an edge   wePt2  first N_EDGE  int i ert  Q for i1 i n i   elipPoint pIn i      s N_EDGE  Left wMin wMax pOur  cnt first  s  closeClip  wMin wMax pOut  cnt first s  return cnt   Convex polygons are correctly clipped by the Sutherland Hodgeman algo rithm but concave polygons may be displayed with extraneous lines as demon strated in  This occurs when the clipped polygon should have two or more separate sections But since there is only one output vertex list the last ver tex in the list is always joined to the first vertex There are several things we could do to correctly display concave polygons For one we could split the con cave polygon into two or more convex polygons and process each convex poly gon separately Another possibility is to modify the Sutherland Hodgeman ap proach to check the final vertex list for multiple vertex points along any chp window baundary and correctly join pairs of vertices Finally we could use a more general polygon clipper such as either the Weiler Atherton algorithm or the Weiler algorithm described in the next section  Weiler Atherton Polygen Clipping Here the vertex processing procedures for window boundaries are modified so that concave polygons are displayed correctly This clipping procedure was de veloped as a method for identifying visible surfaces and so it can be apphed with arbitrary polygon clipping regions  The basic idea in this algorithm is that instead of always proceeding around the polygon edges as vertices are processed we sometimes want to follow the window boundaries Which path we follow depends on the polygon processing direction clockwise or counterclockwise and whether the pair of polygon ver tices currently being processed represents an outside to inside pair or an inside  Window PO Tee I i I J  we     Clipping the concave polygon in a   with the Sutherland Hodgeman Tees n clipper produces the two connevted a  b areas in b   v  stop Ve  b    Clipping a concave polygon a with the Weiler Atherton algorithm generates the two separate polygon areas in b  to outside pair For clockwise processing of polygon vertices we use the follow ing rules   For an outside to inside pair of vertices follow the polygon boundary   For an inside to outside pair of vertices follow the window boundary in a clockwise direction  In  the processing direction in the Weiler Atherton algorithm and the re sulting clipped polygon is shown for a rectangular clipping window  An improvement on the Weiler Atherton algorithm is the Weiler algorithm which applies constructive solid geometry ideas to clip an arbitrary polygon against any polygon clipping region  illustrates the general idea in this approach For the two polygons in this figure the correctly dipped polygon is calculated as the intersection of the clipping polygon and the polygon object  Other Polygon Clipping Algorithms Various parametric line clipping methods have also been adapted to polygon clipping And they are particularly well suited for clipping against convex poly gon clipping windows The Liang Barsky Line Clipper for example can be ex tended to polygon clipping with a general approach similar to that of the Suther land Hodgeman method Parametric line representations are used to process polygon edges in order around the polygon perimeter using region testing proce dures similar to those used in line clipping    polygon tipping  abject clipping pelygon   Clipping a polygon by determining  _ clipped the intersection of two polygon area areas  Section Polygon Clipping   Before Chipping After Clipping   Chipping a filled carcle  STRING   Before Clipt Clipping __  STRING  After Clipy Ci ipping   Text clipping using bounding rectang e about the entire string   Areas with curved boundaries can be clipped with methods similar to those dis cussed in the previous sections Curve clipping procedures will involve nonlin ear equations however and this requires more processing than for objects with linear boundaries  The bounding rectangle for a circle or other curved object can be used first to test for overlap with a rectangular clip window If the bounding rectangle for the object is completely inside the window we save the object If the rectangle is determined to be completely outside the window we discard the object In either case there is no further computation necessary But if the bounding rectangle test fails we can look for other computation saving approaches For a circle we can use the coordinate extents of individual quadrants and then octants for prelimi nary testing before calculating curve window intersections For an ellipse we can test the coordinate extents of individual quadrants  illustrates circle clipping against a rectangular window  Similar procedures can be applied when clipping a curved object against a general polygon clip region On the first pass we can clip the bounding rectangle of the object against the bounding rectangle of the clip region If the two regions overlap we will need to solve the simultaneous line curve equations to obtain the clipping intersection points   There are several techniques that can be used to provide text clipping in a graph ics package The clipping technique used will depend on the methods used to generate characters and the requirements of a particular application  The simplest method for processing character strings relative to a window boundary is to use the all or none string clipping strategy shown in  If all of the string is inside a clip window we keep it Otherwise the string is dis carded This procedure is implemented by considering a bounding rectangle around the text pattern The boundary positions of the rectangle are then com pared to the window boundaries and the string is rejected if there is any overlap This method produces the fastest text clipping  An alternative to rejecting an entire character string that overlaps a window boundary is to use the aff or none character clipping strategy Here we discard only those characters that are not completely inside the window   In this case the boundary limits of individual characters are compared to the window Any character that either overlaps or is outside a window boundary is clipped  A final method for handling text clipping is to clip the components of indi vidual characters We now treat characters in much the same way thal we treated lines If an individual character overlaps a clip window boundary we clip off the parts of the character that are outside the window   Outline character fonts formed with line segments can be processed in this way using a line clipping algorithm Characters defined with bit maps would be clipped by com paring the relative position of the individual pixels in the character grid patterns to the clipping boundaries  So far we have considered only procedures for clipping a picture to the interior of a region by eliminating everything outside the clipping region What is saved by these procedures is inside the region In some cases we want to do the reverse that is we want to clip a picture to the exterior of a specified region The picture parts to be saved are those that are outside the region This is referred to as exte rior clipping  A typical example of the application of exterior clipping is in multiple window systems To correctly display the screen windows we often need to apply both internal and external clipping  illustrates a multiple window display Objects within a window are clipped to the interior of that win dow When other higher priority windows overlap these objects the objects are also clipped to the exterior of the overlapping windows  Exterior clipping is used also in other applications that require overlapping pictures Examples here include the design of page layouts in advertising or pub lishing applications or for adding labels or design patterns to a picture The tech nique can also be used for combining graphs maps or schematics For these ap plications we can use exterior clipping to provide a space for an insert into larger picture   Procedures for clipping objects to the interior of concave polygon windows can also make use of external clipping  shows a line P P that is to be clipped to the interior of a concave window with vertices V V V V Vs Line P P can be clipped in two passes  First P P is clipped to the interior of the convex polygon V V V3V to yield the clipped segment P P   b   Then an external clip of P P  is performed against the convex polygon V V V to yield the final clipped line segment P P    SUMMARY In this chapter we have seen how we can map a two dimensional world coordinate scene to a display device The viewing transformation pipeline in   A multiple window screen display showing examples of both interior and exterior clipping  Courtesy of Sun Microsystems   Summary  STRING oh STRING STRING  Before Clipping  NG oi TRING STRING   After Clipping Ligure 29 Text clipping using a bounding rectangle about individual characters  TRING  on o  oe Before Clipping  TRING  S After Clipping  Text clipping performed on the components of individual characters   Interior Clip tb    Exterior Clip c  Clipping line P P to the interior of a concave polygon with vertices V V V4V4Vs a  using convex polygons V V V V  hb and V_V V  c  to produce the clipped line P P   cludes constructing the werld coordinate scene using modeling transformations transferring world cocrdinates to viewing coordinates mapping the viewing coordinate descriptions f objects to normalized device coordinates and finally mapping to device coordinates Normalized coord nates are specified in the range from to and thev are used to make viewing packages independent ot particular output devices Viewing coordinates are specified by giving the werld coordinate position of the viewing origin and the view up vector that defines the directior of the viewing y axis These parameters are used to construct tne viewing transforma tion matrix that maps world coordinate object descriptions to viewing coordi nates  A window is then set up in viewing coordinates and a viewport is specified in normalized device coordinates Typically the window and viewport are rec tangles in standard position rectangle boundaries are parallel to the coordinate axes  The mapping from viewing coordinates to normal zed device coordinates is then carried out so that relative positions in the window are maintained in the viewport  Viewing functions in a graphics programming package are used to create one or more sets of viewing parameters One function is typically provided tu calculate the elements of the matrix for transforming world coordinates to view ing coordinates Another function is used to set up the window to viewport transformation matrix and a third function can be used to specify combinations of viewing transforma ions and window mapping in a viewing table We can then select different viewing combinations by spo ifving particular view indices listed in the viewing table  When objects are displayed on the output device all parts of a scene our side the window and the viewport are clipped oft unless we set clip parameters to turn off clipping In many packages clipping Is Gone in normalized device co ordinates so that all transformations can be concatenated into a single transfor mation operation before applying the clipping alponthms The clipping region is commonly referred to as the clipping window or as the clisping rectangle when the window and viewport are standard rectangles Several algorithms have been developed for clipping objects against the clip windew boundaries  Line clipping algorithms include the Cohen Sutherland method the Liang Barsky method and the Nicholl Lee Nicholl method The Cohen Suther land method is widely used since it was one of the first line clipping algorithms to be developed Region codes are used to identify the position of line endpoints Telative to the rectangular clipping window boundaries Lines that cannot be im mediately identified as completely ins de the window or completely outside are then chpped against window boundaries Liang and Barsky use a parametric line representation similar to that of the earlier Cyrus Beck algorithm to set up a more efficient line clipping procedure that reduces intersectian calculations The Nicholl Lee Nicholl algorithm uses more region testing in the xy plane to reduce intersection calculations even further Parametric liie clipping is easily extended to convex clipping windows and to three dimensional clipping windows  Line clipping can also be carried out for concave polygon clipping win dows and for chpping windows with curved boundaries With concave poly gons we can use either the vector method or the revational method to split a con cave polygon into a number of convex polygons With curved clipping windows we calculate line intersections using the curve equations  Polygon clipping algorithms include the Sutherland Hodgeman method the Liang Barsky method and the Weiler Atherton method In the Suther land Hodgeman clipper vertices of a convex polygon are processed in order against the four rectangular window boundaries to produce an output vertex list for the clipped polygon Liang and Barsky use parametric line equations to repre sent the convex polygon edges and they use similur testing to that performed n line clipping to produce an outpul vertex list for the clipped polygon Both the Weiler Atherland method and the Weiler method correctly clip both convex and concave polygons and these polygon clippers also allow the clipping window ov be a general polygon The Weiler Atherland algorithm processes polygon ver tices in order to produce one or more lists of output polygon vertices The Weiler method performs clipping by finding the intersection region of the two polygons  Objects with curved boundaries are processed against rectangular clipping windows by calculating intersections using the curve equations These clipping procedures are slower than line clippers or polygon clippers because the curve equations are nonlinear  The fastest text clipping method is to completely clip a string if any part of the string is outside any window boundary Another method for text clipping is to use the all or none approach with the individual characters in a string A third method is to apply either point line polygon or curve clipping to the individual characters in a string depending on whether characters are defined as point prids or as outline fonts  In some applications such as creating picture insets and managing multi ple screen windows exterior clipping is performed In this case all parts of a scene that are inside a window are clipped and the exterior parts are saved  summary  Chapter Two Dimensional Viewing  REFERENCES Line clipping algorithms are discussed in Sproul and Sutherland  Cyrus and Beck  and Liang and Barsky  Methods for improving the speed of the Cohen Sutherland fine clipping algorithm are given in Duvanenko  Polygon clipping methods are presented in Sutherland and Hodgeman  and in Liang and Barsky  General techniques for clipping arbitrarily shaped polygons against each other are given in Weiler and Atherton  and in Weiler   Two dimensional viewing operations in PHIGS are discussed in Howard et al   Gask ins  Hopgood and Duce  and Blake  For information on GKS viewing operatians see Hopgood et al  and Enderle et al   Write a procedure to to implement the evaluateViewOrientationMatrix func tion that calculates the elements of the matrix for transforming world coordinates to viewing coordinates given the viewing coordinate origin Py and the view up vector V Derive the window to viewport transformation equations by first scaling the win dow to the size of the viewport and then translating the scaled window to the view port position  Write a procedure to mplement the evaluateViewMappingMatrix function that calculates the elements of a matrix for performing the window to viewport transforma tion  Write a procedure to implement the set ViewRepresentat ion function to concate nate viewMatrix and viewMappingMatrix and to store the result referenced by a spetitied view index in a viewing table Write a set of procedures to implement the viewing pipeline without clipping and without the workstation transformation Your program should allow a scene ta be con structed with modeling coordinate transformations a specified viewing system and a specified window v ewport pair As an option a viewing table can be implemented to store different sets of viewing transformation parameters  Derive the matrix representation for a workstation transformation  Write a set of procedures to implement the viewing pipeline without clipping but in cluding the workstation transformation Your program should allow a scene to be con structed with modeling coordinate transformations a specified viewing system a specified window viewport pair and workstation transformation parameters For a given world coordinate scene the composite viewing transformation matrix should transform the scene to an output device for display  tmplement the Cohen Sutherland line clipping algorithm  Carefully discuss the rationale behind the various tests and methods for calculating the intersection parametets u and u in the Liang Barsky line clipping algorithm   Compare the number of arithmetic operations performed in the Cohen Sutherland and the Liang Barsky line clipping algorithms for several different line orientations rel ative to a clipping window  Write a procedure to implement the Liang Barsky line c ipping algorithm  Devise symmetry transformations for mapping the intersection calculations for the three regions in  to the other six reg ons of the xy plane Set up a detailed algorithm for the Nicholl Lee Nicholl approach to line clipping for any input pair of line endpoints  Compare the number of arithmetic operations performeo in NLN algorithm to both the Cohen Sutherland and the Liang Barsky line clipping algorithms for several different line orientations relative to a clipping window Write a routine to identify concave polygons by calculating cross products of pairs of edge vectors  Write a routine to split a concave polygon using the vector method  Write a routine to split a concave polygon using the rotational method  Adapt the Liang Barsky line clipping algorithm to polygon clipping Set up a detailed algorithm for Weiler Atherton polygon clipping assuming that the clipping window js a rectangle in standard position   Devise an algorithm for Weiler Atherton polygon clipping where the clipping win  dow can be any specified polygon   Write a routine to clip an ellipse against a rectangular window  Assuming that all characters in a text string have the same width develop a text clip ping algorithm that clips a string according ta the all or none character clipping strategy  Develop a text clipping algorithm that clips individual characters assuming that the characters are defined in a pixel grid of a specified s ze  Write a routine to implement exterior clipping on any part of a defined picture using any specified window  Write a routine to perform both interior and exterior clipping given a particular win dow system display Input to the routine is a set of window positions on the screen the objects to be displayed in each window and the window priorities The individuat objects are to be clipped to fit into their respective windows then clipped to remove parts with overlapping windows of higher display priority  Exercises  CHAPTER    Structures and Hierarchical Modeling  F or a great many applications it is convenient to be able to create and ma nipulate individual parts of a picture without affecting other picture parts Most graphics packages provide this capability in one form or another With the ability to define each object ina picture as a separate module we can make modi fications to the picture more easily In cesign applications we can try out differ ent positions and orientations for a component of a picture without disturbing other parts of the picture Or we can take out parts of the picture then we can easily put the parts back into the display at a later time Similarly in modeling applications we can separately create and position the subparts of a complex ob ject or system into the overall hierarchy And in animations we can apply trans formations to individual parts of the scene so that one object can be animated with one type of motion while other objects in the scene move differently or re main stationary  A labeled set of output primitives and associated attributes in PHIGS is called a structure Other commonly used names for a labeled collection of primitives are segments GKS and olyects Graphics Library on Silicon Graphics systems  In this section we consider the basic structure managing functions in PHIGS Similar operations are available in other packages for handling labeled groups of primi tives in a picture  Basic Structure Functions When we create a structure the coordinate positions and attribute values spec  fied for the structure are stored as a labeled group ina system structure list called the central structure store We create a structure with the function openStructure id  The label for the segment is the positive integer assigned to parameter id In PHIGS  we can use character strings to label the structures instead of using inte ger names This makes it easier to remember the structure identifiers After all primitives and attributes have been listed the end of the structure is signaled with the clogeStructure statement For example the following program Chapter Structures and Hierarchical Modeling statements define structure as the line sequence specified in polyline with the designated line type and color  openStructure ic  setLinetype 1t  setPolylineColourIndex lc  polyline nr pts  cleseStructure  Any number of structures can be created for a picture but only one structure can be open in the creation process at a time Any open structure must be closed be fore a new structure can be created This requirement eliminates the need for a structure identification number in the closeStructure statement  Once a structure has been created it can be displayed ona selected output device with the function postStructure ws id priority  where parameter ws is the workstation identifier ic is the structure name and priority is assigned a rea value in the range from to Parameter pr ority sets the display priority relative to other structures When two structures overlap onan output display device the structure with the higher priority will be visible For example if structures and are posted to workstation with the following priorities postStructure postStructure then any parts of structure that overlap structure will be hidden since struc ture has higher prioritv If two structures are assigned the same priority value the last structure to be posted is given display precedence When a structure is posted to an active workstation the primitives in the structure are scanned and interpreted for display on the selected output device video monitor laser printer etc  Scanning a structure list and sending the graphical output to a workstation is called traversal A list of current attribute values for primitives is stored in a data structure called a traversal state list As changes are made to posted structures both the system structure list and the tra versal state list are updated This automatically modifies the display of the posted structures on the workstation  Ta remove the display of a structure from a part cular output device we in voke the function unpostStructure iws id  This deletes the structure from the active list of structures for the designated out put device but the system structure list is not affected On a raster system a structure is removed from the display by redrawing the primitives in the back ground color This process however may also affect the display of primitives fram other structures that overlap the structure we want to erase To remedy this we can use the coordinate extents of the various structures in a scene to deter mine which ones overlap the structure we are erasing Then we can simply re draw these overlapping structures after we have erased the structure that is to be unposted All structures can be removed from a selected output device with unpostAllStructures ws  If we want to remove a particular structure from the system structure list we accomplish that with the function deleteStructure id  Of course this also removes the display of the structure from all posted output devices Once a structure has been deleted its name can be reused for another set of primitives The entire system structure list can be cleared with deleteAllStructures It is sometimes useful to be able to relabel a structure This is accomplished with changeStructureldentifier oldID newID  One reason for changing a structure label is to consolidate the numbering of the structures after several structures have been deleted Another is to cycle through a set of structure labels while displaying a structure in multiple locations to test the structure positioning  Setting Structure Attributes We can set certain display characteristics for structures with workstation filters The three properties we can set with filters are visibility highlighting and the ca pability of a structure to be selected with an interactive input device  Visibility and invisibility settings for structures on a particular workstation for a selected device are specified with the function setInvisibilityFilter ws devCode invisSet visSet  where parameter invisSet contains the names of structures that will be invisi ble and parameter visSet contains the names of those that will be visible With the invisibility filter we can turn the display of structures on and off at selected workstations without actually deleting them from the workstation lists This al lows us for example to view the outline of a building without all the interior de tails and then to reset the visibility so that we can view the building with all in ternal features included Additional parameters that we can specify are the number of structures for each of the two sets Structures are made invisible on a raster monitor using the same procedures that we discussed for unposting and for deleting a structure The difference however is that we do not remove the structure from the active structure list for a device when we are simply making it invisible  Highlighting is another convenient structure characteristic In a map dis play we could highlight all cities with populations below a certain value or fora Section Structure Concepts Chapter Structures and Hierarchica Modeling  landscape layout we could highlight certain varieties of shrubbery or in a circuit diagram we could highlight all components within a specific voltage range This is done with the function setHighlightingFilter ws devCode highlightSet nohighlightSet  Parameter highlightSet contains the names of the structures that are to be highlighted and parameter nohighlightSet contains the names of those that are not to be highlighted The kind of highlighting used to accent structures de pends on the type and capabilities of the graphics system For a color video mon itor highlighted structures could be displayed in a brighter intensity or in a color reserved for highlighting Another common highlighting implementation is to turn the visibility on and off rapidly so that blinking structures are displayed Blinking can also be accomplished by rapidly alternating the intensity of the highlighted structures between a low value and a high value  The third display characteristic we can set for structures is pickability This refers to the capability of the structure to be selected by pointing at it or position ing the screen cursor over it If we want to be sure that certain structures in a dis play can never be selected we can declare them to be nonpickable with the pick ability filter In the next chapter we take up the topic of input methods in more detail  Often we would like to modify a structure after it has been created and closed Structure modification is needed in design applications to try out different graph ical arrangements or to change the design configuration in response to new test data  If additional primitives are to be added to a structure this can be done by simply reopening the structure with the openStructure nclon and append ing the required statements As an example of simple appending the following program segment first creates a structure with a single fil area and then adds a second fill area to the structure  openStructure shape  setiInteriorStyle solid  setInteriorColouriIndex  fillArea ni vertsl  closeStructure  openStructure shape  setintericrStyle hollow  fillArea nz verts2  closeStructure  This sequence of operations is equivalent to initially creating the structure with both fill areas openStructure shape  setinteriorStyle solid  setInteriorColourIndex  fillArea nl vertsl  setInteriorStyle hollow  fillArea n2 verts2  closeStructure  In addition to appending we may also want sometimes to delete certain items in structure to change primitives or attribute settings or to insert items at selected positions within the structure General editing operations are carried out by accessing the sequence numbers for the individual components of a structure and setting the edit mode  Structure Lists and the Element Pointer Individual items in a structure such as output primitives and attribute values are referred to as structure elements or simply elements Each element is as signed a reference position value as it 1s entered into the structure  shows the storage of structure elements and associated position numbers created by the following program segment  openStructure gizmo  setLinetype ltl  setPolylineColourIndex 1cl1  polyline nl ptsl  setLinetype 1t2  setPolylineColcurIndex 1c2  polyline n2 pts2  closeStructure  Structure elements are numbered consecutively with integer values starting at and the value indicates the position just before the first element When a structure is opened an element pointer is set up and assigned a position value that can be used to edit the structure If the opened structure is new not already existing in the system structure list  the element pointer is set to If the opened structure does already exist in the system list the element pointer is set to the po sition value of the last element in the structure As elements are added to a struc ture the element painter is incremented by  We can set the value of the element pointer to any position within a struc ture with the function setElementPointer k  gizmo structure  setLinetype 1tL setPolylineColcurindex 1c1   setPolylineColourIndex  wp element  Element position values for pointer polyline n2 pts2 structure gizmo  Section Uditing Structures  Chapter Structures and Hierarchical Modeling where parameter k can be assigned any integer vajue from to the maximum number of elements in the structure It is also possible to position the element pointer using the following offset function that moves the pointer relative to the current position  offsetElementPo nter dk  with dk assigned a positive or negative integer offset from the present position of the pointer Once we have positioned the element pointer we can edit the struc ture at that position  Setting the Edit Mode Structures can be modified in one of two possible modes This is referred to as the edit mode of the structure We set the value of the edit mode with setEd tMode moce  where parameter mode is assigned either the value fsert or he value replace  Inserting Structure Elements When the edit mode is set to msert the next item entered into a structure will be placed in the position immediately following the element pointer Elements in the structure list tollowing the inserted item are then automatically renumbered To illustrate the insertion operation let s change the standard line width currently in structure g zmo   to some other value We can do this by in serting a line width statement anywhere before the first polyline command  openStructure gizmo  setEditMode insert  setElemertPointer  setLinewidth lw  closeStructure   shows the modified element list of gizmo created by the previous in sett operation After this insert the element pointer is assigned the value the position of the new line width attribute  Also all elements after the line width statement have been renumbered starting at the value  gizmo structure  element pointer getLinewidth lw  getLinetype t1  setPolylinefolourIndex lcl    Modified element list and position of the clement pointer after inserting a line width attribute into structure gizmo  polyline nl pts1  setLinetype 1t2  setPolylineColourIndex 1c2  swoon ewnad polyline n2 pts2   When a new structure is created the edit mode is automatically et to the value insert Assuming the edit mode has not been changed from this iefault value before we reopen this structure we can append items at the end of the ele ment list without setting values for either the edit mode or element pointer as demonstrated at the beginning of Section This is because the edit mode re mains at the value insert and the element pointer for the reopened structure points to the last element in the list  Replacing Structure Elements When the edit mode is set to the value replace the next item entered into a struc ture is placed at the position of the element pointer The element originally at that position is deleted and the value of the element pointer remains unchanged  As an example of the replace operation suppose we want to change the color of the second polyline in structure gizmo   We can do this with the sequence  openStructure gizmc  setEditMode replace  setElementPointer  setPolylineColourIndex lc2New  closeStructure   shows the element list of gizmo with the new color for the second polyline After the replace operation the element pointer remains at position the position of the new line color attribute  Deleting Structure Elements We can delete the element at the current position of the element pointer with the function deleteEiement This removes the element from the structure and sets the value of the element pointer to the immediately preceding element  As an example of element deletion suppose we decide to have both poly lines in structure gizmo   displayed in the same color We can accom plish this by deleting the second color attribute  gizmo structure  setLinetype    Modified element list and position of the element pointer after changing the color of the second polyline in structure gizmo  setPolylineColourIndex 1c1   setLinetype 1t2  slement pointer get PolylineColourindex 1c2New   polyline nl ptsl   polyline n2 pts2  Section Editing Structures No Chapter Structures and Hierarchical Modeling openStructure gizmo  setElementFointer  deleteElement  closeStructure  The element pointer is then reset to the value and all following elements are renumbered as shown in  A contiguous group of structure elements can be deleted with the function deleteElementRange kl k2  where integer parameter k1 gives the beginning position number and k2 speci fies the ending position number For example we can delete the second polyline and associated attributes in structure gizmo with deleteElementRange  And all elements in a structure can be deleted with the function emptyStructure iid  Labeling Structure Flements Once we have made a number of modifications to a structure we could easily lose track of the element positions Deleting and inserting elements shift the ele ment position numbers To avoid having to keep track of new position numbers as modifications are made we can simply label the different elements in a struc ture with the function label k  where parameter k is an integer position identifier Labels can be inserted any where within the structure list as an aid to locating structure elements without re ferring to position number The label function creates structure elements that have no effect on the structure traversal process We simply use the labels stored in the structure as editing references rather than using the individual element po sitions Also the labeling of structure elements need not be unique Sometimes it is convenient to give two or more elements the same label value particularly if the same editing operations are likely to be applied to several positions in the structure  gizmo structure  setLinetype 1t1 i   eetPolylinecolourindex  ae  Modified element Jist and position  polyline ni ptsl  j element oortinee ie of the element pointer after deleting pointer Bet iinetype  the color attribute statement for the  polyline n2 pts2   second polyline in structure gizmo  To illustrate the use of labeling we create structure labeledGizmo in the following routine that has the elements and position numbers as shown in   openStructure labeledGizmo  label objectlLinetype  setLinetype 1t1  label objectlcolor  setPolylineColouriIndex 1cl1  label objectl  polyline nl pts1  label object2Linetype  setLinetype 1t2  label object2color  setPolylineColourIndex 1c2  label object2  polyline n2 pts2  closeStructure  Now if we want to change any of the primitives or attributes in this structure we can do it by referencing the labels Although we have labeled every item in this structure other labeling schemes could be used depending on what type and how much editing is anticipated For example all attributes could be lumped under one label or all color attributes could be given the same label identifier  A label is referenced with the function setElementPointerAtLabel k  which sets the element pointer to the value of parameter k The search for the label begins at the current element pointer position and proceeds forward through the element list This means that we may have to reset the pointer when reopening a structure since the pointer is always positioned at the last element in a reopened structure and label searching is not done backward through the ele ment list If for instance we want to change the color of the second object in structure labeledGizmo we could reposition the pointer at the start of the ele ment list after reopening the structure to search for the appropriate calor at tribute statement label  iabeledGizme structure label objectiLinetype setLinetype 1tl  label abjeccicolor setPolylineColourIndex 1c1 label objectl  polyline nl ptsi  label object2Linetype  a  getLinatype 1t2 label object2Color  setPolylineColourindex   label object2 element __ pointer  o   A set of labeled objects and associated position numbers stored in structure labeledGizmo   polyline n2 pts2   Section Editing Structures Chapter Steuctures and Hierarchical Modeling openStructure iabeledGizmo  setElementPointer  setEditMode replace  setElementPointerAtLabel object2Color  offsetElementPointer  setPolylineColourIndex 1c2New  closeftructure  Deleting an item referenced with a label is similar to the replacement opera tion illustrated in the last openStructure routine We first locate the appropri ate label and then offset the pointer For example the color attribute for the sec ond polyline in structure labeledGizmo can be deleted with the sequence openStructure labeledGizmo  setElementPointer  setEditMode replace  setElementPointeraAtLabel object2Color  offsetElementPointer  deleteElement  closeStructure  We can also delete a group of structure elements between specified labels with the function deleteElementsBetweenLabels  k1 k2 After the set of elements is deleted the element pointer is set to position k1 Copying Elements from One Structure to Another We can copy all the entries from a specified structure into an open structure with copyAllLElementsFromStructure id The elements from structure id are inserted into the open structure starting at the position immediately following the element pointer regardless of the setting of the edit mode When the copy operation is complete the element pointer is set to the position of the last item inserted into the open structure   An important use of structures is in the design and representation of different types of systems Architectural and engineering systems such as building lay outs and electronic circuit schematics are commonly put together using com puter aided design methods Graphical methods are used also for representing economic financial organizational scientific social and environmental systems Representations for these systems are often constructed to simulate the behavior of a system under various conditions The outcome of the simulation can serve as an instructional tool or as a basis for making decisions about the system To be ef fective in these various applications a graphics package must possess efficient methods for constructing and manipulating the graphical system representations The creation and manipulation of a system representation is termed model ing Any single representation is called a model of the system Models for a sys tem can be defined graphically or they can be purely descriptive such as a set of equations that defines the relationships between system parameters Graphical models are often referred to as geometric models because the component parts of a system are represented with geometric entities such as lines polygons or cir cles We are concerned here only with graphics applications so we will use the term model to mean a computer generated geometric representation of a system  Model Representations  shows a representation for a logic circuit illustrating the features com mon to many system models Component parts of the system are displayed as geometric structures called symbols and relationships between the symbols are represented in this example with a network of connecting lines Three standard symbols are used to represent logic gates for the Boolean operations and or and not The connecting lines define relationships in terms of input and output flow from left to right through the system parts One symbol the and gate is dis played at two different positions within the logic circuit Repeated positioning of a few basic symbols is a common method for building complex models Each such occurrence of a symbol within a model is called an instance of that symbol We have one instance for the or and not symbols in Fig and two instances of the and symbol  In many cases the particular graphical symbols chosen to represent the parts of a system are dictated by the system description For circuit models stan dard electrical or logic symbols are used With models representing abstract con cepts such as political financial or economic systems symbols may be any con venient geometric pattern  Information describing a model is usually provided as a combination of geometric and nongeometric data Geometric information includes coordinate positions for locating the component parts output primitives and attribute func tions to define the structure of the parts and data for constructing connections between the parts Nongeometric information includes text labels algorithms de scribing the operating characteristics of the model and rules for determining the relationships or connections between component parts if these are not specified as geometric data     Model of a logic circuit  Section Basic Modeling Concepts  Chapter Structures and Hierarchical Madeling There are two methods for specifying the information needed to construct and manipulate a model One method is to store the infomation in a data struc ture such as a table or linked list The other method is to specify the information in procedures In general a model specification will contain both data structures and procedures although some models are defined completely with data struc tures and others use only procedural specifications An application to perform solid modeling of objects might use mostly information taken from some data structure to define coordinate positions with very few procedures A weather model on the other hand may need mostly procedures to calculate plots of tem perature and pressure variations  As an example of how combinations of data strictures and procedures can be used we consider some alternative model specifications for the logic circuit of  One method is to define the logic components in a data table Table  with processing procedures used to specify how the network connections are to be made and how the circuit operates Geometric data in this table include coor dinates and parameters necessary for drawing and pasitioning the gates These symbols could all be drawn as polygon shapes or they could be formed as com binations of straight line segments and elliptical arcs Labels for each of the com ponent parts also have been included in the table aithough the labels could be omitted if the symbols are displayed as commonly recognized shapes Proce dures would then be used to display the gates and construct the connecting lines based on the coordinate positions of the gates and a specified order for connect ing them An additional procedure is used to produce the circuit output binary values for any given input This procedure could be set up to display only the final output or it could be designed to display intermediate vutput values to il lustrate the internal functioning of the circuit  Alternatively we might specify graphical information for the circuit model in data structures The connecting lines as well as the gates could then be de fined in a data table that explicitly lists endpoints for each of the lines in the cir cuit A single procedure might then display the circuit and calculate the output At the other extreme we could completely define the model in procedures using no external data structures  Symbol Hierarchies Many models can be organized as a hierarchy of symbols The basic building blocks for the model are defined as simple geometric shapes appropriate to the type of model under consideration These basic symbols can be used to form composite objects called modules which themselves can be grouped to form higher level modules and so on for the various components of the model In the TABLE A DATA TABLE DEFINING THE STRUCTURE AND POSITION OF EACH GATE IN THE CIRCUIT OF FIG   Symbol Geometric Identitying Code Description Label Gate Coordinates and other paramete s and Gate i or Gate  not Gate  and simplest case we can describe a model by a one level hierarchy of component parts as in  For this circuit exarnple we assume that the gates are posi tioned and connected to each other with straight lines according to connection rules that are specified with each gate description The basic symbols in this hier archical description are the logic gates Although the gates themselves could be described as hierarchies formed from straight lines elliptical arcs and text that sort of description would not be a convenient one for constructing logic cir cuits in which the simplest building blocks are gates For an application in which we were interested in designing different geometric shapes the basic symbols could be defined as straight line segments and arcs  An example of a two level symbol hierarchy appears in  Here a fa cility layout is planned as an arrangement of work areas Each work area is out fitted with a collection of furniture The basic symbols are the furniture items worktable chair shelves file cabinet and so forth Higher order objects are the work areas which are put together with different furniture organizations An in stance of a basic symbol is defined by specifying its size position and orientation within each work area For a facility layout package with fixed sizes for objects only position and orientation need be specified by a user Positions are given as coordinate locations in the work areas and orientations are specified as rotations that determine which way the symbols are facing At the second level up the hi erarchy each work area is defined by specifying its size position and orientation within the facility layout The boundary for each work area might be fitted with a divider that encloses the work area and provides aisles within the facility  More complex symbol hierarchies are formed by repeated grouping of sym bol clusters at each higher level The facility layout of  could be extended to include symbol clusters that form different rooms different floors of a build ing different buildings within a complex and different complexes at widely sep arated physical locations  Modeling Packages Some general purpose graphics systems GKS for example are not designed to accommodate extensive modeling applications Routines necessary to handle modeling procedures and data structures are often set up as separate modeling packages and graphics packages then can be adapted to interface with the mod eling package The purpose of graphics routines is to provide methods for gener   A two level hierarchical description of a facility layout  ating and manipulating final output displays Modeling routines by contrast provide a means for defining and rearranging model representations in terms of symbol hierarchies which are then processed by the graphics routines for dis play Systems such as PHIGS and Graphics Library GL on Silicon Graphics equipment are designed so that modeling and graphics functions are integrated into one package  Symbols available in an application modeling package are defined and structured according to the type of application the package has been designed to handle Modeling packages can be designed for either two dimensional or three dimensional displays  illustrates a two dimensional layout used in cir cuit design An example of three dimensional molecular modeling is shown in  and a three dimensional facility layout is given in  Such three dimensional displays give a designer a better appreciation of the appearance of a layout In the following sections we explore the characteristic features of model ing packages and the methods for interfacing or integrating modeling functions with graphics routines    Two dimensional modeling layout used in circuit design  Courtesy of Summagraphics   One half of a stereoscopic image pair showing a three dimensional molecular model of DNA Data supplied by Tamar Schlick NYU and Wilma K Olson Rutgers University visualization by Jerry Greenberg SDSC  Courtesy of Stephanie Sides San Diego Supercomputer Center     A three dimensional view of an office layout Courtesy of Intergraph Corporation   A hierarchical model of a system can be created with structures by nesting the structures into one another to form a tree organization As each structure is placed into the hierarchy it is assigned an appropriate transformation so that it will fit properly into the overall model One can think of setting up an office facil ity in which furniture is placed into the various offices and work areas which in turn are placed into departments and so forth on up the hierarchy  Local Coordinates and Modeling Transformations In many design applications models are constructed with instances transformed copies of the geometric shapes that are defined in a basic symbol set Instances are created by positioning the basic symbols within the world coordinate refer ence of the model The various graphical symbols to be used in an application are each defined in an independent coordinate reference called the modeling coordi nate system Modeling coordinates are also referred to as local coordinates or sometimes master coordinates  illustrates local coordinate definitions Section Hierarchical Modeling with Structures Chapter Structures and Hierarchical Arrays for Chair Coordi nates Modeling  x Chair y Chair  Now Ww   wowww  for two symbols that could be used in a two dimensional facility layout applica tion   To construct the component parts of a graphical model we apply transfor mations to the local coordinate definitions of symbols to produce instances of the symbols in world coordinates Transformations applied to the modeling coordi nate definitions of symbols are referred to as modeling transformations Typi cally modeling transformations involve translation rotation and scaling to posi tion a symbol in world coordinates but other transformations might also be used in some applications  Modeling Transformations We obtain a particular modeling transformation matrix using the geometric transformation functions discussed in Chapter That is we can set up the indi vidual transformation matrices to accomplish the modeling transformation or we can input the transformation parameters and allow the system to build the matrices In either case the modeling package concatenates the individual trans formations to construct a homogeneous coordinate modeling transformation ma trix MT An instance of a symbol in world coordinates is then produced by ap plying MT to modeling coordinate positions P   to generate corresponding world coord inate positions P    P MT  Px  D Structure Hierarchies As we have seen modeling applications typically require the composition of basic symbols into groups called modules these modules may be combined into Arrays tor Worktable Coordinates  Worktable yWernsbn  J   Chair Coisuis Ler tipyiiticins  a 10 Workable fb    Objects defined in local coordinates  higher level modules and so on Such symbol hierarchies can be created by em bedding structures within structures at each successive level in the tree We can first define a module structure as a list of symbol instances and their transfor mation parameters At the next level we define each higher level module as a list of the lower module instances and their transformation parameters This process is continued up to the root of the tree which represents the total picture in world coordinates A structure is placed within another structure with the function executeStruccure id  Jo properly orient the structure we first assign the appropriate local transforma tion to structure id This is done with setLocalTransformation mlt type  where parameter mlt specifies the transformation matrix Parameter type is as signed one of the following three values pre post or replace to indicate the type of matrix composition to be performed with the current modeling transformation matrix If we simply want to replace the current transformation matrix with Imt we set parameter type to the value replace If we want the current matrix to be premultipled with the local matrix we are specifying in this function we choose pre and similarly for the value post The following code section illustrates a se quence of modeling statements to set the first instance of an object into the hier archy below the root node  createStructure id0Q  setLocalTransformation lmt  type execureStructure idl  closeStructure  The same procedure is used to instance other objects within structure id0 to set the other nodes into this level of the hierarchy Then we can create the next level down the tree by instancing objects within structure idl and the other structures that are in idO We repeat this process until the tree is complete The entire tree is then displayed by posting the root node structure id0 in the previ ous example In the following pracedure we illustrate how a hierarchical struc ture can be used to model an object  void main   enum  Frame Wheel Bicycle   int nPcs  wePt2 pts  pMatrix3 m    Routines to generate geometry  I extern void getWheelVertices int  nPts wePt2 prs   extern voic getframeVertices int  npts wcPt2 pts      Make the wheel structure   Section Hierarchical Modeling with Structures getWheelVertices nPts prsi epenStructure Wreeli setLineWidth   polyline nPts f ts  closeStructure   Make the frame structure  getFramevertices npts pts  openStructure iFrame  setLinewidth   polyline inFts  ts  closeStructure  Make the bic openStructure Bic   nclude the frame  executeStructure iFrame    Position and include rear wheel  matrixSetIdenti mi  m    L     setLocalTransformationMatrix m REPLACE  executeStructure wheel    Position and iaclude front wheel  m   mfl    O setLocalTransformationmatrix m REPLACE executeStructure Wheel  closeStructure  We delete a hierarchy with the function deleteStructureNetwork 1d  where parameter id rererences the root structure of the tree This deletes the root node of the hierarchy and all structures that have been placed below the root using the executeStyucture function assuming thet the hierarchy is orga nized as a tree  SUMMARY A structure also called a segment or an object in some systems is a labeled group of output statements and associated attributes Bv designing pictures as sets of structures we can easily add delete or manipwate picture components independently of each another As structures are created they are entcred into a central structure store Structures are then displayed by posting them to various output devices with assigned priorities When two structures overlap the struc ture with the higher pnority is displayed over the structt re with the lower prior itv  We can use workstation filters to set attributes such as visibility and high lighting for structures With the visibility filter we can turn off the display of a structure while retaining it in the structure list The highlighting filter is used to emphasize a displayed structure with blinking color or high intensity patterns  Various editing operations can be applied to structures We can reopen structures to carry out append insert or delete operations Locations in a struc ture are referenced with the element pointer In addition we mdividually label the primitives or attributes in a structure The term model in graphics applications refers to a graphical representa tion for some system Components of the system are represented as symbols de Exercises fined in loca  modeling coordinate reference frames Many models such as elec trical circuits are constructed by placing instances of the symbols at selected locations  Many medels are constructed as symbol hierarchies A bicycle for instance can be constructed with a bicycle frame and the wheels The frame can include such parts as the handlebars and the pedals And the wheels contain spokes rims and tires We can construct a hierarchial model by nesting structures For example we can set up a bike structure that contains a frame structure and a wheel structure Both the frame and wheel structures can then contain primitives and additional structures We continue this nesting down to structures that con tain only output primitives and attributes  As each structure is nested within another structure an associated model ing transformation can be set for the nested structure This transformation de scribes the operations necessary to properly orient and scale the structure to fit into the hierarchy  REFERENCES Structure operations and hierarchical modeling in PHIGS are discussed in Hopgoed and Duce  Howard et al   Gaskins  and Blake   For information on GKS segment operations see Hopgood  and Enderle et ai   EXERCISES  Write a procedure for creating and manipulating the intormation in a central structure store This procedure is to be invoked by functians such as openStructure deleteStructure and changeStructurelIdent ifier   Write a routine tor storing information in a traversal state fist KR  Write a routine for erasing a specified structure on a raster system given the coordi  nate extents for all displayed structures in a scene  Wrile a procedure to implement the unsost Structure function on a raster system   Write a procedure to implement the deleteStructure function on a rasler system   Write a procedure to implement highlighting as a blinking operation   Write a set of routines for editing structures Your routines should provide ror the fol lowing types of editing appending insening replacing and deleting structure ele ments   Discuss model representations that would be appropriate far several distinctly difter  ent kinds of systems Also discuss how graphical representations might be imple mented tor each system  Fora logi circuit modeling applicatior such as that in  give a detailed graph tcal description of the standard logic symbols to be used in constructing a display of a Circuit 10 Develop a modeling package for electrical design that will allow a user to position elecrical symbols within a circuit network Only translations reed be applied to place an instance of one of the electrical menu shapes into the network Once a component has been placed in the network it is to be connected to other specified components with straight line segments  11 Devwse a two dimensional facility layout package A nenu of furniture shapes is to be Chapter  Structures and Hierarchical Modeling  12  pravided to a des gner who can place the objects in ary location within a single room one level hierarchy  Instance transformations can be limited to translatiors and reta tions  Devise a two dimensional facility layout package that presents a menu of furniture shapes A two leve hierarchy is to be used so that furniture items can be placed into various work areas and the work areas can be arranged within a larger area Instance transformations may be limited to translations and rotations but scaling could be used if furniture items of different sizes are to be available CHAPTER oe  Graphical User Interfaces and Interactive Input Methods  T he human computer interface for most systems invalves extensive graph ics regardless ot the application Typically general systems now consist of windows pull down and pop up menus icons and pomting devices such as a mouse or spaceball for positioning the screen cursor Popular graphical user in terfaces include X Windows Windows Macintosh OpenLook and Motif These interfaces are used in a variety of applications including word processing spreadsheets databases and file management systems presentation systems and page layout systems In graphics packages specialized interactive dialogues are designed for individual applications such as engineering design architectural design data visualization drafting business graphs and artist s paintbrush pro grams For general graphics packages interfaces are usually provided through a standard system An example is the X Window System interface with PHIGS In this chapter we take a look at the basic elements of graphical user interfaces and the techniques for interactive dialogues We also consider how dialogues in graphics packages in particular can allow us to construct and manipulate pic ture components select menu options assign parameter values and select and position text strings A variety of input devices exists and general graphics packages can be designed to interface with various devices and to provide exten sive dialogue capabilities   For a particular application the user s mioael serves as the basis for the design of the dialogue The user s model describes what the system is designed to accom plish and what graph cs operations are available  t states the type of objects that can be displayed and how the objects can be manipulated For example if the graphics system is ta be used as a tool for architectural design the model de scribes how the package can be used to construct and display views of buildings by positioning walls doors windows and other building components Similarly for a facilitv layout system objects could be defined as a set of furniture items tables chairs etc  and the available operations would include those for posi tioning and removing different pieces of furniture within the facility layout And a circuit design prograni might use electrical or logic elements for objects with positioning operations available for adding or deletirg elements within the over all circuit design All information in the user dialogue is then presented in the language of the Section application In an architectural design package this means that all interactions The User Dialogue are described only in architectural terms without reference to particular data structures or other concepts that may be unfamiliar to an architect In the follow ing sections we discuss some of the general considerations in structuring a user dialogue  Windows and Icons  shows examples of common window and icon graphital interfaces Vi sual representations are used both for objects to be manipulated in an application and for the actions to be performed on the application objects  A window system provides a window manager interface for the user and functions for handling the display and manipulation of the windows Common functions for the window system are opening and closing windows reposition ing windows resizing windows and display routines that provide interior and exterior clipping and other graphics functions Typically windows are displayed with sliders buttons and menu icons for selecting various window options Some general systems such as X Windows and NeWS are capable of supporting multiple window managers so that different window styles can be accommo dated each with its own window manager The window managers can then be designed for particular applications In other cases a window system is designed for one specific application and window style  Icons representing objects such as furniture items and circuit elements are often referred to as application icons The icons representing actions such as ro tate magnify scale clip and paste are called control icons or command icons  Accommodating Multiple Skill Levels Usually interactive graphical interfaces provide several methods for selecting ac tions For example options could be selected by pointing at an icon and clicking different mouse buttons or by accessing pull down or pop up menus or by typ ing keyboard commands This allows a package to accommodate users that have different skill levels  For a less experienced user an interface with a few easily understood oper ations and detailed prompting is more effective than one with a large compre   c   Examples of screen layouts using window systems and icons  Courtesy of a Intergraph Corporation  b Visual Numerics Inc  and c Sun Microsystems  NM Caples Graphical ser lilertaces aac Interac t  e faput Methods hensive operation set A simplifies setot menus and options is easy to learn and remember and the user can concentrate on the application uistead of on the de tails of the intertace Sim ple point and click operations arv often easiest for an in experienced user of an applications package Therefore mterfaces typically pro vide a means for maskirg the complexity of a package so that beginners can use the system without being overwhelmed wrth too much deta  Experienced users on the other hand typically want speed This means fewer prompts and more input trom the kevboard or sith multiple mouse but ton clicks Actions are selected with function keys or with simultaneous combina tions of keyboard keys  ince experienced users will remember these shortcuts for commonly used actions Similarly help tacuities can be designed on several levels so that beginners can carry on a detailed dialogue while more experienced neers can reduce or eliminate prompts and messages Help facilities can also include one or more tu torial applications which provide users with an introduction to the capabilities and use of the svstem  Corns stencs An important design consideration in an interface is consistenev For example a particular icon shape should always have a single mear ing rather than serving to represent different actions or objects depending on the context Some other ex amples of consistency are always placing menus in the same relative positions so that a user does not have to hunt for a particular option always using a particu lar combination of keyboard keys for the same action and always color coding sa that the same color docs not have different meanings in vifferent situations Generally a complicated inconsistent model is difficult for a user to under stand and to work with in an effective way The objects and operations provided should be designed to farm a minimum and consistent set so that the svstent is easy to learn but not os ersimplified to the point where it is ditficult to apply  Minimizing Memarngaitien Operations in an interface should also be structured so hat thev are easy to un derstand and to remem er Obscure complicated incorsistent and abbreviated command formats lead confusion and reduction in the effeetiveness ol the use of the package One key or button used for ail delete operations for example is easier to remember than a number of different keys for different tvpes of delete Operations  Icons and window systems also aid in minimizing memorization Different kinds of information can be separated into different windows so that we do nat have to rely on memorization when different information displays overlap We can simply retain the multiple information on the screen in different windows and switch back and forth between window areas  cons are used to reduce mem orizing by displaying casily recognizable shapes for various objects and actions To select a particular action we simply select the icon that resembles that action  Bas kup and Brot andi oe A mechanism tor backing up or aborting during a sequence of operations is an other common teature cl an interface Often an operation can be canceled before execution is completed with the system restored to the state it was in before the operation was started With the ability to back up at any point we can confi dently explore the capabilities of the system knowing that the effects of a mis take can be erased  Backup can be provided in many forms A standard undo key or command is used to cancel a single operation Sometimes a system can be backed up through several operations allowing us to reset the system to some specified point In a system with extensive backup capabilities all inputs could be saved so that we can back up and replay any part of a session  Sometimes operations cannot be undone Once we have deleted the trash in the desktop wastebasket for instance we cannot recover the deleted files In this case the interface would ask us to verify the delete operation before proceeding  Good diagnostics and error messages are designed to help determine the cause of an error Additionally interfaces attempt to minimize error possibilities by anticipating certain actions that could lead to an error Examples of this are not allowing us to transform an object position or to delete an object when no ob ject has been selected not allowing us to select a line attribute if the selected ob ject is not a line and not allowing us to select the paste operation if nothing is in the clipboard  Feedback Interfaces are designed to carry on a continual interactive dialogue so that we are informed uf actions in progress at each step This is particularly important when the response time is high Without feedback we might begin to wonder what the system is doing and whether the input should be given again  As each input is received the system normally provides some type of re sponse An object is highlighted an icon appears or a message is displayed This not only informs us that the input has been received but it also tells us what the system is doing If processing cannot be completed within a few seconds several feedback messages might be displayed to keep us informed of the progress of the system In some cases this could be a flashing message indicating that the system is still working on the input request It may also be possible for the system to dis play partial results as they are completed so that the final display is built up a piece at a time The system might also allow us to input other commands or data while one instruction is being processed  Feedback messages are normally given clearly enough so that they have lit tle chance of being overlooked but not so overpowering that our concentration is interrupted With function keys feedback can be given as an audible click or by lighting up the key that has been pressed Audio feedback has the advantage that it does not use up screen space and we do not need to take attention from the work area to receive the message When messages are displayed on the screen a fixed message area can be used so that we always know where to look for mes sages In some cases it may be advantageous to place feedback messages in the work area near the cursor Feedback can also be displayed in different colors to distinguish it from other displayed objects  To speed system response feedback techniques can be chosen to take ad vantage of the operating characteristics of the type of devices in use A typical raster feedback technique is to invert pixel intensities particularly when making menu selections Other feedback methods include highlighting blinking and color changes  Section The User Dialogue No Chapter  Graphical User Interfaces and Interactive Input Methods Special symbols are designed for different types of feedback For example a cross a frowning face or a thumbs down symbol is often used to indicate an error and a blinking at work sign is used to indicate that processing is in progress This type of feedback can be very effect ve with a more experienced user but the beginner may need more detailed feedback that not only clearly in dicates what the system is doing but also what the user should input next  With some types of input echo feedback is desirable Typed characters can be displayed on the screen as they are input so that we can detect and correct er rors immediately Button and dial input can be echoed in the same way Scalar values that are selected with dials or from displayed scales are usually echoed on the screen to let us check input values for accuracy Selection of coordinate points can be echoed with a cursor or other symbol that appears at the selected position For more precise echoing of selected positions the coordinate values can be dis played on the screen  Graphics programs use several kinds of input data Picture specifications need values for coordinate positions values for the character string parameters scalar values for the transformation parameters values specifying menu options and values for identification of picture parts Any of the input devices discussed in Chapter can be used to input the various graphical data types but some de vices are better suited fur certain data types than others To make graphics pack ages independent of the particular hardware devices used input functions can be structured according to the data description to be handjed by each function This approach provides a logical input device classification in terms of the kind of data to be input by the device  Logical Classitication of Input Dey ices The various kinds of input data are summarized in the following six logical de vice classifications used by PH GS and GKS  LOCATOR a device for specifying a coordinate position x y STROKE a device for specifying a series of coordinate positions STRING  a device for specifying text input VALUATOR a device for specifying scalar value  CHOICE a device for selecting menu options PICK a device ter selecting picture components In some packages a single logical device is used tur both locator and stroke operations Some other mechanism such as a switch can then be used to indicate whether one coordinate position or a stream of positions is to be input  Each of the six logical input device classifications van be implemented with any of the hardware devices but some hardware devices are more convenient for certain kinds of data than others A device that can be pointed at a screen posi tion is more convenient for entering coordinate data than a kevboard for exam ple In the following sections we discuss how the vanous physical devices are used to provide input ithin each of the logical classifications Locator Devices A standard method for interactive selection of a coordinate point is by position ing the screen cursor We can do this with a mouse joystick trackball spaceball thumbwheels dials a digitizer stylus or hand cursor or some other cursor posi tioning device When the screen cursor is at the desired location a button is acti vated to store the coordinates of that screen point  Keyboards can be used for locator input in several ways A general purpose keyboard usually has four cursor control keys that move the screen cursor up down left and right With an additional four keys we can move the cursor diag onally as well Rapid cursor movement is accomplished by holding down the se lected cursor key Alternatively a joystick joydisk trackball or thumbwheels can be mounted on the keyboard for relative cursor movement As a last resort we could actually type in coordinate values but this is a slower process that also re quires us to know exact coordinate values  Light pens have also been used to input coordinate positions but some spe cial implementation considerations are necessary Since light pens operate by de tecting light emitted from the screen phosphors some nonzero intensity level must be present at the coordinate position to be selected With a raster system we can paint a color background onto the screen As long as no black areas are present a light pen can be used to select any screen position When it is not pos sible to eliminate all black areas in a display such as on a vector system for ex ample  a light pen can be used as a locator by creating a small light pattern for the pen to detect The pattern is moved around the screen until it finds the light pen  Stroke Devices This class of logical devices is used to mput a sequence of coordinate positions Stroke device input is equivalent to multiple calls to a locator device The set of input points is often used to display line sections  Many of the physical devices used for generating locator input can be used as stroke devices Continuous movement of a mouse trackball joystick or tablet hand cursor is translated into a series of input coordinate values The graphics tablet is one of the more common stroke devices Button activation can be used to place the tablet into continuous mode As the cursor is moved across the tablet surface a stream of coordinate values is generated This process is used in pain  brush systems that allow artists to draw scenes on the screen and in engineering systems where layouts can be traced and digitized for storage  String Devices The primary physical device used for string input is the keyboard Input charac ter strings are typically used for picture or graph labels  Other physical devices can be used for generating character patterns in a text writing mode For this input individual characters are drawn on the screen with a stroke or locator type device A pattern recognition program then interprets the characters using a stored dicticnary of predefined patterns  Valuator Devices This logical class of devices is employed in graphics systems to input scalar val ues Valuators are used for setting various graphics parameters such as rotation Section B Input of Graphical Data Chapter Graphical User Interfaces and  interactive Input Methods angle and scale factors and for setting physical parameters associated with a par ticular application temperature settings voltage levels stress factors etc   A typical physical device used to provide valuator input is a set of control dials Floating point numbers within any predefined range are input by rotating the dials Dial rotations in one direction increase the numeric input value and opposite rotations decrease the numeric value Rotary potentiometers convert dial rotation into a corresponding voltage This voltage is then translated into a real ridinber within a defined scalar range such as to Instead of dials slide potentiometers are sometimes used to convert linear movements into scalar values  Any keyboard with a set of numeric keys can be used as a valuator device A user siriply types the numbers directly in floating point format although this is a slower iriethod than using dials or slide potentiometers  Joysticks trackballs tablets and other interactive devices can be adapted for valuator input by interpreting pressure or movement of the device relative to a scalar range For one direction of movement say left to right increasing scalar values can be input Movement in the opposite direction decreases the scalar input value  Another technique for providing valuator input is to display sliders but tons rotating scales and menus on the video monitor  illustrates some possibilities for scale representations Locator input from a mouse joystick spaceball or other device is used to select a coordinate position on the display and the screen coordinate position is then converted to a numeric input value As a feedback mechanism for the yser the selected position on a scale can be marked with some symbol Numeric values may also be echoed somewhere on the screen to confirm the selections      Scales displayed on a video monitor for interactive selection of parameter values In this display sliders are provided for selecting scalar values for superellipse parameters s1 and s2 and for individual R G and B color values In addition a small circle can be positioned on the color wheel for selection of a combined RGB color and buttons can be activated to make small changes in color values Choice Devices Graphics packages use menus to select programming options parameter values and object shapes to be used in constructing a picture   A choice device is defined as one that enters a selection from a list menu of alternatives Com monly used choice devices are a set of buttons a cursor positioning device such as a mouse trackball or keyboard cursor keys and a touch panel  A function keyboard or button box  designed as a stand alone unit is often used to enter menu selections Usually each button is programmable so that its function can be altered to suit different applications Single purpose but tons have fixed predefined functions Programmable function keys and fixed function buttons are often included with other standard keys on a keyboard  For screen selection of listed menu options we can use cursor control de vices When a coordinate position x y is selected it is compared to the coordi nate extents of each listed menu item A menu item with vertical and horizontal boundaries at the coordinate values Xpin Xmax Yminv ANd Ymax is selected if the input coordinates x y satisfy the inequalities Xmin  X S Xmaxs Ymin    Ymax  For larger menus with a few options displayed at a time a touch panel is commonly used As with a cursor control device such as a mouse a selected screen position is compared to the area occupied by each menu choice  Alternate methods for choice input include keyboard and voice entry A standard keyboard can be used to type in commands or menu options For this method of choice input some abbreviated format is useful Menu listings can be numbered or given short identifying names Similar codings can be used with voice input systems Voice input is particularly useful when the number of op tions is small or less  Pick Devices Graphical object selection is the function of this logical class of devices Pick de vices are used to select parts of a scene that are to be transformed or edited in some way  Typical devices used for object selection are the same as those for menu se lection the cursor positioning devices With a mouse or joystick we can position the cursor over the primitives in a displayed structure and press the selection button The position of the cursor is then recorded and several levels of search may be necessary to locate the particular object if any that is to be selected First the cursor position is compared to the coordinate extents of the various structures in the scene If the bounding rectangle of a structure contains the cur sor coordinates the picked structure has been identified But if two or more structure areas contain the cursor coordinates further checks are necessary The coordinate extents of individual lines in each structure can be checked next If the cursor coordinates are determined to be inside the coordinate extents of only one line for example we have identified the picked object Otherwise we need addi tional checks to determine the closest line to the cursor position  One way to find the closest line to the cursor position is to calculate the dis tance squared from the cursor coordinates x y to each line segment whose bounding rectangle contains the cursor position   For a line with end points x  y  and x  y2  distance squared from x y to the line is calculated as Section Input of Graphical Data   Distances to line segments from the pick position  _ Axy y  Aye P Ax  Ay  a where Ax x x  and Ay y y  Various approximations can be used to speed up this distance calculation or other identification schemes can be used  Another method for finding the closest line to the cursor position is to spec ify the size of a pick window The cursor coordinates are centered on this win dow and the candidate lines are dipped to the window as shown in  By making the pick window small enough we can ensure that a single line will cross the window The method for selecting the size of a pick window is de scribed in Section where we consider the parameters associated with various input functions  A methed for avoiding the calculation of pick distances or window clipping intersections is to highlight the candidate structures and allow the user to resolve the pick ambiguity One way to do this is to highlight the structures that overlap the cursor position one by one The user then signals when the desired structure is highlighted  An alternative to cursor positioning is to use button input to highlight suc cessive structures A second button is used to stop the process when the desired structure is highlighted If very many structures are to be searched in this way the process can be speeded up and an additional button is used to help identify the structure The first button can initiate a rapid successive highlighting of struc tures A second button can again be used to stop the process and a third button can be used to back up more slowly if the desired structure passed before the op erator pressed the stop button  Finally we could use a keyboard tu type in structure names This is a straightforward but less interactive pick selection method Descriptive names can be used to help the user in the pick process but the method has several drawbacks It is generally slower than interactive picking on the screen and a user will probably need prompts to remember the various structure names In addition picking structure subparts from the keyboard can be more difficult than picking the subparts on the screen  w e   p   A pick window centered on pick Tyra  coordinates x  y  used to resolve w _ i i pick object overlaps  Graphical input functions  be set up to allow users to specify the following options   Which physical devices are to provide input within a particular logical clas sification for example a tablet used as a stroke device   How the graphics program and devices are to interact input mode  Either the program or the devices can initiate dat  entry or both can operate si multaneously   When the data are to be input and which device is to be used at that time to deliver a particular input type to the specified data variables  Input Modes Functions to provide input can be structured to operate in various input modes which specify how the program and input devices interact Input could be initi ated by the program or the program and input devices both could be operating simultaneously or data input could be initiated by the devices These three input modes are referred to as request mode sample mode and event mode  In request mode the application program initiates data entry Input values are requested and processing is suspended until the required values are received This input mode corresponds to typical input operation in a general program ming language The program and the input devices operate alternately Devices are put into a wait state until an input request is made then the program waits until the data are delivered  In sample mode the application program and input devices operate inde pendently Input devices may be operating at the same time that the program is processing other data New input values from the input devices are stored re placing previously input data values When the program requires new data it samples the current values from the input devices  In event mode the input devices initiate data input to the application pro gram The program and the input devices again operate concurrently but now the input devices deliver data to an input queue All input data are saved When the program requires new data it goes to the data queue  Any number of devices can be operating at the same time in sample and event modes Some can be operating in sample mode while others are operating in event mode But only one device at a time can be providing input in request mode  An input mode within a logical class for a particular physical device operat ing on a specified workstation is declared with one of six input class functions of the form set   Moce ws deviceCode inputMode echoFlaq  where deviceCode is a positive integer inputMode is assigned one of the val ues request sampl2 or event and parameter echoFlag is assigned either the value echo or the value noecho How input data will be echoed on the display de vice is deterrained by parameters set in othex input functions to be described later in this section  Section Input Functions  Chapter  TABLE Graphical User Interfaces and ASSIGNMENT OF INPUT DEVICE Interactive Input Methods CODES  Device Code  Physical Device Type Keyboard Graphics Tablet Mouse lovstick Trackball Button Tu e wn Device code assignment is installation dependent One possible assignment of device codes is shown in Table Using the assignments in this table we could make the following declarations  setLocatorMode  sample noecho setTextMode  request echo setPickMode  event echo  Thus the graphics tablet is declared to be a locator device in sample mode on workstation with no input data feedback echo the keyboard is a text device in request mode on workstation with input echo and the mouse is declared to be a pick device in event mode on workstation with input echo  Request Mode Input commands used in this mode correspond to standard input functions in a high level programming language When we ask for an input in request mode other processing is suspended until the input is received After a device has been assigned to request mode as discussed in the preceding section input requests can be made to that device using one of the six logical class functions represented by the following  request   ws deviceCode status     Values input with this function are the workstation code and the device code Re turned values are assigned to parameter status and to the data parameters cor responding to the requested logical class  A value of ok or none is returned in parameter status according to the va lidity of the input data A value of none indicates that the input device was acti vated so as to produce invalid data For locator input this could mean that the coordinates were out of range For pick input the device could have been acti vated while not pointing at a structure Or a break button on the input device could have been pressed A returned value of none can be used as an end of data signal to terminate a programming sequence  Locator and Stroke Input in Request Mode The request functions for these two logical input classes are requestLocator ws devCode status viewIndgex ot regquestStroke ws devCcce nMax status viewlIndex m prs For locator input pt is the world coordinate position selected For stroke input pts is a list of n coordinate positions where parameter nMax gives the maxi mum number of points that can go in the input list Parameter viewIndex is as signed the two dimensional view index number  Determination of a world coordinate position is a two step process  The physical device selects a point in device coordinates usually from the video dis play screen and the inverse of the workstation transformation is performed to obtain the corresponding point in normalized device coordinates  Then the inverse of the window to viewport mapping is carried out to get to viewing co ordinates then to world coordinates  Since two cr more views may overlap on a device the correct viewing transformation is identified according to the view transformation input priority number By default this is the same as the view index number and the lower the number the higher the priority View index has the highest priority We can change the view priority relative to another reference viewing transformation with setViewTransformationInputPriority ws viewIndex refViewIndex prior ty  where viewIndex identifies the viewing transformation whose priority is to be changed refViewIndex identifies the reference viewing transformation and parameter priority is assigned either the value lower or the value higher For example we can alter the priority of the first four viewing transformations on workstation as shown in  with the sequence of functions  setViewTransformationInputPriority  higher set ViewTransformationInputPriority   lower  String Input in Request Mode Here the request input function is reguestString ws devCode status nChars str  Parameter str in this function is assigned an input string The number of charac ters in the string is given in parameter nChars      Original Final Priority Ordering Priority Ordering   Rearranging viewing priorities  Section input Functions Chapter Graphical User Interfaces and  Inleractive Input Methods Valuator Input in Request Mode A numerical value is input in request mode with requestValuator ws devCode status value  Parameter value cai be assigned any real number value  Choice Input in Request Mode We make a menu selection with the following request function requestChoice ws devCode status itemNum  Parameter itemNum is assigned a positive integer value corresponding to the menu item selected  Pick Input in Request Mode For this mode we obtain a structure identifier number with the function requestPick ws devCode maxPathDepth status pathDepth pickPath  Parameter pickPath is a list of information identifying the primitive selected This list contains the structure name pick identifier for the primitive and the ele ment sequence number Parameter pickDepth is the number of levels returned in pickPath and maxPathDepth is the specified maximum path depth that can be included in pickPath  Subparts of a structure can be labeled for pick input with the following function  setPickIdentifier pickID  An example of sublabeling during structure creation is given in the following programming sequence  openStructure 1d  for k  k  n k   setPickIdentifier k    closeStructure  Picking of structures and subparts of structures is also controlled by some work station filters Section Objects cannot be picked if they are invisible Also we can set the ability to pick objects independently of their visibility This is accom plished with the pick filter  setPickFilter ws devCode pickables nonp ckabes where the set pickabies contains the names of objects structures and primi tives that we may want to select with the specified pick device Similarly the set nonpickables contains the names of objects that we do not want to be avail able for picking with this input device  Sample Mode Once sample mode has been set for one or more physical devices data input be zins without waiting for program direction If a joystick has been designated as a ocator device in sample mode coordinate values for the current position of the activated joystick are immediately stored As the activated stick position changes the stored values are continually replaced with the coordinates of the current stick position  Sampling of the current values from a physical device in this mode begins when a sample command is encountered in the application program A locator device is sampled with one of the six logical class functions represented by the following  sample   ws deviceCode a  Some device classes have a status parameter in sample mode and some do not Other input parameters are the same as in request mode  As an example of sample input suppose we want to translate and rotate a selected object A final translation position for the object can be obtained with a locator and the rotation angle can be supplied by a valuator device as demon strated in the following statements  sampleLocator wsl devl viewindex pr sampleValuator ws2 dev2 angle  Event Mode When an input device is placed in event mode the program and device operate simultaneously Data input from the device is accumulated in an event queue or input queue All input devices active in event mode can enter data referred to as events  into this single event queue with each device entering data values as they are generated At any one time the event queue can contain a mixture of data types in the order they were input Data entered into the queue are identi tied according to logical class workstation number and physical device code  An application program can be directed to check the event queue for any input with the function awaitEvent time ws deviceClass dev ceCode  Parameter time is used to set a maximum waiting time for the application pro gram lf the queue happens to be empty processing is suspended until either the number of seconds specified in time has elapsed or an input arrives Should the waiting time run out before data values are input the parameter aeviceClass is assigned the value none When time is given the value the program checks the queue and immediately returns to other processing if the queue is empty  Section Input Functions Chapter Graphical User Interfaces and  Interactive Input Methods If processing is directed to the event queue with the awaitEvent function and the queue is not empty the first event in the queue is transferred to a current event record The particular logical device class such as locator or stroke that made this input is stored in parameter deviceClass Codes identifying the particular workstation and physical device that made the input are stored in pa rameters ws and deviceCode respectively  To retrieve a data input from the current event record an event mode input function is used The functions in event mode are similar to those in request and sample modes However no workstation and device code parameters are neces sary in the commands since the values for these parameters are stored in the data record A user retrieves data with get       For example to ask for locator input we invoke the function getLocator viewIndex pt  In the following program section we give an example of the use of the awaitEvent and get functions A set of points from a tablet device code on workstation is input to plot a series of straight line segments connecting the input coordinates  setStrokeMode  event noecho   do  awaitEvent  ws deviceClass deviceCode  while deviceClass  stroke  getStroke nMax viewIndex n pts  polyline n pts  The repeat until loop bypasses any data from other devices that might be in the queue If the tablet is the only active input device in event mode this loop is not necessary  A number of devices can be used at the same time in event mode for rapid interactive processing of displays The following statements plot input lines from a tablet with attributes specified by a button box  setPolylineIndex    set tablet to stroke device event mode  setStrokeMode  evenz noecho     set buttons to choice device event mode  setChoiceMode  event noecho  do  awaitEvent  ws deviceClass deviceCode  if deviceClass  choice   getChoice status option  setPolylineIndex option   i else if deviceClass  stroke  getStroke nMax viewIndex n pts  polyline n pts    while deviceClass  none  Some additional housekeeping functions can be used in event mode Func tions for clearing the event queue are useful when a process is terminated and a new application is to begin These functions can be set to clear the entire queue or to clear only data associated with specified input devices and workstations  Concurrent Use of Input Modes An example of the simultaneous use of input devices in different modes is given in the following procedure An object is dragged around the screen with a mouse When a final position has been selected a button is pressed to terminate any further movement of the sbject The mouse positions are obtained in sample mode and the button input is sent to the event queue   drags object in response to mouse input    terminate processing by button press  setLocatorMode  sample echo  setChoiceMode  event noecho  do  sampleLocator  viewIndex pt    trarslate object centroid to position pt and draw   awaitEvent  ws class code   while class  choice    Quite a number of parameters can be set for input devices using the nitial ize function for each logical class  initialize   ws deviceCode    ps coordExt dataRec  Parameter pe is the prompt and echo type parameter coordExt is assigned a set of four coordinate values and parameter dataRec is a record of various con trol parameters  For locator input some values that can be assigned to the prompt and echo parameter are pe  installation defined pe  crosshair cursor centered at current position pe  line from initial position to current position pe  rectangle defined by current and initial points Several other options are also available For structure picking we have the following options  pe  highlight picked primitives pe  highlight all primitives with value of pick id pe  highlight entire structure as well as several others  Section initial Valves for Input Device Parameters Chapter  Graphical User Interfaces and  Interactive Input Methods When an echo of the input data is requested it is displayed within the bounding rectangle specified by the four coordinates in parameter coordExt Additional options can also be set in parameter dataRec For example we can set any of the following   size of the pick window  minimum pick distance  type and size of cursor display  type of structure highlighting during pick op rations  range min and max for valuator input  resolution scale for valuator input plus a number of other options   There are several techniques that are incorporated into graphics packages to aid the interactive construction of pictures Various input options can be provided so that coordinate information entered with locator and stroke devices can be ad justed or interpreted according to a selected option For example we can restrict all lines to be either horizontal or vertical Input coordinates can establish the po sition or boundaries for objects to be drawn or they can be used to rearrange pre viously displayed objects  Basic Positioning Methods Coordinate values supplied by locator input are often used with positioning methods to specify a location for displaying an object or a character string We in teractively select coordinate positions with a pointing device usually by posi tioning the screen cursor Just how the object or text string positioning is pe formed depends on the selected options With a text string for example the scTeen point could be taken as the center string position or the start or end posi tion of the string or any of the other string positioning options discussed in Chapter For tines straight line segments can be displayed between two se lected screen positions  As an aid in positioning objects numeric values for selected positions can be echoed on the screen Using the echoed coordinate values as a guide we can make adjustments in the selected location to obtain accurate positioning  Constraints With some applications certain types of prescribed orientations or object align ments are useful A constraint is a rule for altering input coordinate values to produce a specified orientation or alignment of the displayed coordinates There are many kinds of constraint functions that can be specified but the most com mon constraint is a horizontal or vertical alignment of straight lines This type of constraint shown in Figs and is useful in forming network layouts With this constraint we can create horizontal and vertical lines without worrying about precise specification of endpoint coordinates Section interactive Picture Construction Techniques  3E Select First Select Endpoint Position Second Endpoint Position Along Approximate Horizontal Path   Horizontal line constraint   I _ Select First Select Endpoint Position Second Endpoint Position Along Approximate Vartical Path   Vertical line constraint  A horizontal or vertical constraint is implemented by determining whether any two input coordinate endpoints are more nearly horizontal or more nearly vertical If the difference in the y values of the two endpoints is smaller than the difference in x values a horizontal line is displayed Otherwise a vertical line is drawn Other kinds of constraints can be applied to input coordinates to produce a variety of alignments Lines could be constrained to have a particular slant such as  and input coordinates could be constrained to lie along predefined paths such as circular arcs  Grids Another kind of constraint is a grid of rectangular lines displayed in some part of the screen area When a grid is used any input coordinate position is rounded to the nearest intersecton of two grid lines  illustrates line drawing with a grid Each of the two cursor positions is shifted to the nearest grid intersection point and the line is drawn between these grid points Grids facilitate object con structions because a new line can be joined easily to a previously drawn line by selecting any position near the endpoint grid intersection of one end of the dis played line   Seiect First Endpoint Position Near a Grid Intersection  Select a Position Near a Second Grid Intersection   Line drawing using a grid Chapter Graphical User Interfaces and Interactive Input Methods   Gravity field around a line Any selected point in the shaded area is shifted to a position on the line  Spacing between grid lines is often an option that can be set by the user Similarly grids can be turned on and off and it is sometimes possible to use par tial grids and grids of different sizes in different screen areas  Gravity Field In the construction of figures we sometimes need to connect lines at positions be tween endpoints Since exact positioning of the screen cursor at the connecting point can be difficult graphics packages can be designed to convert any input position near a line to a position on the line  This conversion of input position is accomplished by creating a gravity field area around the line Any selected position within the gravity field of a line is moved  gravitated  to the nearest position on the line A gravity field area around a line is illustrated with the shaded boundary shown in  Areas around the endpoints are enlarged to make it easier for us to connect lines at their endpoints Selected positions in one of the circular areas of the gravity field are attracted to the endpoint in that area The size of gravity fields is chosen large enough to aid positioning but small enough to reduce chances of overlap with other lines If many lines are displayed gravity areas can overlap and it may be difficult to specify points correctly Normally the boundary for the gravity field is not displayed  Rubber Band Methads Straight lines can be constructed and positioned using rubber band methods which stretch out a line from a starting position as the screen cursor is moved  demonstrates the rubber band method We first select a screen posi tion for one endpoint of the line Then as the cursor moves around the line is displayed from the start position to the current position of the cursor When we finally select a second screen position the other line endpoint is set  Rubber band methods are used to construct and position other objects be sides straight lines  demonstrates rubber band construction of a rec tangle and  shows a rubber band circle construction   ao  ee   f i Qo  SN   ee Ne Select As the Cursor Line Follows First Moves A Line Cursor Position Line Stretches out until the Second Endpoint from the Initial Endpoint Is Point Selected  Rubber band method for drawing and positioning a straight line segment    Select Rectangle Select Final Pasition Stretches Out Position for for One Corner As Cursor Moves Opposite Corner of the Rectangle of the Ractangle  Rubber band method for constructing a rectangle  Dragging A technique that is often used in interactive picture construction is to move ob jects into position by dragging them with the screen cursor We first select an ob ject then move the cursor in the direction we want the object to move and the se lected object follows the cursor path Dragging objects to various positions in a scene is useful in applications where we might want to explore different possibil ities before selecting a final location  Painting and Drawing Options for sketching drawing and painting come in a variety of forms Straight lines polygons and circles can be generated with methods discussed in the pre vious sections Curve drawing options can be provided using standard curve shapes such as circular arcs and splines or with freehand sketching procedures Splines are interactively constructed by specifying a set of discrete screen points that give the general shape of the curve Then the system fits the set of points with a polynomial curve In freehand drawing curves are generated by follow ing the path of a stylus on a graphics tablet or the path of the screen cursor on a video monitor Once a curve is displayed the designer can alter the curve shape by adjusting the positions of selected points along the curve path  JOO  Select Position Circle Stretches Select the for the Circle Out as the Final Radius Center Cursor Moves of the Circle  Constructing a circle using a rubber band method  Chapter  Graphical User Interfaces and Interactive Input Ss os ET il cal nl   A screen layout showing one type of interface to an artist s painting package  Courtesy of Thomson Digital Image  Line widths line styles and other attribute options are also commonly found in painting and drawing packages These options are implemented with the methods discussed in Chapter Various brush styles brush patterns color combinations object shapes and surface texture patterns are also available on many systems particularly those designed as artist s workstations Some paint systems vary the line width and brush strokes according to the pressure of the artist s hand on the stylus  shows a window and menu system used with a painting package that allows an artist to select variations of a specified ob ject shape different surface textures and a variety of lighting conditions for a scene  A typical virtual reality environment is illustrated in  Interactive input is accomplished in this environment with a data glove Section  which is ca pable of grasping and moving objects displayed in a virtual scene The computer generated scene is displayed through a head mounted viewing system Section as a stereoscopic projection Tracking devices compute the pasition and ori entation of the headset and data glove relative to the object positions in the scene With this system a user can move through the scene and rearrange object posi tions with the data glove  Another method for generating virtual scenes is to display stereoscopic pro jections on a raster monitor with the two stereoscopic views displayed on alter nate refresh cycles The scene is then viewed through stereoscopic glasses Inter active object manipulations can again be accomplished with a data glove and a tracking device to monitor the glove position and orientation relative to the posi tion of objects in the scene    Using a head tracking stereo display called the BOOM Fake Space Labs Inc  and a Dataglove VPL Inc  a researcher interactively manipulates exploratory probes in the unsteady flow around a Harrier jet airplane Software developed by Steve Bryson data from Harrier  Courtesy of Sam Uselton NASA Ames Research Center   SUMMARY A dialogue for an applications package can be designed from the user s model which describes the furictions of the applications package All elements of the di alogue are presented in the language of the applications Examples are electrical and architectural design packages  Graphical interfaces are typically designed using windows and icons A window system provides a window manager interface with menus and icons that allows users to open close reposition and resize windows The window system then contains routines to carry out these operations as well as the various graphics operations General window systems are designed to support multiple window managers Icons are graphical symbols that are designed for quick iden tification of application processes or control processes  Considerations in user dialogue design are ease of use clarity and flexibil ity Specifically graphical interfaces are designed to maintain consistency in user interaction and to provide for different user skill levels In addition interfaces are designed to minimize user memorization to provide sufficient feedback and to provide adequate backup and error handling capabilities  Input to graphics programs can come from many different hardware de vices with more than one device providing the same general class of input data Graphics input functions can be designed to be independent of the particular input hardware in use by adopting a logical classification for input devices That is devices are classified according to the type of graphics input rather than a Summary  Chapter Graphical User interfaces and Interactive Input Methods hardware designation such as mouse or tablet The six logical devices in com mon use are locator stroke string valuator choice and pick Locator devices are any devices used by a program to inpul a single coordinate position Stroke de vices input a stream of coordinates String devices are used to input text Valuator devices are any input devices used to enter a scalar value Choice devices enter menu selections And pick devices input a structure name  Input functions available in a graphics package can be defined in three input modes Request mode places input under the control of the application program Sample mode allows the input devices and program to operate concur rently Event mode allows input devices to initiate data entry and control pro cessing of data Once a mode has been chosen for a logical device class and the particular physical device to be used to enter this class of data input functions in the program are used to enter data values into the program An application pro gram can make simultaneous use of several physical input devices operating in different modes  Interaclive picture construction methods are commonly used in a variety of applications including design and painting packages These methods provide users with the capability to position objects to constrain figures to predefined orientations or alignments to sketch figures and to drag objects around the screen Grids gravity fields and rubber band methods are used to aid in posi tioning and other picture construction operations  REFERENCES Guidelines for user interface design are presented in Apple  Bleser  Digital  and OSF MOTIF  For information on the X Window Svstem see Young  and Cutler Gilly and Reilly  Additional discussions of interface design can be found in Phillips  Goodman and Spence  Locdding  Swezey and Davis  Carroll and  arrithers  Foley Wallace and Chan  and Good et al   The evolution of the concept of logical or virtual input devices is discussed in Wallace  and in Rosenthal et al   An early discussion of input device classifications is to be found in Newman   Input operations in PHIGS can be found in Hopgood and buce  Howard et al   Gaskins  and Blake  For iniormation on GKS input functions see Hopgood et al  ana Enderle  ansy and Pratt   EXERCISES  Select some graphics application witn which you are familiar and set up a user model that will serve as the basis for the design of a user interface tor yraphics applications in that area  List possible help facilines that can be provided in a user interface and discuss which types of help would be appropriate for different levels ef users  Summarize the possible ways of handling backup and errors State which approaches are more Suitable foy the beginner and which are better suited to the experienced user  List the possible iormats for presenting menus to a user ard explain under whal cir  cumstances each might be appropriate   Discuss alternatives or feedback in terms of the various Jevels of users  List the functions that must be performed by a window manager in handling screen layouts with multiple ovedapping windows  10  11  12   21  22  23  24 25 26 27  Set up a design ior a window manager package  Mesign a user interface for a painting program  Design a user interface for a two level hierarchical modeling package  For any area with which you are familiar design ac umplete user interface to a graph ics package providing capabilities to any users in that area  Develop a program that allows objects to be positioned on the screen using a locator device An object menu of geometric shapes is be presented to a user who is to se lect an object and a placement position The program should allow any number of ob jects to be positioned until a terminate signal ts given Extend the program of the previous exercise so that selected objects can be scaled and rotated before positioning The transformation chcices and transformation parameters are to be presented to the user as menu options  Write a program that allows a user to interactively sketch pictures using a stroke de vice   Discuss the methods that could be employed in a pattern recognition procedure to match input characters against a stored library of shapes  Write a routine that displays a linear scale and a slider on the screen and allows nu  meric values to be selected by positioning the slider along the scale line The number value selected is to be echoed in a box displayed near the linear scale   Write a routine that displays a circular scale and pointer or a slider that can be moved around the circle to select angles in degrees  The angular value selected is ta be echoed in a box displayed near the circular scale  Write a drawing program that allows users to create a picture as a set of line segments drawn between specified endpoints The coordinates of the individual line segments are to be selected with a locator device  Write a drawing package that allows pictures to be created with straight line segments drawn between specified endpoints Set up a gravity field around each line in a pic lure as an aid in connecting new lines to existing lines   Modify the drawing package in the previous exercise that allows lines to be con  strained horizontally or vertically   Develop a drawing package that can display an optional grid pattern so that selected screen positions are rounded to grid intersections The package is to provide line drawing capabilities with line endpoinis selected with a locator device   Write a routine that allows a designer to create a picture by sketching straight lines with a rubber band method  Write a drawing package that allows straight lines rectangles and circles to be con structed with rubber band methods  Write a program that allows a user to design a picture from a menu of basic shapes by dragging each selected shape into position with a pick device  Design an implementation of the input functions for request mode Design an implementation of the sample mode input functions Design an implementation of the input functions for event mode  Set up a general implementation of the input functions for request sample and event modes  Fxercises CHAPTER  Three Dimensional Concepts    ade  W hen we model and display a three dimensional scene there are many more considerations we must take into account besides just including coordinate values for the third dimension Object boundaries can be constructed with various combinations of plane and curved surfaces and we sometimes need to specify information about object interiors Graphics packages often provide routines for displaying internal components or cross sectional views of solid ob jects Also some geometric transformations are more involved in three dimen sional space than in two dimensions For example we can rotate an object about an axis with any spatial orientation in three dimensional space Two dimensional rotations on the other hand are always around an axis that is perpendicular to the xy plane Viewing transformations in three dimensions are much more com plicated because we have many more parameters to select when specifying how a three dimensional scene is to be mapped to a display device The scene descrip tion must be processed through viewing coordinate transformations and projec tion routines that transform three diniensional viewing coordinates onto two di mensional device coordinates Visible parts of a scene for a selected view must be identified and surface rendering algorithms must be applied if a realistic ren dering of the scene is required   To obtain a display of a three dimensional scene that has been modeled in world coordinates we must first set up a coordinate reference for the camera  This co ordinate reference defines the position and orientation for the plane of the cam era film   which is the plane we want to use to display a view of the ob jects in the scene Object descriptions are then transferred to the camera reference coordinates and projected onto the selected display plane We can then display as aan Display TT Prone fogure dd Ps Coordinate reference for obta ning  particular view ofa _   three dimensional scene   the objects in wireframe outline form as in  or we can apply lighting and surface rendering techniques to shade the visible surfaces  Parallel Projection One method for generating a view of a solid object is to project points on the ob ject surface along parallel lines onto the display plane By selecting different viewing positions we can project visible points on the object onto the display plane to obtain different two dimensional views of the object as in  In a parallel projection parallel lines in the world coordinate scene project into parallel lines on the two dimensional display plane This technique is used in engineer ing and architectural drawings to represent an object with a set of views that maintain relative proportions of the object The appearance of the solid object can then be reconstructured from the major views    Wireframe display of three objects with back lines removed from a commercial database of object shapes Each object in the database is defined as a grid of coordinate points which can then be viewed in wireframe form or in a surface rendered form  Courtesy of Viewpoint  DataLabs  tc Top Side Frant  Three parallel projection views of an object showing relative proportions from different viewing positions Perspective Projection Another method for generating a view of a three dimensional scene is to project points to the display plane along converging paths This causes objects farther from the viewing position to be displayed smaller than objects of the same size that are nearer to the viewing position In a perspective projection parallel lines in a scene that are not parallel to the display plane are projected into converging lines Scenes displayed using perspective projections appear more realistic since this is the way that our eyes and a camera lens form images In the perspective projection view shown in  parallel lines appear to converge to a distant point in the background and distant objects appear smaller than objects closer to the viewing position  Depth Cueing With few exceptions depth information is important so that we can easily iden tify for a particular viewing direction which is the front and which is the back of displayed objects  illustrates the ambiguity that can result when a wireframe object is displayed without depth information There are several ways in which we can include depth information in the two dimensional representa tion of solid objects  A simple method for indicating depth with wireframe displays is to vary the intensity of objects according to their distance from the viewing position Fig ure shows a wireframe object displayed with depth cueing The lines closest to Section Three Dimensional Display Methods   A perspective projection view of an airport scene  Courtesy of Evans  Sutherland   a  b   The wireframe representation of the pyramid in a contains no depth information to indicate whether the viewing direction is b downward from a position above the apex or c upward from a position below the base    A wireframe object displayed with depth cueing so that the intensity of lines decreases from the front to the back of the object  the viewing position are displayed with the highest intensities and lines farther away are displayed with decreasing intensities Depth cueing is applied by choosing maximum and minimum intensity or color values and a range of dis tances over which the intensities are to vary  Another application of depth cueing is modeling the effect of the atmos phere on the perceived intensity of objects More distant objects appear dimmer to us than nearer objects due to light scattering by dust particles haze and smoke Some atmospheric effects can change the perceived color of an object and we can model these effects with depth cueing  Visible Line and Surface Identification We can also clarify depth relationships in a wireframe display by identifying visi ble lines in some way The simplest method is to highlight the visible lines or to display them in a different color Another technique commonly used for engi neering drawings is to display the nonvisible lines as dashed lines Another ap proach is to simply remove the nonvisible lines as in Figs b and c  But removing the hidden lines also removes information about the shape of the back surfaces of an object These visible line methods also identify the visible surfaces of objects  When objects are to be displayed with color or shaded surfaces we apply surface rendering procedures to the visible surfaces so that the hidden surfaces are obscured Some visible surface algorithms establish visibility pixel by pixel across the viewing plane other algorithms determine visibility for object surfaces as a whole  Surface Rendering Added realism is attained in displays by setting the surface intensity of objects according to the lighting conditions in the scene and according to assigned sur face characteristics Lighting specifications include the intensity and positions of light sources and the general background illumination required for a scene Sur face properties of objects include degree of transparency and how rough or smooth the surfaces are to be Procedures can then be applied to generate the cor rect itumination and shadow regions for the scene In  surface rendering methods are combined with perspective and visible surface identification to gen erate a degree of realism in a displayed scene  Exploded and Cutaway Views Many graphics packages allow objects to be defined as hierarchical structures so that internal details can be stored Exploded and cutaway views of such objects can then be used to show the internal structure and relationship of the object parts  shows several kinds of exploded displays for a mechanical de sign An alternative to exploding an object into its component parts is the cut away view   which removes part of the visible surfaces to show internal structure  Three Dimensional and Stereascepic Views Another method for adding a sense of realism to a computer generated scene is to display objects using either three dimensional or stereoscopic views As we have seen in Chapter three dimensional views can be obtained by reflecting a Section Three Dimensional Display Methods   _A realistic room display achieved with stochastic ray tracing methods that apply a perspective projection surface texture mapping and illumination models  Courtesy of John Snyder Jed Lengyel Devendra Kalra and A Barr California Institute of Technology Copyright  Caltech    A fully rendered and assembled turbine display a can also be viewed as b an exploded wireframe display  c a surface rendered exploded display or d a surface rendered color coded exploded display  Courtesy of Autodesk Inc  raster image from a vibrating flexible mirror The vibrations of the mirror are syn chronized with the display of the scene on the CRT As the mirror vibrates the focal length varies so that each point in the scene is projected to a position corre sponding to its depth  Stereoscopic devices present two views of a scene one for the left eye and the other for the right eye The two views are generated by selecting viewing po sitions that correspond to the two eye positions of a single viewer These two views then can be displayed on alternate refresh cycles of a raster monitor and viewed through glasses that alternately darken first one lens then the other in synchronization with the monitor refresh cycles  Chapter Three Dimensional Concepts    Color coded cutaway view of a lawn mower engine showing the structure and relationship of internal components  Courtesy of Autodesk Inc   Design of three dimensional packages requires some considerations that are not necessary with two dimensional packages A significant difference between the two packages is that a three dimensional package must include methods for mapping scene descriptions onto a flat viewing surface We need to consider im plementation procedures for selecting different views and for using different pro jection techniques We also need to consider how surfaces of solid objects are to be modeled how visible surfaces can be identified how transformations of ob jects are performed in space and how to describe the additional spatial proper ties introduced by three dimensions Later chapters explore each of these consid erations in detail  Other considerations for three dimensional packages are straightforward extensions from two dimensional methods World coordinate descriptions are extended to three dimensions and users are provided with output and input rou tines accessed with specifications such as polyline3 n wePoints  fillarea3 n wcPoints  text3 wePoint string  getLocator3 wcPoint translate3 translateVector matrixTranslate  where points and vectors are specified with three components and transforma tion matrices have four rows and four columns  Two dimensional attribute functions that are independent of geometric con siderations can be applied in both two dimensional and three dimensional appli cations No new attribute functions need be defined for colors line styles marker Modeling Coordinates World Coordinates Projection Coordinates   Pipeline for transforming a view of a world coordinate scene to device coordinates  attributes or text fonts Attribute procedures for orienting character strings how ever need to be extended to accommodate arbitrary spatial orientations Text at tribute routines associated with the up vector require expansion to include z co ordinate data so that strings can be given any spatial orientation Area filling routines such as those for positioning the pattern reference point and for map ping patterns onto a fill area need to be expanded to accommodate various ori entations of the fill area plane and the pattern plane Also most of the two di mensional structure operations discussed in earlier chapters can be carried over to a three dimensional package   shows the general stages in a three dimensional transformation pipeline for displaying a world coordinate scene After object definitions have been converted to viewing coordinates and projected to the display plane scan conversion algorithms are applied to store the raster image  Device Coordinates CHAPTER  Three Dimensional Object Representations      raphics scenes can contain many different kinds of objects trees flowers  clouds rocks water bricks wood paneling rubber paper marble steel glass plastic and cloth just to mention a few So itis probably not too surprising that there is no one method that we can use to describe objects that will include all characteristics of these different materials And to produce realistic displays of scenes we need to use representations that accurately model object characteris tics  Polygon and quadric surfaces provide precise descriptions for simple Eu clidean objects such as polyhedrons and ellipsoids spline surfaces and construc tion techniques are useful for designing aircraft wings gears and other engineer ing structures with curved surfaces procedural methods such as fractal constructions and particle systems allow us to give accurate representations for clouds clumps of grass and other natural objects physically based modeling methods using systems of interacting forces can be used to describe the nonrigid behavior of a piece of cloth or a glob of jello octree encodings are used to repre sent internal features of objects such as those obtained from medical CT images and isosurface displays volume renderings and other visualization techniques are applied to three dimensional discrete data sets to obtain visual representa tions of the data  Representation schemes for solid objects are often divided into two broad categories although not all representations fall neatly into one or the other of these two categories Boundary representations B reps describe a three dimen sional object as a set of surfaces that separate the object interior from the environ ment Typical examples of boundary representations are polygon facets and spline patches Space partitioning representations are used to describe interior properties by partitioning the spatial region containing an object into a set of small nonoverlapping contiguous solids usually cubes  A common space par titioning description for a three dimensional object is an octree representation In this chapter we consider the features of the various representation schemes and how they are used in applications   The most commonly used boundary representation for a three dimensional graphics object is a set of surface polygons that enclose the object interior Many graphics systems store all object descriptions as sets of surface polygons This simplifies and speeds up the surface rendering and display of objects since all surfaces are described with linear equations For this reason polygon descrip     Wireframe representation of a cylinder with back hidden lines removed  tions are often referred to as standard graphics objects  In some cases a polyg onal representation is the only one available but many packages allow objects to be described with other schemes such as spline surfaces that are then converted to polygonal representations for processing  A polygon representation for a polyhedron precisely defines the surface fea tures of the object But for other objects surfaces are tesselated or tiled to produce the polygon mesh approximation In  the surface of a cylinder is repre sented as a polygon mesh Such representations are common in design and solid modeling applications since the wireframe outline can be displayed quickly to give a general indication of the surface structure Realistic renderings are pro duced by interpolating shading patterns across the polygon surfaces to eliminate or reduce the presence of polygon edge boundaries And the polygon mesh ap proximation to a curved surface can be improved by dividing the surface into smaller polygon facets  Polygon Tables We specify a polygon surtace with a set of vertex coordinates and associated at tribute parameters As information for each polygon is input the data are placed into tables that are to be used in the subsequent processing display and manipu lation of the objects in a scene Polygon data tables can be organized into two groups geometric tables and attribute tables Geometric data tables contain ver tex coordinates and parameters to identify the spatial orientation of the polygon surfaces Attribute information for an object includes parameters specifying the deyree of transparency of the object and its surface reflectivity and texture char acteristics  A convenient organization for storing geometric data is to create three lists a vertex table an edge table and a polygon table Coordinate values for each ver tex in the object are stored in the vertex table The edge table contains pointers back into the vertex table to identify the vertices for each polygon edge And the polygon table contains pointers back into the edge table to identify the edges for each polygon This scheme is illustrated in  for two adjacent polygons on an object surface In addition individual objects and their component polygon faces can be assigned object and facet identifiers for easy reference  Listing the geometric data in three tables as in  provides a conve nient reference to the individual components vertices edges and polygons of each object Also the object can be displayed efficiently by using data from the edge table to draw the component lines An alternative arrangement is to use just two tables a vertex table and a polygon table But this scheme is less convenient and some edges could get drawn twice Another possibility is to use only a poly gon table but this duplicates coordinate information since explicit coordinate values are listed for each vertex in each polygon Also edge information would have to be reconstructed from the vertex listings in the polygon table  We can add extra information to the data tables ot  for faster infor mation extraction For instance we could expand the edge table to include for ward pointers into the polygon table so that common edges between polygons could be identified more rapidly   This is particularly useful for the ren dering procedures that must vary surface shading smoothly across the edges from one polygon to the next Similarly the vertex table could be expanded so that vertices are cross referenced to corresponding edges  Additional geometric information that is usually stored in the data tables includes the slope for each edge and the coordinate extents for each polygon As vertices are input we can calculate edge slopes and we can scan the coordinate     i E     Vv p Vy   Vv My VERTEX TABLE EDGE TABLE POLYGON SURFACE Ver Xe ey Ey VY Sy    Ey Vai Xge Vas 2a Ey Va V4 S  Ey  8s Ey Vai Xy  as Ey V3 Vy Val Nye Var Ze Ey Va Vy Vas ee Vor Ey Va Vs Eg Vue Vy     Geometric data table representation for two adjacent polygon surfaces formed with six edges and five vertices  values to identify the minimum and maximum x y and z values for individual polygons Edge slopes and bounding box information for the polygons are needed in subsequent processing for example surface rendering Coordinate ex tents are also used in some visible surface determination algorithms  Since the geometric data tables may contain extensive listings of vertices and edges for complex objects it is important that the data be checked for consis tency and completeness When vertex edge and polygon definitions are speci fied it is possible particularly in interactive applications that certain input er rors could be made that would distort the display of the object The more information included in the data tables the easier it is to check for errors There fore error checking is easier when three data tables vertex edge and polygon are used since this scheme provides the most information Some of the tests that could be performed by a graphics package are  that every vertex is listed as an endpoint for at least two edges  that every edge is part of at least one polygon  that every polygon is closed  that each polygon has at least one shared edge and  that if the edge table contains pointers to polygons every edge ref erenced by a polygon pointer has a reciprocal pointer back to the polygon  Plane Equations To produce a display of a three dimensional object we must process the input data representation for the object through several procedures These processing steps include transformation of the modeling and world coordinate descriptions to viewing coordinates then to device coordinates identification of visible sur faces and the application of surface rendering procedures For some of these processes we need information about the spatial orientation of the individual Section Polygon Surfaces   Ey Vi Ve Sy Ey Vy Vy Sy E53 Va Vy Sy Se Ey Va Vu S Ey Vys Vee Eg Vs Vy Sz  Edge table for the surfaces of  expanded to include pointers to the polygon table  Chapter Three mensional Object Representations  Figitre The vector N normal to the surtace of a plane described by the equation Ax  By  Cz  D  has Cartesian components G4 B CY  surface components ot the object This information 1s Obtained from the vertex coordinate values and tne equations that describe the polygon planes The equation for a plane surface can be expressed in the form Ay  By  Cz D Wie LY where   y  iy any point on the plane and the coetficients A B C and D are constants describing the spatial properties of the plane We can obtain the values of A B C and D by solving a set of three plane equations using the coordinate values for three noncollinear points in the plane For this purpose we can Select three successive polygon vertices  xy  y    Yo  and xs y3  and solve the following set of simultaneous linear plane equations for the ratios A D B D and C D  A D x   B Dyy   C D z   k  uo The solution for this set of equations can be obtained in determinant form using Cramer s rule as  an x A We B x  Ys x3 I  regdest a How Cz fy y D   ws   s v Expanding the determinants we can write the calculations for the plane coeffi cients in the form Am yey   Yolea   yale  Za B    29lx  xy  2x Xe  Lytdeg Co xy yn  Yay  X2 Ya  Wy  XAG  Yo  Dis  xyQysts  Yeo  X32  Wizs  EslyZe  As vertex values and other information are entered inte the polygon data struc ture values for A B C and D are computed for each polygon and stored with the other polygon data Orientation of a plane surface in space can be described with the normal vector to the plane as shown in  This surface normal vector has Carte sian components A B C  where parameters A B and C are the plane coeffi cients calculated in Eqs  Since we are usuaily dealing with polygon surfaces thal enclose an object interior we need o distinguish between the two sides of the surface The side of the plane that faces the object interior is called the inside face and the visible or outward side ts the outside face If polygon vertices are specified in a coun terclockwise direction when viewing the outer side of the plane in a right handed coordinate system the direction of the normal vecter will be from inside to out side This ts demonstrated for one plane of a unit cube in  To determine the components of the normal vector for the shaded surface shown in  we select three of the four vertices along the boundary of the polygon These points are selected in a counterclockwise direction as we view from outside the cube toward the origin Coordinates for these vertices in the order selected can be used in Eqs to obtain the plane coefficients A  B C D   Thus the normal vector for this plane is in the direction of the positive x axis  The elements of the plane normal can also be obtained using a vector cross product calculation We again select three vertex positions V  Vz and V3 taken in counterclockwise order when viewing the surface from outside to inside in a right handed Cartesian system Forming two vectors one from V to V and the other from V to V3 we calculate N as the vector cross product  N  V2  V   V5  Vy   This generates values for the plane parameters A B and C We can then obtain the value for parameter D by substituting these values and the coordinates for one of the polygon vertices in plane equation and solving for D The plane equation can be expressed in vector form using the normal N and the position P of any point in the plane as N P D  Plane equations are used also to identify the position of spatial points rela tive to the plane surfaces of an object For any point x y z not on a plane with parameters A B C D we have Ax By Cz D We can identify the point as either inside or outside the plane surface according to the sign negative or positive of Ax  By  Cz  D  if Ax  By  Cz  D  the point x y z is inside the surface if Ax  By  Cz  D  the point x y z is outside the surface These inequality tests are valid in a right handed Cartesian system provided the plane parameters A B C and D were calculated using vertices selected in a counterclockwise order when viewing the surface in an outside to inside direc tion For example in  any point outside the shaded plane satisfies the in equality x   while any point inside the plane has an x coordinate value less than  Polygon Meshes Some graphics packages for example PHIGS provide several polygon functions for modeling objects A single plane surface can be specified with a function such as illArea But when object surfaces are to be tiled it is more convenient to specify the surface facets with a mesh function One type of polygon mesh is the triangle strip This function produces m  connected triangles as shown in Fig  given the coordinates for n vertices Another similar function is the quadri lateral mesh which generates a mesh of n  by mn  quadrilaterals given   The shaded polygon surface of the unit cube has plane equation x  O and normal vector N     S    A triangle strip formed w triangles connecting vertices  ith ae   ay   A quadrilateral mesh containing quadrilaterals constructed from a by input vertex array  the coordinates for an n by m array of vertices  shows vertices forming a mesh of quadrilaterals  When polygons are specified with more than three vertices it is possible that the vertices may not all lie in one plane This can be due to numerical errors or errors in selecting coordinate positions for the vertices One way to handle this situation is simply to divide the polygons into triangles Another approach that is sometimes taken is to approximate the plane parameters A B and C We can do this with averaging methods or we can project the polygon onto the coordinate planes Using the projection method we take A proportional to the area of the polygon projection on the yz plane B proportional to the projection area on the xz plane and C proportional to the projection area on the xy plane  High quality graphics systems typically model objects with polygon meshes and set up a database of geometric and attribute information to facilitate processing of the polygon facets Fast hardware implemented polygon renderers are incorporated into such systems with the capability for displaying hundreds of thousands to one million or more shaded polygons per second usually trian gles  including the application of surface texture and special lighting effects  Displays of three dimensional curved lines and surfaces can be generated from an input set of mathematical functions defining the objects or from a set of user specified data points When functions are specified a package can project the defining equations for a curve to the display plane and plot pixel positions along the path of the projected function For surfaces a functional description is often tesselated to produce a polygon mesh approximation to the surface Usually this is done with triangular polygon patches to ensure that all vertices of any polygon are in one plane Polygons specified with four or more vertices may not have all vertices in a single plane Examples of display surfaces generated from functional descriptions include the quadrics and the superquadrics  When a set of discrete coordinate points is used to specify an object shape a functional description is obtained that best fits the designated points according to the constraints of the application Spline representations are examples of this class of curves and surfaces These methods are commonly used to design new object shapes to digitize drawings and to describe animation paths Curve fit ting methods are also used to display graphs of data values by fitting specified curve functions to the discrete data set using regression techniques such as the least squares method  Curve and surface equations can be expressed in either a parametric or a nonparametric form Appendix A gives a summary and comparison of paramet ric and nonparametric equations For computer graphics applications parametric representations are generally more convenient  A frequently used class of objects are the quadric surfaces which are described with second degree equations quadratics  They include spheres ellipsoids tori paraboloids and hyperboloids Quadric surfaces particularly spheres and ellip soids are common elements of graphics scenes and they are often available in graphics packages as primitives from which more complex objects can be con structed  Sphere In Cartesian coordinates a spherical surface with radius r centered on the coordi nate origin is defined as the set of points x y z that satisfy the equation tyra  We can also describe the spherical surface in parametric form using latitude and longitude angles    x rcos cos  a 2S os y rcos sin  ms sn  z rsing The parametric representation in Eqs provides a symmetric range for the angular parameters and  Alternatively we could write the parametric equations using standard spherical coordinates where angle  is specified as the colatitude   Then  is defined over the range    and  is often taken in the range    We could also set up the representation using pa rameters u and v defined over the range from to by substituting   au and  2a  Ellipsoid An ellipsoidal surface can be described as an extension of a spherical surface where the radii in three mutually perpendicular directions can have different val ues   The Cartesian representation for points over the surface of an el lipsoid centered on the origin is ve UR  d And a parametric representation for the ellipsoid in terms of the latitude angle  and the longitude angle  in  is   x r cos cos  m 2ehs w y r cosd sin  mws9sq7 10 z r sing Torus A torus is a doughnut shaped object as shown in  It can be generated by rotating a circle or other conic about a specified axis The Cartesian represen  Section Quadric Surfaces axia Psixy  V axis x axis   Parametric coordinate position r   on the surface of a sphere with radius r  x axis   Spherical coordinate parameters r    using colatitude for angle     An ellipsoid with radii r  and r centered on the coordinate origin  Zaxis a xy z  i xy plane  A torus with a circular cross section x axis centered on the coordinate origin   tation for points over the surface of a torus can be written in the form z r       11 ry Yr  VAY where r is any given offset value Parametric representations for a torus are simi lar to those for an ellipse except that angle  extends over  Using latitude and longitude angles  and we can describe the torus surface as the set of points that satisfy x rr  cos d cos  rShen y r r  cos sin  ns sa7 12 z r sing  This class of objects is a generalization of the quadric representations Super quadrics are formed by incorporating additional parameters into the quadric equations to provide increased flexibility for adjusting object shapes The number of additional parameters used is equal to the dimension of the object one para meter for curves and two parameters for surfaces  Superellipse We obtain a Cartesian representation for a superellipse from the corresponding equation for an ellipse by allowirg the exponent on the x and y terms to be vari able One way to do this is to write the Cartesian superellipse equation in the form Qs s    Se  13 Ty ty where parameter s can be assigned any real value When s  we get an ordi nary ellipse Corresponding parametric equations for the superellipse of Eq 13 can be expressed as x  r cos  ms0s7 74 y rysin   illustrates supercircle shapes that can be generated using various values for parameter s  Superellipsoid A Cartesian representation for a superellipsoid is obtained from the equation for an ellipsoid by incorporating two exponent parameters           se 15 ry ry n For s    we have an ordinary ellipsoid We can then write the corresponding parametric representation for the superellipsoid of Eq 15 as X  r cos cos26  n 2sosn y  r cos  sin  a s0S7 16  r sin id  illustrates supersphere shapes that can be generated using various values for parameters s and s  These and other superquadric shapes can be com bined to create more complex structures such as furniture threaded bolts and other hardware   e    Superellipses plotted with different values for parameter  and with r  Section S5uperquadrics Representations 88S   Molecular bonding As two molecules move away from each other the surface shapes stretch snap and finally contract into spheres  a  C2  b  Blobby muscle shapes in a human arm  Oto7  Od  BQO Dd a ee    Superellipsoids plotted with different values for parameters  and s  and with r  r  r   Some objects do not maintain a fixed shape but change their surface characteris tics in certain motions or when in proximity to other objects Examples in this class of objects include molecular structures water droplets and other liquid ef fects melting objects and muscle shapes in the human body These objects can be described as exhibiting blobbiness and are often simply referred to as blobby objects since their shapes show a certain degree of fluidity  A molecular shape for example can be described as spherical in isolation but this shape changes when the molecule approaches another molecule This distortion of the shape of the electron density cloud is due to the bonding that occurs between the two molecules  illustrates the stretching snap ping and contracting effects on molecular shapes when two molecules move apart These characteristics cannot be adequately described simply with spherical or elliptical shapes Similarly  shows muscle shapes in a human arm which exhibit similar characteristics In this case we want to model surface shapes so that the total volume remains constant  Several models have been developed for representing blobby objects as dis tribution functions over a region of space One way to do this is to model objecis as combinations of Gaussian density functions or bumps    A sur face function is then defined as fix yz  X be F T 17 i where rj  Vx    zt parameter T is some specified threshold and parame ters  and b are used to adjust the amount of blobbiness of the individual objects Negative values for parameter b can be used to produce dents instead of bumps  illustrates the surface structure of a composite object modeled with four Gaussian density functions At the threshold level numerical root finding techniques are used to locate the coordinate intersection values The cross sec tions of the individual objects are then modeled as circles or ellipses If two cross sections aie near to each other they are merged to form one blobby shape as in  whose structure depends on the separation of the two objects  Other methods for generating blobby objects use density functions that fall off to in a finite interval rather than exponentially The metaball model de scribes composite objects as combinations of quadratic density functions of the form b1 3r   if0 r d fry  5o ridY  ifa rsd 18  ifr d And the soft object model uses the function 22r 4r   o S COif0 r d fir    9d  Od  19  ifr d Some design and painting packages now provide blobby function modeling for handling applications that cannot be adequately modeled with polygon or spline functions alone  shows a user interface for a blobby object modeler using metaballs  in drafting terminology a spline is a flexible strip used to produce a smooth curve through a designated set of points Several small weights are distributed along the length of the strip to hold it in position on the drafting table as the curve is drawn The term spline curve originally referred to a curve drawn in this manner We can mathematically describe such a curve with a piecewise cubic   A screen layout used in the Blob Modeler and the Blob Animator packages for modeling objects with metaballs  Courtesy of Thorson Digital Image   Section Spline Representations a   A three dimensional Gaussian bump centered at position with height b and standard deviation a    A composite blobby object formed with four Gaussian bumps  Chapter Three Dimensional Object Representations   A set of six contro points interpolated with piecewise continuous polynomial sections   A set of six control points approximated with piecewise continuous polynomial sections  polynomial function whose first and second derivatives are continuous across the various curve sections In computer graphics the term spline curve now refers to any composite curve farmed with polynomial sections satisfying speci fied continuity conditions at the boundary of the pieces A spline surface can be described with two sets of orthogonal spline curves There are several different kinds of spline specifications that are used in graphics applications Each individ ual specification simply refers to a particular type of polynomial with certain specified boundary conditions  Splines are used sn graphics applications to design curve acd surface shapes to digitize drawings for computer storage ard to specify animation paths for the objects or the camera in a scene Typical CAD applications for splines include the design of automobile bodies aircraft and spacecraft surfaces and ship hulls  Interpolation and Approximation Splines We specify a spline curve by giving a set of coordinate positions called control points which indicates the general shape of the curve These control points are then fitted with piecewise continuous parametric polynomial functions in one of two ways When polynemual sections are fitted so that the curve passes through each control point as in  the resulting curve is said to interpolate the set of control points On the other hand when the polynomials are fitted to the general control point path without necessarily passing through any control point the resulting curve is said to approximate the set of control points    Interpolation curves are commonly used to digitize drawings or to specify animation paths Appreximation curves are primarily used as design tools to structure object surfaces  shows an approximation spline surface created for a design application Straight lines connect the control point positions above the surface  A spline curve is defined modified and manipulated with operations on the control points By interactively selecting spatial positions for the control points a designer can set up an initial curve After the polynomial fit is displayed for a given set of control points the designer can then reposition some or all of the control points to restructure the shape of the curve In addition the curve can be translated rotated or scaled with transformations applied to the control points CAD packages can also insert extra contro points to aid a designer in ad justing the curve shapes  The canvex polygon boundary that encloses a set of control points is called the convex hull One way to envision the shape of a convex hull is to imagine a rubber band stretched around the positions of the contro points so that each con trol point is either on the perimeter of the hull or inside it   Convex hulls provide a measure for the deviation of a curve or surface from the region bounding the contro points Some splines are bounded by the convex hull thus ensuring that the polynomials smoothly follow the control points without erratic oscillations Also the polvgon region inside the convex hull is useful in some al gorithms asa clipping region  A polyline connecting the sequence of control points for an approximation spline is usually displaved to remind a designer of the control point ordering This set of connected line segments is often referred to as the control graph of the curve Other names for the series of straight line sections connecting the control points in the order specified are control polygon and characteristic polygon Fig ure 23 shows the shape of the control graph for the control point sequences in    An approximation spline surface for a CAD application in automotive design Surface contours are plotted with polynomial curve sections and the surface control points are connected with straight line segments  Courtesy of Evans  Sutherland  Parametric Continuity Conditions To ensure a smooth transition from one section of a piecewise parametric curve to the next we can impose various continuity conditions at the connection points If each section of a spline is described with a set of parametric coordinate functions of the form x  x u  y  yu  z u  W Susu  20  Pz   Convex hull shapes dashed Jines for two sets of control points Section Spline Representations Chapter Thiee Dumensional Object Representations Fronre 24 Piecewise construction of a curve by joining two curve segments using different orders of continuity  a zero  order continuily only  b first order continuity and   second order continuity  R g ws P rs A t      i  t  Po  Py    of p SO b XN ae a  Figure 23 Control graph shapes dashed lines for two different sets of contro points  we set parametric continuity by matching the parametric derivatives of adjoin ing curve sections at their common boundary  Zero order parametric continuity described as C continuity means simply that the curves meet That is the values of x y and z evaluated at uv for the first curve section are equal respectively to the values of x y and z evaluated at u for the next curve section First order parametric continuity referred to as C continuity means that the first parametric derivatives tangent lines of the coor dinate functions in Eq 20 for two successive curve sections are equal at their joining point Second order parametric continuity or C continuity means that both the first and second parametric derivatives of the two curve sections are the same at the intersection Higher order parametric continuity conditions are de fined similarly  shows examples of CY C  and C continuity  With second order continuity the rates of change of the tangent vectors for connecting sections are equal at their intersection Thus the tangent line transi tions smoothly from one section of the curve to the next Fig  24 c  But with first order cor tinuity the rates of change of the tangent vectors for the two sec tiors can be quite different  b  so that the general shapes of the two adjacent sections can change abruptly First order continuity is often sufficient for digitizing drawings and some design applications while second order continuity is useful for setting up animation paths for camera motion and for many preci sion CAD requirements A camera traveling along the curve path in  b with equal steps in parameter  would experience an abrupt change in accelera tion at the boundary of the two sections producing a discontinuity in the motion sequence But if the camera were traveling along the path in  c  the frame sequence for the motion would smoothly transition across the boundary  Geometric Continuity Conditions An alternate method for joining two successive curve sections is to specify condi tions for geometric continuity  n this case we only require parametric deriva tives of the two sections te be proportional te each other at their common bound ary instead of equal to each other  Zero order geometric continuity described as G continuity is the same as zero order parametric continuity That is the two curves sections must have the same coordinate position at the boundary point First order geometric continu ity or G continuity means that the parametric first derivatives are proportional at the intersection of two successive sections If we denote the parametric posi tion on the curve as P u  the direction of the tangent vector P u  but not neces sarily its magnitude will be the same for two successive curve sections at their joining point under G continuity Second order geometric continuity or G con tinuity means that both the first and second parametric derivatives of the two curve sections are proportional at their boundary Under G continuity curva tures of two curve sections will match at the joining position  A curve generated with geometric continuity conditions is similar to one generated with parametric continuity but with slight differences in curve shape  provides a comparison of geometric and parametric continuity With geometric continuity the curve is pulled toward the section with the greater tan gent vector  Spline Specifications There are three equivalent methods for specifying a particular spline representa tion  We can state the set of boundary conditions that are imposed on the spline or  we can state the matrix that characterizes the spline or  we can state the set of blending functions or basis functions that determine how spec ified geometric constraints on the curve are combined to calculate positions along the curve path  To illustrate these three equivalent specifications suppose we have the fol lowing parametric cubic polynomial representation for the x coordinate along the path of a spline section  xu  aw  bw  cu  dy Osus  21  Boundary conditions for this curve might be set for example on the endpoint co ordinates x and x and on the parametric first derivatives at the endpoints x  and   These four boundary conditions are sufficient to determine the values of the four coefficients a  b  c  and d  From the boundary conditions we can obtain the matrix that characterizes this spline curve by first rewriting Eq 21 as the matrix product Po a aa P Po B  GO cc ta  b   Three control points fitted with two curve sections joined with  a parametric continuity and b geometric continuity where the tangent vector of curve C at point p has a greater magnitude than the tangent vector of curve C at p  Section Spline Representations  Chapter Three Dimensional Object Representations oe   x a   w3  22  Ro oF FR M4   u Cc where U is the row matrix of powers of parameter u and C is the coefficient col umn matrix Using Eq 22 we can write the boundary conditions in matrix form and solve for the coefficient matrix C as C Moypine  Mgeom 23  where Mygeom is a four element column matrix containing the geometric constraint values boundary conditions on the spline and Mypine is the by matrix that transforms the geometric constraint values to the polynomial coefficients and provides a characterization for the spline curve Matrix Mgcom contains control point coordinate values and other geometric constraints that have been specified Thus we can substitute the matrix representation for C into Eq 22 to obtain x t  U Mapiine  Migeom 24  The matrix Mypune Characterizing a spline representation sometimes called the basis matrix is particularly useful for transforming from one spline representation to another  Finally we can expand Eq 24 to obtain a polynomial representation for coordinate x in terms of the geometric constraint parameters  aw    gy BF 23 k O where g are the constraint parameters such as the control point coordinates and slope of the curve at the control points and BF u are the polynomial blending functions In the following sections we discuss some commonly used splines and their matrix and blending function specifications  This class of splines is most often used to set up paths for object motions or to provide a representation for an existing object or drawing but interpolation splines are also used sometimes to design object shapes Cubic polynomials offer a reasonable compromise between flexibility and speed of computation Com pared to higher order polynomials cubic splines requ re less calculations and memory and they are more stable Compared to lower order polynomials cubic splines are more flexible for modeling arbitrary curve shapes  Given a set of control points cubic interpolation splines are obtained by fit ting the input points with a piecewise cubic polynomial curve that passes through every control point Suppose we haven  contro points specified with coordinates Pow Oe Ye ah k 02  A cubic interpolation fit of these points is illustrated in  We can de scribe the parametric cubic polynomial that is to be fitted between each pair of control points with the following set of equations  x u  aw  bu  cu  dy yu  aye  bye t ourd su  26  zlu  au  ba  cu  d  For each of these three equations we need to determine the values of the four co efficients b c and d in the polynomial representation for each of the curve sections between the n  control points We do this by setting enough bound ary conditions at the joints between curve sections so that we can obtain nu merical values for all the coefficients In the following sections we discuss com mon methods for setting the boundary conditions for cubic interpolation splines  Natural Cubic Splines One of the first spline curves to be developed for graphics applications is the nat ural cubic spline This interpolation curve is a mathematical representation of the original drafting spline We formulate a natural cubic spline by requiring that two adjacent curve sections have the same first and second parametric deriva tives at their common boundary Thus natural cubic splines have C continuity  If we have   control points to fit as in  then we have n curve sections with a total of polynomial coefficients to be determined At each of the n  interior control points we have four boundary conditions The two curve sections on either side of a control point must have the same first and sec ond parametric derivatives at that control point and each curve must pass through that control point This gives us 4n  equations to be satisfied by the 4n polynomial coefficients We get an additional equation from the first control point pp the position of the beginning of the curve and another condition from control point p  which must be the last point on the curve We still need two more conditions to be able to determine values for all coefficients One method for obtaining the two additional conditions is to set the second derivatives at pp and p  to Another approach is to add two extra dummy control points one at each end of the original control point sequence That is we add a control point p  and a control point p   Then all of the original control points are interior points and we have the necessary 4n boundary conditions  Although natural cubic splines are a mathematical model for the drafting spline they have a major disadvantage If the position of any one control point is altered the entire curve is affected Thus natural cubic splines allow for no local control  so that we cannot restructure part of the curve without specifying an entirely new set of control points    A piecewise continuous cubic spline interpolation of n  control points  Section Cubic Spline interpolation Methods Chapter Three Dimensional Object Representations Hermile Interpolation A Hermite spline named after the French mathematician Charles Hermite is an interpolating piecewise cubic polynomial with a specified tangent at cach control point Unlike the natural cubic splines Hermite splines can be adjusted locally because each curve section is only dependent on its endpoint constraints  If P u represents a parametric cubic point function for the curve section be tween control points py and p   a5 shown in  ther the boundary con ditions that define this Hermite curve section are P O  py POD Pay 27 PO  Dp  P t  Dp  with Dp and Dp  specifying the values for the parametric derivatives slope of the curve at control points p and p  respectively  We can write the vector equivalent of Eqs 26 for this Hermite curve sec tion as POO  au  but  ecu d Qfue1 28  where the x component of P is x u  a6  bu  cu  d  and similar y for the yand z components The matrix equivalent of Eq 28 1s a  b  Pu  fut wt dy   29 d and the derivative of the point function can be expressed as Pra   2u Q  30  aa oe Substituting endpoint values and for parameter w into the previous two equa tions we can express the Hermite boundary conditions 27 in the matrix form  Pi 1 a  101 Pro   b C30 Dp c Dpi  d PB ee we ee   ee Plu  Olu  youl  XN tal  Gla yi 20a Parametric point function P u for a bp Hermite curve section between control points p and p  Solving this equation for the polynomial coefficients we have Px  Part Dp  Dp  1 Px   Pao 0 Dp  0 Dpist ane ep wor oo W  32  Pitt Dp Dpx  iH  where M  the Hermite matrix is the inverse of the boundary constraint matrix Equation 29 can thus be written in terms of the boundary conditions as Px P u   we v2 uw My  PE  33 Dp Dp  Finally we can determine expressions for the Hermite blending functions by carrying out the matrix multiplications in Eq 33 and collecting coefficients for the boundary constraints to obtain the polynomial form  Pad  p28    py   3x7  Dp    ue  Dpy u   34  pou  pyr Wi u  Dp Hu  Dp Hy  The polynomials H u for k   are referred to as blending functions be cause they blend the boundary constraint values endpoint coordinates and slopes to obtain each coordinate position along the curve  shows the shape of the four Hermite blending functions  Hermite polynomials can be useful for some digitizing applications where it may not he too difficult to specify or approximate the curve slopes But for most problems in computer graphics it is more useful to generate spline curves without requiring input values for curve slopes or other geometric information in addition to control point coordinates Cardinal splines and Kochanek Bartels splines discussed in the following twa sections are variations on the Hermite splines that do not require input values for the curve derivatives at the control points Procedures for these splines compute parametric derivatives from the co ordinate positions of the control points  Cardinal Splines As with Hermite splines cardinal splines are interpolating piecewise cubics with specified endpoint tangents at the boundary of each curve section The difference Section Cubic Spline Interpolation Methods Hla  4  H u   o e be e    Hiya  4  H u  6 2 u eda a a OUT  d  The Hermite blending functions   Pee   Parametric puint function P u  for a cardinal spline section between control points p and p    is that we do not have to give the values for the endpoint tangents For a cardinal spline the value for the slope at a control point is calculated from the coordinates of the two adjacent control points  A cardinal spline section is completely specified with four consecutive con tral points The middie two control points are the section endpoints and the other two points are used in the calculation of the endpoint slopes If we take P u as the representation for the parametric cubic point function for the curve section between contrel points p  and   as in  then the four control points from p _ to py  are used to set the boundary conditions for the cardinal spline section as P O  P  Pias  35 PO   Depa  Pe  Pl    per  pod Thus the slopes at control points p and p  are taken to be proportional respec  tively to the chords p  Py  and Py Pxrz   Parameter  is called the tension parameter since it controls how loosely or tightly the cardinal spline fits the input control points  illustrates the shape of a cardinal curve for very small and very large values of tension t When   this class of curves is referred to as Catmull Rom splines or Overhauser splines  Using methods similar to those for Hermite splines we can convert the boundary conditions 35 into the matrix form P u   u2 ww Me  Pe 30  where the cardinal matrix is M   37  withs     Expanding matrix equation 36 into polynomial form we have P u  py  su  28u su  pyl  su   s  u   Proil s  u8 3B  2s u  sul pyaolsu  su  38  py 1CAR u  p CAR u  pyis CAR W  pyy2CAR u where the polynomials CAR u for k   are the cardinal blending func  tions  gives a plot of the basis functions for cardinal splines with   Kochanek Bartels Splines These interpolating cubic polynomials are extensions of the cardinal splines Two additional parameters are introduced into the constraint equations defining Kochanek Bartels splines to provide for further flexibility in adjusting the shape of curve sections  Given four consecutive control points labeled p _1 Pp Pes1 ANd Py we define the boundary conditions for a Kochanek Bartels curve section between p and px as P Q  Px PCD  piss PO n   NIG  HA  py  Pr  349  b  CP ear  Pd  Pou   UG  OU  Oper  Pd    BML  ch Pes  Ped where t is the tension parameter b is the bias parameter and c is the continuity parameter In the Kochanek Bartels formulation parametric derivatives may not be continuous across section boundaries    Tangent vectors at the endpoints of a cardinal spline section are proportional to the chords formed with neighboring control points dashed lines  LN aan  Effect of the tension parameter on t t d the shape of a cardinal spline Looser Curve  Tighter Curve  _ section  Tension parameter t has the same interpretation as in the cardinal spline formulation that is it controls the looseness or tightness of the curve sections Bias b is used to adjust the amount that the curve bends at each end of a section so that curve sections can be skewed toward one end or the other   Pa rameter  controls the continuity of the tangent vector across the boundaries of sections If c is assigned a nonzero value there is a discontinuity in the slope of the curve across section boundaries  Kochanek Bartel splines were designed to model animation paths In par ticular abrupt changes in motion of a object can be simulated with nonzero val ues for parameter c  CARglu CAR u    4 st ee te ttt uv  4 1  ah b CARAu CARSu 0 Poi ditsirtiri Liriitiiiit y uv 0 0  te id    The cardinal blending functions for t  Oand s  P P2 YN  Po wd py pO Ps Po P oe Po b O o b   Effect of the bias parameter on the shape of a Kochanek Bartels spline section  This spline approximation method was developed by the French engineer Pierre B zier for use in the design of Renault automobile bodies B zier splines have a number of properties that make them highly useful and convenient for curve and surface design They are also easy to implement For these reasons B zier splines are widely available in various CAD systems in general graphics packages such as GL on Silicon Graphics systems  and in assorted drawing and painting pack ages such as Aldus SuperPaint and Cricket Draw  B zier Curves In general a B zier curve section can be fitted to any number of control points The number of control points to be approximated and their relative position de termine the degree of the B zier polynomial As with the interpolation splines a B zier curve can be specified with boundary conditions with a characterizing matrix or with blending functions For general B zier curves the blending func tion specification is the most convenient  Suppose we are given n  control point positions py    yy   with k varying from to These coordinate points can be blended to produce the fol lowing position vector P u  which describes the path of an approximating B zier polynomial function between p and p   Pu  Sp BEZ  uw  OFu 40 k  The B zier blending functions BEZ  u are the Bernstein polynomials BEZ  u  Cin Ku  wet 41 where the C n k are the binomial coefficients  n  kG _ 42  C n k   Equivalently we can define B zier blending functions with the recursive calcula tion BEZ  u    u BEZ    u  uBEZ _    a  H k 43  Section B zier Curves and Surfaces Chapler with BEZ   w  and BF Z    wk Vector equation 40 represents a set of Three Oimensional Object three parametric equations for the individual curve coordinates Representations  x    x BEZ  u  k yu  y BEZ  u  44 k u   z BEZ Au k Asa rule a B zier curve is a polynomial of degree cne less than the number of control points used Three points generate a parabola four points a cubic curve and so forth  demonstrates the appearance of some Bezier curves for various selections of control points in the xy plane z   With certain control point placements however we obtain degenerate B zier polynomials For example a B zier curve generated with three collinear contro points is a straight line segment And a set of control points that are all at the same coordi nate position produces a B zier curve that is a single point  B zier curves are commonly found in painting and drawing packages as well as CAD systems since they are easy to implement and they are reasonably powerful in curve design Efficient methods for determining coordinate positions along a B zier curve can be set up using recursive calculations For example suc cessive binomial coefficients can be calculated as   Examples of two dimensional B zier curves generated trom three four  and five control points Dashed lines connect the control point positions  k Ci k a La k 45  for n  k The following example program illustrates a method for generating B zier curves  include math h  include graphics h  void computeCoefficients int n int  c   int k i  for k ks n k     Compute n  k i n k    e k   for isn c k   for i n k e k   i  i k l a   i  i     void computePoint    float u woPt3  pt int nControls wePt  controls int    int k n  nfontrols   float blend i pt x  pt y  pt z2    Add in influence of each control point  for k O k nControls k    blend  c k  powf u k  powf l u n k   pt x  controls k x  blend  pt y  controls k y  blend   controls k z  blend   pt z void bezier wePt3  controls int nControls int m wePt3  curve    Allocate space for the coefficients  int    int  malloc nControls  sizeof int  int i computeCoefficients nControls c  for  is m i  i computePoint i  float m  curveli  nControls controls c    free c   Properties of B zier Curves A very useful property of a B zier curve is that it always passes through the first and last contro points That is the boundary conditions at the two ends of the curve are P O  Po  46 P t  p  Section B zier Curves and Surfaces Chapter  Three Dimensional Ohyect Representations P3 Lf v4          oN  Poe i     L     pe o oe p Bo Px Pp Piguere E0 A closed Bezier curve generated by specifying the first and last control points at the same location P B P  Pe Pe figtay 36 A Bezier curse can be made to pass closer loa given coordinate pusition by assigning muluple control points to that positian  Values of the parametric first derivatives of a B zier curve at the endpoints can be calculated from control point coordinates as P   np t Poume 47 P   np   ap  Thus the slope at the beginning of the curve is along the hne joining the first two control points and the slope at the end of the curve is along the line joining the last two endpoints Similarly the parametric second derivatives of a B zier curve at the endpoints are calculated as P 0O  nt  Dips  pd  pi  pol  45 P C1  Gr  Dip  Poe  ae d Pw  Another important property of any B zier curve is that it lies within the convex hull convex polygon boundary of the control points This follows from the properties of B zier blending functions They are all positive and their sum is always  NBEZ  ub  49   kat so that any curve position is simply the weighted sum of the control point posi tions The convex hull property for a B zier curve ensures that the polynomial smoothly follows the control points without erratic oscillations  Design Techniques Using Bezier Curves Closed B zier curves are penerated by specifving the first and last control paints at the same position as in the example shown in  Also specifying mul tiple control points at a single coordinate position gives more weight to that posi tion In  a single coordinate position is input as two centro points and the resulting curve is pulled nearer to this position  We can fit a B zier curve to any number of control points but this requires the calculation of polynonial functions of higher degree When complicated curves are to be generated they can be formed by piecing several B zier sections of lower degree together Piecing together smaller sections also gives us better control over the shape of the curve in small regions Since Bezier curves pass through endpoints it 1s easy to match curve sections zero order continuity  Alsu B zier curves have the invportant property that the tangent to the curve at an endpoint 1s along the line joining that endpoint to the adjacent control point Therefore to obtain first order continuity between curve sections we can pick control points p  and p of a new section to be along the same straight line as control points p  and p of the previous section   When the two curve sections have the same number of contre points we obtain C continuity by choosing the first cantrol point of the new section as the last control point of the previous section and by positioning the second control point of the new sec tion at position Pot Py pe   Piecewise approximation curve formed with two B zier sections Zero order and first order continuity are attained between curve sections by setting po  p and by making points p  p  and p collinear  Thus the three control points are collinear and equally spaced  We obtain C continuity between two B zier sections by calculating the po sition of the third control point of a new section in terms of the positions of the last three control points of the previous section as Pn  Pa  Pra  Requiring second order continuity of B zier curve sections can be unnecessarily restrictive This is especially true with cubic curves which have only four control points per section In this case second order continuity fixes the position of the first three control points and leaves us only one point that we can use to adjust the shape of the curve segment  Cubic B zier Curves Many graphics packages provide only cubic spline functions This gives reason able design flexibility while avoiding the increased calculations needed with higher order polynomials Cubic B zier curves are generated with four control points The four blending functions for cubic B zier curves obtained by substi tuting  into Eq 41 are BEZo u    wu  BEZ u  3u  wu  BEZ  u  3w1  w  BEZ  u   50  Plots of the four cubic B zier blending functions are given in  The form of the blending functions determine how the control points influence the shape of the curve for values of parameter u over the range from to At u   Section B zier Curves and Surfaces BEZ iu Ie  BEZ ur   LE Q Popuidrrrit L poi y 0 08 a BEZ u BEZ 6 O2t tot u 0 66   The four B zier blending functions for cubic curves     the only nonzero blending function is BEZ  which has the value Atu  the only nonzero function is BEZ   with a value of J at that point Thus the cubic B zier curve will always pass through control points p and p  The other func tions BEZ  and BEZ  influence the shape of the curve at intermediate values of parameter u so that the resulting curve tends toward points p and py  Blend ing function BEZ  is maximum at wv  and BEZ  is maximum at    We note in  that each of the four blending functions is nonzero over the entire range of parameter u Thus B zier curves do not allow for local control of the curve shape If we decide to reposition any one of the control points the entire curve will be affected  At the end positions of the cubic B zier curve the parametric first deriva tives slopes are PO  Sp  pod PC  p ps And the parametne second derivatives are P   p  2p  py  P   p 2p  pd We can use these expressions for the parametric derivatives te construct piece wise curves with C or C continuity between sections By expanding the polynomial expressions for the blending functions we Section can write the cubic B zier point function in the matrix form Be zier Curves and Surfaces Po Pi  Plu  fe uw Moe  p  51 P3 where the B zier matrix is   M3  52 me  30 CO esa 0 We could also introduce additional parameters to allow adjustment of curve tension and bias  as we did with the interpolating splines But the more use ful B splines as well as splines provide this capability  Bezier Surfaces Two sets of orthogonal B zier curves can be used to design an object surface by specifying by an input mesh of control points The parametric vector function for the B zier surface is formed as the Cartesian product of B zier blending func tions  Plu v SD py BEZ   BEZ  UY  53 pa0k with p  specifying the location of the Gm  by n  control points   illustrates two B zier surface plots The control points are con nected by dashed lines and the solid lines show curves of constant u and con stant v Each curve of constant u is plotted by varying v over the interval from to with u fixed at one of the values in this unit interval Curves of constant v are plotted similarly       ae Z  oe f v  t a  prs t   Ps fT  y     o _s NNO SIZ XN Wie b  B zier surfaces constructed for a m    and b m    Dashed lines connect the contro points Chapter Three Dimensional Object Representations  Boundary Line   A composite B zier surface constructed with two B zier sections joined at the indicated boundary line The dashed lines connect specified control points First order continuity is established by making the ratio of length L to length L constant for each collinear line of control points across the boundary between the surface sections  B zier surfaces have the same properties as B zier curves and they provide a convenient method for interactive design applications For each surface patch we can select a mesh of control points in the xy ground plane then we choose elevations above the ground plane for the z coordinate values of the control points Patches can then be pieced together using the boundary constraints   illustrates a surface formed with two B zier sections As with curves a smooth transition from one section to the other is assured by establish ing both zero order and first order continuity at the boundary line Zero order continuity is obtained by matching control points at the boundary First order continuity is obtained by choosing control points along a straight Jine across the boundary and by maintaining a constant ratio of col inear line segments for each set of specified contro points across section boundar cs  These are the most widely used class of approximating splines B splines have two advantages over B zier splines  the degree cf a B spline polynomial can be set independently of the number of control points with certain limitations  and  B splines allow local control over the shape of a spline curve or surface The trade off is that B splines are more complex than B zier splines B Spline Curves We can write a general expression for the calculation of coordinate positions along a B spline curve in a blending function formulation as Plu    pp By Uymin SUS Mg   2d EH d 54 k where the p are an input set of n  control points There are several differences between this B spline formulation and that for B zier splines The range of para meter u now depends on how we choose the B spline parameters And the B spline blending functions B  are polynomials of degree d  where parameter d can be chosen to be any integer value in the range from up to the number of control points   Actually we can also set the value of d at but then our curve is just a point plot of the control points  Local control for B splines is achieved by defining the blending functions over subintervals of the total range of u  Blending functions for B spline curves are defined by the Cox deBoor re cursion formulas  ifuySu ua B  a   a otherwise  55 u u Upeg Bu   __t_ By g lu  Sv Bysia  Uged   Ux Ugrg  Heat where each blending function is defined over d subintervals of the total range of u The selected set of subinterval endpoints u is referred to as a knot vector We can choose any values for the subinterval endpoints satisfying the relation u  Wy4  Values for uy and ua then depend on the number of control points we select the value we choose for parameter d and how we set up the subinter vals knot vector  Since it is possible to choose the elements of the knot vector so that the denominators in the previous calculations can have a value of this for mulation assumes that any terms evaluated as are to be assigned the value   demonstrates the local control characteristics of B splines In addition to Jocal control B splines allow us to vary the number of control points used to design a curve without changing the degree of the polynomial Also any number of control points can be added or modified to manipulate curve shapes Similarly we can increase the number of values in the knot vector to aid in curve design When we do this however we also need to add control points since the size of the knot vector depends on parameter  B spline curves have the following properties   The polynomial curve has degree d  and C  continuity over the range of u   For  control points the curve is described with n  blending func tions  Each blending function is defined over d subintervals of the total range of u starting at knot value   The range of parameter u is divided into n  d subintervals by the n  d values specified in the knot vector  Section B Spline Curves and Surfaces Chapter Three Dimensional Object Representations   Local modification of a B spline curve Changing one of the control points in a produces curve b  which is modified only in the neighborhood of the altered control point   With knot values labeled as uo       the resulting B spline curve is defined only in the interval from knot value u   up to knot value u    Each section of the spline curve between two successive knot values is in fluenced by d control points   Any one control point can affect the shape of at most d curve sections  In addition a B spline curve lies within the convex hull of at most d  control points so that B splines are tightly bound to the input positions For any value of u in the interval from knot value u _ to u   the sum over all basis functions is  SB   56 k Given the control point positions and the value of parameter d we then need to specify the knot values to obtain the blending functions using the recur rence relations 55 There ate three general classifications for knot vectors uni form open uniform and nonuniform B splines are commonly described accord ing to the selected knot vector class  Uniform Periodic B Splines When the spacing between knot values is constant the resulting curve is called a uniform B spline For example we can set up a uniform knot vector as     0   Often knot values are normalized to the range between and as in    It is convenient in many applications to set up uniform knot values with a sepa ration of and a starting value of  The following knot vector is an example of this specification scheme        By u B alu 8 6 O4 a2 footbag pti Leto petririiy u 4 1 5 a  b   u  u 0 0 04 2 Toda a Cortirritiit Lriit y poi tera isis batts Ly 0G 4 c  d   Periodic B spline blending functions for n  d  and a uniform integer knot vector  Uniform B splines have periodic blending functions That is for given val ues of and d all blending functions have the same shape Each successive blending function is simply a shifted version of the previous function  B g t  By  gut Au  Boy du  An  57  where Aw is the interval between adjacent knot values  shows the quadratic uniform B spline blending functions generated in the following exam  ple for a curve with four control points  Example Uniform Quadratic B Splines To illustrate the calculation of B spline blending functions for a uniform integer knot vector we select parameter values d  n  The knot vector must then contain n  d   knot values     and the range of parameter u is from to with n  d  subintervals  Chapter Each of the four blending functions spans  subintervals of the total range of Three Dimensional Object y Using the recurrence relations 55 we obtain the first blending function as Representations tw fordsu By  bul2  w  Fe  GB  w forl u 43 for2 u We obtain the next periodic blending function using relationship 57 substitut ing u  for vin Bg3 and shifting the starting positions up by  tu  forl u By    Fu DG wW yu D4 yw  for2su  uy for3su Similarly the remaining two periodic functions are obtained by successively shifting B  to the right  r  qlu  2y for  u Boga  Fle   w bQi 3KS 4H  fordsu a  uF for4su x a stu   for3eu By  dtu   wu   v    for4su L 2M  uy forS su  A plot of the four periodic quadratic blending functions is given in  which demonstrates the local feature of B solines The first control point is multi plied by blending function Bo  u  Therefore changing the position of the first control point only affects the shape of the curve up to u  Similarly the last control point influences the shape of the spline curve in the interval where B  is defined   also illustrates the limits of the B spline curve for this example All blending functions are present in the interval from uy   to u    Below 338 and above not all blending functions are present This is the range of the poly Pp Pz c ay  Quadratic periodic B spline fitted   to four control points in the xy es Ps plane  nomial curve and the interval in which Eq 56 is valid Thus the sum of all blending functions is within this interval Outside this interval we cannot sum all blending functions since they are not all defined below and above  Since the range of the resulting polynomial curve is from to we can deter mine the starting and ending positions of the curve by evaluating the blending functions at these paints to obtain Poan   Po  ppd Pend   pr  py  Thus the curve starts at the midposition between the first two control points and ends at the mid position between the last two control points  We can also determine the parametric derivatives at the starting and ending posi tions of the curve Taking the derivatives of the blending functions and substitut ing the endpoint values for parameter u we find that Pon  Pi Po  Pend  P3  Po The parametric slope of the curve at the start position is parallel to the line join ing the first two control points and the parametric slope at the end of the curve is parallel to the line joining the last two control points  An example plot of the quadratic periodic B spline curve is given in  for four control points selected in the xy plane  In the preceding example we noted that the quadratic curve starts between the first two control points and ends at a position between the last two control points This result is valid for a quadratic periodic B spline fitted to any number of distinct control points In general for higher order polynomials the start and end positions are each weighted averages of d  control points We can pull a spline curve closer to any control point position by specifying that position mul tiple times  General expressions for the boundary conditions for periodic B splines can oe obtained by reparameterizing the blending functions so that parameter u is mapped onto the unit interval from to Beginning and ending conditions are then obtained at u  Dandu  Cubic Periodic B Splines Since cubic periodic B splines are commonly used in graphics packages we con sider the formulation for this class of splines Periodic splines are particularly useful for generating certain closed curves For example the closed curve in Fig 44 can be generated in sections by cyclically specifying four of the six control  Chapter Three Dimensiona Object Representations   A closed periodic piecewise cubic B spline constructed with cyclic specification of the six control points  points shown at each step If any three consecutive control points are identical the curve passes through that coordinate position  For cubics d  and each blending function spans four subintervals of the total range of u If we are to fit the cubic to four control points then we could use the integer knot vector      and recurrence relations 55 to obtain the periodic blending functions as we did in the last section for quadratic periodic B splines  In this section we consider an alternate formulation for periodic cubic B splines We start with the boundary conditions and obtain the blending functions normalized to the interval  u  Using this formulation we can also easily obtain the characteristic matrix The boundary conditions for periodic cubic B splines with four consecutive control points labeled po p  pz and ps are PO   po  4p  pad PC   Py  4p2  Ps  58 P O   p  po  P   p5 py  These boundary conditions are similar to those for cardinal splines Curve sec tions are defined with four control points and parametric derivatives slopes at the beginning and end of each curve section are parallel to the chords joining ad jacent contro points The B spline curve section starts at a position near p and ends at a position near p  A matrix formulation for a cubic periodic B splines with four control points can then be written as P u   v2 M   59  where the B spline matrix for periodic cubic polynomials is 30 M  60 Be  1 This matrix can be obtained by solving for the coefficients in a general cubic polynomial expression using the specified four boundary conditions  We can also modify the B spline equations to include a tension parameter  as in cardinal splines  The periodic cubic B spline with tension matrix then has the form  t 9r     a M   18  61    sO t 21 t which reduces to Mg when t   We obtain the periodic cubic B spline blending functions over the parame ter range from to by expanding the matrix representation into polynomial form For example for the tension value t  we have Boat  ta u  Osusl B  u      62 Bz s u    3u8   Bn   By u   Open Uniform B Splines This class of B splines is a cross between uniform B splines and nonuniform B splines Sometimes it is treated as a special type of uniform B spline and some times it is considered to be in the nonuniform B spline classification For the open uniform B splines or simply open B splines the knot spacing is uniform except at the ends where knot values are repeated d times  Following are two examples of open uniform integer knot vectors each with a starting value of      ford  2andn      ford 4andn We can normalize these knot vectors to the unit interval from to   33 67   ford  2andn      ford  4andn Section B Spline Curves and Surfaces Chapter Three Dimensional Object Representations For any values of parameters d and n we can generate an open uniform knot vector with integer values using the calculations  forOsj d we qr od d ford y en 63  n d forj n for values of j ranging from to x  d With this assignment the first d knots are assigned the value and the last knots have the value n  d   Open uniform B splines have characteristics that are very similar to B zier splines In fact when d    degree of the polynomial is open B splines re duce to B zier splines and all knot values are either or For example with a cubic open B spline d  and four control points the knot vector is  0 01  The polynomial curve jor an open B spline passes through the first and last con trol points Also the slope of the parametric curves at the first control point is parallel to the line connecting the first two control points And the parametric slope at the last control point is parallel to the line connecting the last two control points So geometric constraints for matching curve sections are the same as for B zier curves  As with B zier curves specifying multiple control points at the same coor dinate position pulls any B spline curve closer to that position Since open B splines start at the first control point and end at the last specified control point closed curves are genetated by specifying the first and last control points at the same position  Example Open Uniform Quadratic B Splines From conditions 63 with d  and n  five control points  we obtain the following eight values for the knot vector  INg Wy Ws Ma Ug Ws Meg Uz      The total range of u is divided into seven subintervals and each of the five blend ing functions B is defined over three subintervals starting at knot position Thus Bg is defined from up  to u3  By is defined from xu  to u   and B  is defined from u  to uw   Explicit polynomial expressions zre ob tained for the blending functions from recurrence relations 55 as Bo alt   wu  OSu l   QSu t By lu   uy l u O u By wy  Fu2 w t 4t DG w 1su  uy Qsu tu  lsu By u    u Bu   u By y u   u  2eu  shows the shape of the these five blending functions The local fea tures of B splines are again demonstrated Blending function By is nonzero only in the subinterval from to so the first control point influences the curve only in this interval Similarly function B  is zero outside the interval from to and the position of the last control point does not affect the shape of the beginning and middle parts of the curve  Matrix formulations for open B splines are not as conveniently generated as they are for periodic uniform B splines This is due to the multiplicity of knot values at the beginning and end of the knot vector  Nonuniform B Splines For this class of splines we can specify any values and intervals for the knot vec tor With nonuniform B splines we can choose multiple internal knot values and unequal spacing between the knot values Some examples are              Nonuniform B splines provide increased flexibility in controlling a curve shape With unequally spaced intervals in the knot vector we obtain different shapes for the blending functions in different intervals which can be used to ad just spline shapes By increasing knot multiplicity we produce subtle variations in curve shape and even introduce discontinuities Multiple knot values also re duce the continuity by for each repeat of a particular value  We obtain the blending functions for a nonuniform B spline using methods similar to those discussed for uniform and open B splines Given a set of  control points we set the degree of the polynomial and select the knot values Then using the recurrence relations we could either obtain the set of blending functions or evaluate curve positions directly for the display of the curve Graph ics packages often restrict the knot intervals to be either or to reduce compu tations A set of characteristic matrices then can be stored and used to compute Section Spline Curves and Surfaces By ylub By alu t b  O4   u pee deri ivi vida pedi dt y 1 3    Qo 2 a  b B lu B Au 8 6 O4 2 pat tad u wititiiia it Pe et Pe u 1 3 Q 18 3 c  d B  u 6 2 poirot irrirtririnr boos tut Lol y 1 5 fe   Open uniform B spline blending functions for n  andd  values along the spline curve without evaluating the recurrence relations for each curve point to be plotted  B Spline Suriaces Formulation of a B spline surface is similar to that for B zier splines We can ob tain a vector point function over a B spline surface using the Cartesian product of B spline blending functions in the form    A prototype helicopter designed and modeled by Daniel Langlois of SOFTIMAGE Inc  Montreal using 000 B spline surface patches The scene was then rendered using ray tracing bump mapping and reflection mapping  Courtesy of Silicon Graphics Inc  nl nz Pu    Pi epBeysty  Biy ag  64 ky  ko O where the vector values for p   specify positions of the   by nm   con trol points  B spline surfaces exhibit the same properties as those of their component B spline curves A surface can be constructed from selected values for parameters d and d  which determine the polynomial degrees to be used and from the specified knot vector  shows an object modeled with B spline sur faces   A generalization of B splines are the beta splines also referred to as splines that are formulated by imposing geometric continuity conditions on the first and second parametric derivatives The continuity parameters for beta splines are called B parameters  Beta Spline Continuity Canditions For a specified knot vector we can designate the spline sections to the left and right of a particular knot  with the position vectors P _ u and P u    Zero order continuity positional continuity  G  at u is obtained by requiring P_ u  PAu  65  First order continuity unit tangent continuity  G  is obtained by requiring tangent vectors to be proportional  Section 10 Beta Splines   Position vectors along curve sections to the left and right of knot u   Chapter Three Dimensional Oaject Representations BP uw  Pu   B  UP  Here parametric first derivatives are proportional and the unit tangent vectors are continuous across the knot  Second order continuity curvature vector continuily  G  is imposed with the condition BiP  ua  BoP Cu  Pu  67  where can be assigned any rea number and f   The curvature vector pro vides a measure of the amount of bending of the curve at position u  When  and f   beta splines reduce to B splines  Parameter is called the vias parameter since it controls the skewness of the curve For PB   the curve tends to flatten to the right in the direction of the unit tangent vector at the knots For    the curve tends to flatten to the left The effect of on the shape of the spline curve is shown in   Parameter  is called the tension parameter since it controls how tightly or loosely the spline fits the control graph As  increases the curve approaches the shape of the control graph as shown in   Cubic Periodic Beta Spline Matrix Representation Applying the beta spline boundary conditions to a cubic polynomial with a uni form knot vector we obtain the following matrix representation for a periodic beta spline   a2      I I I  I I  s5   t t I I     RB v    Effect of parameter on the shape of a beta spline curve        B By    Effect of parameter B on the shape of a beta spline curve  AB  BR BR PB  WAB Be B  Mg  i   B  2B  2B  3B   68   B  B 6B 2p By  B  B  where  B     4B   We obtain the B spline matrix M when  and    And we get the B spline with tension matrix Mz when A B a   A rational function is simply the ratio of two polynomials Thus a rational spline is the ratio of two spline functions For example a rational B spline curve can be described with the position vector  vv  Sg Pr By gu  Pu   69   ayB y glu  Ko where the p are a set of n  control point positions Parameters  are weight factors for the control points The greater the value of a particular  the closer the curve is pulled toward the control point p weighted by that parameter When all weight factors are set to the value we have the standard B spline curve since the denominator in Eq 69 is the sum of the blending functions  Rational splines have two important advantages compared to nonrational splines First they provide an exact representation for quadric curves conics  such as circles and ellipses Nonrational splines which are polynomials can only approximate conics This allows graphics packages to model all curve shapes with one representation rational splines without needing a library of curve functions to handle different design shapes Another advantage of rational splines is that they are invariant with respect to a perspective viewing transfor mation Section  This means that we can apply a perspective viewing trans formation to the control points of the rational curve and we will obtain the cor rect view of the curve Nonrational splines on the other hand are not invariant with respect to a perspective viewing transformation Typically graphics design packages use nonuniform knot vector representations for constructing rational B splines These splines are referred to as NURBs nonuniform rational B splines  Homogeneous coordinate representations are used for rational splines since the denominator can be treated as the homogeneous factor in a four dimen sional representation of the control points Thus a rational spline can be thought of as the projection of a four dimensional nonrational spline into three dimen sional space  Constructing a rational B spline representation is carried out wilh the same procedures for constructing a nonrational representation Given the set of contrel points the degree of the polynomial the weighting factors and the knot vector we apply the recurrence relations to obtain the blending functions  Section 11 Rationat Splines Chapter  Three Dimensional Object  Representations To plot conic sections with NURBs we use a quadratic spline function d  and three contro points We can do this with a B spline function defined with the open knot vector    which is the same as a quadratic B zier spline We then set the weighting func tions to the following values  g    r   and the rational B spline representation is P u   PoBosty   P  rp By  pzBz sw  71 Bo gu   r l  r B su By x u  We then obtain the various conics   with the following values for para meter r  r   hyperbola section r w   parabola section r w  ellipse section  r o  straight line segment  We can generate a one quarter arc of a unit circle in the first quadrant of the xy plane   by setting w  cos and by choosing the control points as nn hyperbola ir    Po  _ straight line parabola r  w   r   ellipse r     Conic sections generated with various values of the rational spline weighting factor w   Tone tno PP    A circular arc in the first quadrant p   of the xy plane  Po    pi  pp   Other sections of a unit circle can be obtained with different control point posi tions A complete circle can be generated using geometric transformation in the xy plane For example we can reflect the one quarter circular arc about the x and y axes to produce the circular arcs in the other three quadrants  In some CAD systems we construct a conic section by specifying three points on an arc A rational homogeneous coordinate spline representation is then determined by computing control point positions that would generate the selected conic type As an example a homogeneous representation for a unit cir cular arc in the first quadrant of the xy plane is x a t w yale  Qu zu   h ttu  Sometimes it is desirable to be able to switch from one spline representation to another For instance a B zier representation is the most convenient one for sub dividing a spline curve while a B spline representation offers greater design flex ibility So we might design a curve using B spline sections then we can convert to an equivalent Bezier representation to display the object using a recursive sub division procedure to locate coordinate positions along the curve  Suppose we have a spline description of an object that can be expressed with the following matrix product  Plu  U Myonne  Maggot 72  where Meaine i5 the matrix characterizing the spline representation and Mgeom  S the column matrix of geometric constraints for example control point coordi nates  To transform to a second representation with spline matrix Mpinez We need to determine the geometric constraint matrix Mogeomz that produces the same vector point function for the object That is  Charter  Three Dimenstonal Object Representations PQ  G Mopiyer M  geom P73  U  Maoigw2  Maeone  UM  geam   spline  M vom So ving for Mgcomz We have  m M yeom  Moohine2  Mypnet  Meco  Maa Moacomi  ith and the required transformation matrix that converts from the first spline repre sentation to the second is then calculated as Myo  My ne M Ue  sphnel A nonuniform B spline cannot be characterized with a general spline ma trix But we can rearrange the knot sequence to change the nonuniform B spline to a B zier representation Then the B zier matrix couid be converted to any other form  The following example calculates the transformation matrix for conversion from a periodic cubic B spline representation to a cubic B zier spline representa tion   31 030 3 i 3a Mee  of 0 1 10 76 _ Jo4a2  Mla And the the transformation matrix for converting from a cubic Bezier representa tion to a periodic cubic B spline representation is  2 6 Moe   2 moet NN  ON  2 6 aa  771 13 a DISPLAYING SPLINE CURVES AND SURFACES  To display a spline curve or surface we must determine coordinate positions on the curve or surface that project to pixel positions on the display device This means that we must evaluate the parametric polynomial spline functions in cer tain increments over the range of the functions There are several methods we can use to calculate positions over the range of a spline curve or surface  Horner s Rule The simplest method for evaluating a polynomial other than a brute force calcu lation of each term in succession is Horner s rule which performs the calculations by successive factoring This requires one multiplication and one addition at each step For a polynomial of degree n there are n steps  As an example suppose we have a cubic spline representation where coor dinate positions are expressed as x u  aye  bw  ou t dy 78  with similar expressions for the y and z coordinates For a particular value of pa rameter u we evaluate this polynomial in the following factored order  x u   Cau  bu  clu d  79  The calculation of each x value requires three multiplications and three additions so that the determination of each coordinate position x y z along a cubic spline curve requires nine multiplications and nine additions  Additional factoring tricks can be applied to reduce the number of compu tations required by Horner s method especially for higher order polynomials degree greater than  But repeated determination of coordinate positions over the range of a spline function can be computed much faster using forward differ ence calculations or spline subdivision methods  Forward Difference Calculations A fast method for evaluating polynomial functions is to generate successive val ues recursively by incrementing previously calculated values as for example  Xeey  Xe  AN  80  Thus once we know the increment and the value of x at any step we get the next value by adding the increment to the value at that step The increment Ax at each step is called the forward difference  f we divide the total range of u into subintervals of fixed size then two successive x positions occur at x  x u  and xy   x u  where yay  Uy  k   81  and us   Section 13 Displaying Spline Curves and Surfaces Chapter Three Dimensional Object Representations To illustrate the method suppose we have the linear spline representation x u  a u  b  Two successive x coordinate positions are represented as X  ayy  by 82 Xa  au   b  Subtracting the two equations we obtain the forward difference Ax   In this case the forward difference is a constant With higher order polynomials the forward difference is itself a polynomial function of parameter u with degree one less than the original polynomial For the cubic spline representation in Eq 78 two successive x coordinate positions have the polynomial representations xy ue  bug  cy  dy Ye  a uy  BY  bay   cy    d  83 The forward difference now evaluates to Ax  3a     854   69     84 which is a quadratic function of parameter u  Since Ax is a polynomial function of u we can use the same incremental procedure to obtain successive values of Ax  That is  Ax   Ax  Atx  85 where the second forward difference is the linear function A x  67m  53   86 Repeating this process once more we can write A xy   Atx  Bay  87 with the third forward ditference as the constant Bx   88 Equations 80 85 87 and 88 provide an incremental forward differ ence calculation of points along the cubic curve Starting at up  with a step size we obtain the initial values for the x coordinate and its tirst two forward differ  ences as Xo  a Axy  a8    89 A x  89  2b  Once these initia values have been computed the calculation for each successive x coordinate position requires only three additions We can apply forward difference methods to determine positions along spline curves of any degree n Each successive coordinate position x y  is evaluated with a series of 3n additions For surfaces the incremental calculations are applied to both parameter u and parameter v  Subdivision Methods Recursive spline subdivision procedures are used to repeatedly divide a given curve section in half increasing the number of control points at each step Subdi vision methods are useful for displaying approximation spline curves since we can continue the subdivision process until the control graph approximates the curve path Control point coordinates then can be plotted as curve positions An other application of subdivision is to generate more control points for shaping the curve Thus we could design a general curve shape with a few control points then we could apply a subdivision procedure to obtain additional control points With the added control points we can make fine adjustments to small sections of the curve  Spline subdivision is most easily applied to a B zier curve section because the curve passes through the first and last control points the range of parameter u is always between and and it is easy to determine when the control points are near enough to the curve path B zier subdivision can be applied to other spline representations with the following sequence of operations  Convert the spline representation in use to a Bezier representation  Apply the B zier subdivision algorithm  Convert the B zier representation back to the original spline representation   shows the first step in a recursive subdivision of a cubic B zier curve section Positions along the B zier curve are described with the parametric point function P u for O u At the first subdivision step we use the halfway point P to divide the original curve into two sections The first sec tion is then described with the point function P s  and the section is described with P t  where s  2u forOsus05 99 t 2u for Sus Each of the two new curve sections has the same number of control points as the original curve section Also the boundary conditions position and parametric    By Prs Pro Pr as Pp ty Rg Po Py Pro Pr Before After Subdivision Subdivision  Subdividing a cubic B zier curve section into two sections each with four control points  Section 13  Displaying Spline Curves and Surfaces Chapter slope at the two ends of each new curve section must match the position and Three Dimensiona Object slope values for the original curve P u  This gives us four conditions for each Representations curve section that we can use to determine the control point positions For the first half of the curve the four new control points are Pio  Po Pi  Po  py  91 Piz  Po  2p  po Pis  Po  3p  3p2  ps  And for the second half of the curve we obtain the four control points Pro  Po  3p  3p2  ps P21  F Pi  2p  ps  92  Piz  P2  py P23  Ps An efficient order for computing the new control points can be set up with only add and shift division by operations as Pio  Po Pi  Po  Py T 5p  p  Pi2  Pin  T P2a  Ps 93  Po   py  ps Po  r  Poa  Pan  4p  pr  Pis  Pro These steps can be repeated any number of times depenaing on whether we are subdividing the curve to gain more control points or whether we are try ing to locate approximate curve positions When we are subdividing to obtain a set of display points we can terminate the subdivision procedure when the curve sections are small enough One way to determine this is to check the distances between adjacent pairs of control points for each section If these distances are sufficiently small we can stop subdividing Or we could stop subdividing when the set of control points for each section is nearly along a straight line path  Subdivision methods can be applied to B zier curves of any degree For a B zier polynomial of degree n  the control points for each half of the curve at the first subdivision step are r  Pur  Ck Dp k   n 94   Pak  gt Cn kin Dp  where C k  and C n  k n  i are the binomial coefficients  We can apply subdivision methods directly to nonuniform B splines by adding values to the knot vector But in general these methods are not as effi cient as B zier subdivision  14  SWEEP REPRESENTATIONS  Solid modeling packages often provide a number of construction techniques Sweep representations are useful for constructing three dimensional objects that possess translational rotational or other symmetries We can represent such ob jects by specifying a two dimensional shape and a sweep that moves the shape through a region of space A set of two dimensional primitives such as circles and rectangles can be provided for sweep representations as menu options Other methods for obtaining two dimensional figures include closed spline curve constructions and cross sectional slices of solid objects   illustrates a translational sweep The periodic spline curve in  a defines the object cross section We then perform a translational  Pig    Py t   I     I     Pus Pru v u pp     p  a  b   Constructing a solid with a translational sweep Translating the control paints of the periodic spline curve in a generates the solid shown in b  whose surface can be described with point function P u v  Section 14 Sweep Representations  Auda of Rotation Pe oPa Plu Po  p  a   Constructing a solid with a rotatianal sweep Rotating the control points of the periodic spline curve in a about the given rotation axis generates the solid shown in b  whose surface can be described with point function P u v  sweep by moving the control points py through p a set distance along a straight line path perpendicular to the plane of the cross section At intervals along this path we replicate the cross sectional shape and draw a set of connecting lines in the direction of the sweep to obtain the wireframe representation shown in Fig 53 b  An example of object design using a rotational sweep is given in  This time the periodic spline cross section is rotated about an axis of rotation specified in the plane of the cross section to produce the wireframe representa tion shown in  b  Any axis can be chosen for a rotational sweep If we use a rotation axis perpendicular to the plane of the spline cross section in Fig 54 a  we generate a two dimensional shape But if the cross section shown in this figure has depth then we are using one three dimensional object to generate another  In general we can specify sweep constrictions using any path For rota tional sweeps we can move along a circular path through any angular disfance from to  For noncircular paths we can specify the curve function describ ing the path and the distance of travel along the path In addition we can vary the shape or size of the cross section along the sweep path Or we could vary the orientation of the cross section relative to the sweep path as we move the shape through a region of space  Another technique for solid modeling is to combine the volumes occupied by overlapping three dimensional objects using set operations This modeling method called constructive solid geometry CSG  creates a new volume by ap plying the union intersectian or difference operation to two specified volumes Figures 55 and 56 show examples for forming new shapes using the set operations In  a  a block and pyramid are placed adjacent to each other Specifying the union operation we obtain the combined object shown in  b   a shows a block and a cylinder with overlapping vol umes Using the intersection operation we obtain the resulting solid in Fig  b  With a difference operation we can get the solid shown in  c  A CSG application starts with an initial set of three dimensional objects primitives  such as blocks pyramids cylinders cones spheres and closed spline surfaces The primitives can be provided by the CSG package as menu se lections or the primitives themselves could be formed using sweep methods spline constructions or other modeling procedures To create a new three dimen sional shape using CSG methods we first select two primitives and drag them into position in some region of space Then we select an operation union inter section or difference for combining the volumes of the two primitives Now we have a new object in addition to the primitives that We can use to form other ob jects We continue to construct new shapes using combinations of primitives and the objects created at each step until we have the final shape An object designed with this procedure is represented with a binary tree An example tree represen tation for a CSG object is given in   Ray casting methods are commonly used to implement constructive solid geometry operations when objects are described with boundary representations We apply ray casting by constructing composite objects in world coordinates with the xy plane corresponding to the pixel plane of a video monitor This plane is then referred to as the firing plane since we fire a ray from each pixel posi tion through the objects that are to be combined   We then determine surface intersections along each ray path and sort the intersection points accord ing to the distance from the firing plane The surface limits for the composite ob ject are then determined by the specified set operation An example of the ray casting determination of surface limits for a CSG object is given in  which shows yz cross sections of two primitives and the path of a pixel ray per pendicular to the firing plane For the union operation the new volume is the combined interior regions occupied by either or both primitives For the intersec tion operation the new volume is the interior region common to both primitives   a  b  c    a Two overlapping objects  b A wedge shaped CSG object formed with the intersection operation  c A CSG object formed with a difference operation by subtracting the overlapping volume of the cylinder from the block volume  Section 15 Constructive Sofid Geometry Methods si   Combining two objects  a with a union operation produces a single composite solid object b   Firing Plane   implementing CSG operations using ray casting  Firing Plane   y  A  x ae  Determining object volume along a ray path for a small area A  on the firing plane       CSG Object  A CSG tree representation for an object     wY obj obj aL D Pixe Ray Cperation  Surface Limits A Union A D NY Intersection c Firing Difference B D Plane  obj  obj  Zz  fa  b   Determining surface limits along a pixel ray  And a difference operation subtracts the volume of one primitive from the other  Each primitive can be defined in its own local modeling coordinates Then a composite shape can be formed by specifying the modeling transforma tion matrices that would place two primitives in an overlapping position in world coordinates The inverse of these modeling matrices can then be used to transform the pixel rays to modeling coordinates where the surface intersection calculations are carried out for the individual primitives Then surface intersec tions for the two objects are sorted and used to determine the composite object limits according to the specified set operation This procedure is repeated for each pair of objects that are to be combined in the CSG tree for a particular object  Once a CSG object has been designed ray casting is used to determine physica properties such as volume and mass To determine the volume of the object we can divide the firing plane into any number of small squares as shown in  We can then approximate the volume V  of the object for a crass sectional slice with area A along the path of a ray from the square at position i pas V  Ay dz  95  where Az is the depth of the object along the ray from position    f the object has internal holes Az is the sum of the distances between pairs of intersection points along the ray The total volume of the CSG object is then calculated as v S Vv  96  Given the density function p x y  for the object we can approximate the mass along the ray from position i J as m  A  px Yy 2d2 97  where the one dimensional integral can often be approximated without actually carrying out the integration depending on the form of the density function The total mass of the CSG object is then approximated as m M   98   EM    Other physical properties such as center of mass and moment of inertia can be obtained with similar calculations We can improve the approximate calculations for the values of the physical properties by taking finer subdivisions in the firing plane  If object shapes are represeined with octrees we can implement the set op erations in CSG procedures by scanning the tree structure describing the contents of spatial octants This procedure described in the following section searches the octants and suboctants of a unit cube to locate the regions occupied by the two objects that are to be combined  Hierarchical tree structures called octrees are used to represent solid objects in some graphics systems Medical imaging and other applications that require dis plays of object cross sections commonly use octree representations The tree structure is organized so that each node corresponds to a region of three dimen sional space This representation for solids takes advantage of spatial coherence to reduce storage requirements for three dimensional objects It also provides a convenient representation for storing information about object interiors  The octree encoding procedure for a three dimensional space is an exten sion of an encoding scheme for two dimensional space called quadtree encod ing Quadtrees are generated by successively dividing a two dimensional region usually a square into quadrants Each node in the quadtree has four data ele ments one for each of the quadrants in the region   If all pixels within a quadrant have the same color a homogeneous quadrant  the corresponding data element in the node stores that color In addition a flag is set in the data ele ment to indicate that the quadrant is homogeneous Suppose all pixels in quad rant of  are found to be red The color code for red is then placed in data element of the node Otherwise the quadrant is said to be heterogeneous and that quadrant is itself divided into quadrants   The corresponding data element in the node now flags the quadrant as heterogeneous and stores the pointer to the next node in the quadtree  An algorithm for generating a quadtree tests pixel intensity values and sets up the quadtree nodes accordingly If each quadrant in the original space has a Section 16 Octrees Chapter  Three Dimensional Object Representations        Quadrant Quadrant o  Quadran Quadrant _ Data Elements in the Representative Quadtree Node Region of a Two Dimensional Space   Region of a two dimensional space divided intu numbered quadrants and the associated quadtree node with four data elements  single color specification the quadtree has only one node For a heterogeneous region of space the successive subdivisions into quadrants continues until all quadrants are homogeneous  shows a quadtree representation for a region containing one area with a solid color that is different from the uniform color specified for all other areas in the region  Quadtree encodings provide considerable savings in storage when large color areas exist in a region of space since each single color area can be repre sented with one node For an area containing by pixels a quadtree repre sentation contains at most levels Each node in the quadtree has at most four immediate descendants An octree encoding scheme divides regions af three dimensional space usually cubes into octants and stores eight data elements in each node of the tree   Individual elements of a three dimensional space are called vol ume elements or voxels When all voxels in an octant are of the same type this            t 1  t 3 Quadtree Representation Region ot a Two Dimensional Space   Region of a two dimensional space with two levels of quadrant divisions and the associated quadtree representation           3           Quadtree representation for a region containing one foreground color pixel on a solid background  type value is stored in the corresponding data element of the node Empty re gions of space are represented by voxel type void  Any heterogeneous octant is subdivided into octants and the corresponding data element in the node points to the next node in the octree Procedures for generating octrees are similar to those for quadtrees Voxels in each octant are tested and octant subdivisions con tinue until the region of space contains only homogeneous octants Each node ir the octree can now have from zero to eight immediate descendants  Algorithms for generating octrees can be strictured to accept definitions of objects in any form such as a polygon mesh curved surface patches or solid geometry constructions Using the minimum and maximum coordinate values of the object we can define a box parallelepiped around the object This region of three dimensional space containing the object is then tested octant by octant to generate the octree representation  Once an octree representation has been established for a solid object vari ous manipulation routines can be applied to the solid An algorithm for perform ing set operations can be applied to two octree representations for the same re gion of space For a union operation a new octree is constructed with the combined regions for each of the input objects Similarly intersection or differ  are   SSS HE DHBBOEoe  Data Elements in the Representative Octree Node Aegion of a Three Dimensional Space   Region of a three dimensional space divided into numbered octants and the associated octree node with eight data elements  Section 16 Octrees w nN Chapter Three Dimensional Object Representations ence Operations are perforined by looking for regions of overlap in the two oc trees The new octree is then formed by either storing the octants where the two objects overlap or the region occupied by one object but not the other  Three dimensonal octree rotations are accomplished by applying the trans formations to the occupied octants Visible surface identification is carried out by searching the octants from front to back The first object detected is visible so that information can be transferred to a quadtree representation for display   This representation scheme is similar to octree encoding except we now divide space into two partitions instead of eight at each step With a binary space parti tioning BSP tree we subdivide a scene into two sections at each step with a plane that can be at any position and orientation In an octree encoding the scene is subdivided at each step with three mutually perpendicular planes aligned with the Cartesian coordinate planes  For adaptive subdivision of space BSP trees can provide a more efficient partitioning since we can position and orient the cutting planes to suit the spatial distribution of the objects This can reduce the depth of the tree representation for a scene compared to an octree and thus reduce the time to search the tree In ad dition BSP trees are useful for identifying visible surfaces and for space parti tioning in ray tracing algorithms  All the object representations we have considered in the previous sections used Euclidean geometry methods that is object shapes were described with equa tions These methods are adequate for describing manufactured objects those that have smooth surfaces and regular shapes But natural objects such as moun tains and clouds have irregular or fragmented features and Euclidean methods do not realistically model these objects Natural objects can be realistically de scribed with fractal geometry methods where procedures rather than equations are used to model objects As we might expect procedurally defined objects have characteristics quite different from objects described with equations Fractal geometry representations for objects are commonly applied in many fields to de scribe and explain the features of natural phenomena In computer graphics we use fractal methods to generate displays of natural objects and visualizations of various mathematical and physical systems  A fractal object has two basic characteristics infinite detail at every point and a certain self similarity between the object parts and the overall features of the object The self similarity properties of an object can take different forms de pending on the choice of fractal representation We describe a fractal object witha procedure that specifies a repeated operation for producing the detail in the ob ject subparts Natural objects are represented with procedures that theoretically repeat an infinite number of times Graphics displays of natural objects are of course generated witha finite number af steps  If we zoom in on a continuous Euclidean shape no matter how campli cated we can eventually get the zoomed in view to smoath out But if we zoom oN  Distant Mountain Closer View Do Closer Yat   The ragged appearance of a mountain outline at different levels of magnification  in on a fractal object we continue to see as much detail in the magnification as we did in the original view A mountain outlined against the sky continues to have the same jagged shape as we view it from a closer and closer position Fig 65  As we near the mountain the smaller detail in the individual ledges and boulders becomes apparent Moving even closer we see the outlines of rocks then stones and then grains of sand At each step the outline reveals more twists and turns If we took the grains of sand and put them under a microscope we would again see the same detail repeated down through the molecular level Similar shapes describe coastlines and the edges of plants and clouds  Zooming in on a graphics display of a fractal object is obtained by selecting a smaller window and repeating the fractal procedures to generate the detail in the new window A consequence of the infinite detail of a fractal object is that i has no definite size As we consider more and more detail the size of an objec tends to infinity but the coordinate extents of the object remain bound within a finite region of space  We can describe the amount of variation in the object detail with a number called the fractal dimension Unlike the Euclidean dimension this number is not necessarily an integer The fractal dimension of an object is sometimes referred to as the fractional dimension which is the basis for the name fractal  Fractal methods have proven useful for modeling a very wide variety of natural phenomena In graphics applications fractal representations are used to model terrain clouds water trees and other plants feathers fur and various surface textures and just to make pretty patterns In other disciplines fractal pat terns have been found in the distribution of stars river islands and moon craters in rain fields in stock market variations in music in traffic flow in urban prop erty utilization and in the boundaries of convergence regions for numerical analysis techniques  Fractal Generation Procedures A fractal object is generated by repeatedly applying a specified transformation function to points within a region of space If Py  xg Yo  i8 a Selected initial point each iteration of a transformation function F generates successive levels of detail with the calculations P  FCP  P  F P  P F P     99  Section 18 Fractal Geometry Methods  Chapter Three Dimensional Obyect Representations in general the transformation function can be applied to a specified point set or we could apply the transformation function to an initial set of primitives such as straight lines curves color areas surfaces and solid objects Also we can use either deterministic or random generation procedures at each iteration The transformation function may be defined in terms of geometric transformations scaling translation rotation  or it can be set up with nonlinear coordinate trans formations and decision parameters  Although fractal objects by definition contain infinite detail we apply the transformation function a finite number cf times Therefore the objects we dis play actually have finite dimensions A procedural representation approaches a true fractal as the number of transformations is increased to produce more and more detail The amount of detail included in the final graphical display of an ob ject depends on the number of iterations performed and the resolution of the dis play system We cannot display detail variations that are smaller than the size of a pixel To see more of the object detail we zoom in on selected sections and re peat the transformation function iterations  Classification of Fractals Self similar fractals have parts that are scaled down versions of the entire object Starting with an initial shape we construct the object subparts by apply a scaling parameter s to the overall shape We can use the same scaling factor s for all sub parts or we can use different scaling factors for different scaled down parts of the object If we also apply random variations to the scaled down subparts the fractal is said to be statistically self similar The parts then have the same statistical properties Statistically self similar fractals are commonly used to mode trees shrubs and other plants  Self affine fractals have parts that are formed with different scaling para meters  Sy   different coordinate directions And we can also include ran dom variations to obtain statistically self affine fractals Terrain water and clouds are typically modeled with statistically self affine fractal construction methods  Invariant fractal sets are formed with nonlinear transformations This class of fractals includes selj squaring fractals such as the Mandelbrot set which are formed with squaring functions in complex space and_ self inverse fractals formed with inversion procedures  Fractal Dimension The detail variation in a fractal object can be described with a number D called the fractal dimension which is a measure of the roughness or fragmentation of the object More jagged looking objects have larger fractal dimensions We can set up some iterative procedures to generate fractal objects using a given value for the fractal dimension D With other procedures we may be able to determine the fractal dimension from the properties of the constructed object although in gen eral the fractal dimension is difficult to calculate  An expression for the fractal dimension of a self similar fractal constructed with a single scalar factor s is obtained by analogy with the subdivision of a Eu clidean object  shows the relationships between the scaling factor s and the number of subparts n for subdivision of a unit straight line segment a square and a cube With s  the unit line segment  a  is divided into two equal length subparts Similarly the square in  b is divided into four equal area subparts and the cube  c  is divided into eight equal volume subparts For each of these objects the relationship between the             k  HI    j    De  n  e s ns  a LA Ales A     f oO aga Agen b on b fern Vv  O  s Vy n ns3 i  Subdividing objects with Euclidean dimensions a Dp   b Dy  and c De  using scaling factor s   number of subparts and the scaling factor is m s F  In analogy with Euclid ean objects the fractal dimension D for self similar objects can be obtained from ns   709  Solving this expression for D the fractal similarity dimension we have _ Inn In s   udy For a self similar fractal constructed with different scaling factors for the different parts the fractal similarity dimension is obtained from the implicit relationship qs  1012  on where is the scaling factor for subpart number k  In  we considered subdivision of simple shapes straight line rec tangle box  If we have more complicated shapes including curved lines and ob jects with nonplanar surfaces determining the structure and properties of the subparts is more difficult For general abject shapes we can use topological cover  Section 18 Fractal Geometry Methods Chapter Three Dimensional Object Representations   Box covering of an irregularly shaped object  ing methods that approximate object subparts with simple shapes A subdivided curve for example can be approximated with straight line sections and a subdi vided polygon could be approximated with small squares or rectangles Other covering shapes such as circles spheres and cylinders can also be used to ap proximate the features of an object divided into a number of smaller parts Cov ering methods are commonly used in mathematics to determine geometric prop erties such as length area or volume of an object by summing the properties of a set of smaller covering objects We can also use covering methods to determine the fractal dimension D of some objects  Topological covering concepts were originally used to extend the meaning of geometric properties to nonstandard shapes An extension of covering meth ods using circles or spheres led to the notion of a Hausdorff Besicovitch dimension or fractional dimension The Hausdorff Besicovitch dimension can be used as the fractal dimension of some objects but in general it is difficult to evaluate More commonly the fractal dimension of an object is estimated with box covering meth ods using rectangles or parallelepipeds  illustrates the notion of a box covering Here the area inside the large irregular boundary can be approxi mated by the sum of the areas of the small covering rectangles  We apply box covering methods by first determining the coordinate extents of an object then we subdivide the object into a number of small boxes using the given scaling factors The number of boxes that it takes to cover an object is called the box dimension and n is related to the fractal dimension D of the object For statistically self similar objects with a single scaling factor s we can cover the object with squares or cubes We then count the number of covering boxes and use Eq 101 to estimate the fractal dimension For self affine objects we cover the object with rectangular boxes since different directions are scaled differently In this case the number of boxes is used with the affine transformation para meters to estimate the fractal dimension  The fractal dimension of an object is always greater than the corresponding Euclidean dimension or topological dimension  which is simply the least num ber of parameters needed to specify the object A Euclidean curve is one dimen sional a Euclidean surface is two dimensional and a Euclidean solid is three di mensional  For a fractal curve that lies completely within a two dimensional plane the fractal dimension D is greater than the Euclidean dimension of a curve  The closer D is to the smoother the fractal curve If D  we have a Peano curve that is the curve completely fills a finite region of two dimensional space For D the curve self intersects and the area could be covered an infinite number of times Fractal curves can be used to model natural object boundaries such as shorelines  Spatial fractal curves those that do not lie completely within a single plane also have fractal dimension D greater than but D can be greater than without self intersecting A curve that fills a volume of space has dimension D  and a self intersecting space curve has fractal dimension D   Fractal surfaces typically have a dimension within the range  D  If D  the surface fills a volume of space And if D  there is an overlapping coverage of the volume Terrain clouds and water are typically modeled with fractal surfaces  The dimension of a fractal solid is usually in the range  D  Again if D  we have a self overlapping object Fractal solids can be used for example to model cloud properties such as water vapor density or temperature within a region of space Sectian 18 A   Fractal Geomerry Methods  fo    _   Generator NY Initiator   Initiator and generator for the Koch curve  Geometric Construction of Deterministic Self Similar Fractals To geometrically construct a deterministic nonrandom self similar fractal we start with a given geometric shape called the initiatar Subparts of the initiator are then replaced with a pattern called the generator  As an example if we use the initiator and generator shown in  we can construct the snowflake pattern or Koch curve shown in  Each straight line segment in the initiator is replaced with four equal length line seg ments at each step The scaling factor is so the fractal dimension is D  In In  2619 Also the length of each line segment in the initiator increases by AYE  a  or e  di  First three iterations in the generation of the Koch curve  A snowflake filling Peano curve   Segment Length  Segment Length   Segment Length     a     va yp  Length  Length  Length   Length of each side of the Koch curve increases by a factor of at each step while the line segment lengths are reduced by a factor of Segment Segment Segment Length  f Length  Length  D 129 D  500 D  a  b  c  Vigure 717 Self similar curve constructions and associated fractal dimensions  oe tnd OF ee o  e e o oe e           Segment Segment Segment Length  Length  Length  O  631 O  333 O 333  Generators wit multiple disjoint parts  a factor of at each step so that the length of the fractal curve tends to infinity as more detail is added tu the curve   Examples of other self similar fractal curve constructions are shown in  These examples illustrate the more jagged appearance of objects with higher fractal dimensions  We can also use generators with multiple disjoint components Some exam ples of compound generators are shown in  Using random variations with compound generators we can model various natural objects that have com pound parts such as island distributions along coastlines   shows an example of a self similar construction using multi ple scaling factors The fractal dimension of this object is determined from Eq 102  As an example of self similar fractal construction tor a surface we scale the regular tetrahedron shown in  by a factor of then place the scaled Section 18 Fractal Geometry Methods Front Face Scated Copy of Tetrahedron  a  b    Scaling the tetrahedron in a by a factor of and positioning the scaled version on one face of the original tetrahedron produces the fractal surface b  object on each of the original four surfaces of the tetrahedron Each face of the original tetrahedron is converted to smaller faces and the original face area is increased by a factor of The fractal dimension of this surface is In6    D in  which indicates a fairly fragmented surface  Another way to create self similar fractal objects is to punch holes in a given initiator instead of adding more surface area  shows some examples of fractal objects created in this way  Geometric Construction of Statistically Self Similar Fractals One way we can introduce some randomness into the geometric construction of a self similar fractal is to choose a generator randomly at each step from a set of predefined shapes Another way to generate random self similar objects is to compute coordinate displacements randomly For example in  we cre ate a random snowflake pattern by selecting a random midpoint displacement distance at each step   Self similar tiree dimensional fractals formed with generators that subtract subparts from an initiator  Courtesy of John C Hart Washington State University   A modified snowflake pattern using random midpoint displacement  Displays of trees and other plants can be constructed with similar geometric methods  shows a self similar construction for a fern In a of this figure each branch is a scaled version of the total object and b shows a fully rendered fern with a twist applied to each branch Another example of this method is shown in  Here random scaling parameters and branching directions are used to model the vein patterns in a leaf  Once a set of fractal objects has been created we can model a scene by plac ing several transformed instances of the fractal objects together  il lustrates instancing with a simple fractal tree In  a fractal forest is dis  layed To model the gnarled and contorted shapes of some trees we can apply twisting functions as well as scaling to create the random self similar branches   b   Self similar constructions for a fern  Courtesy of Peter Oppenheimer Computer ha Graphics Lab New York Institute of Technology   Fighre 78 Random self similar construction of vein branching in a fall leaf Boundary of the leaf is the limit of the vein growth  Courtesy of Peter Oppenheimer Computer Graphics Lab New York Institute of Technology     Modeling a scene using multiple object instancing Fractal leaves are attached to a tree and several instances of the tree are used to form a grove The grass is modeled with multiple instances of green cones  Courtesy of John C Hart Washington State University  This technique is illustrated in  Starting with the tapered cylinder on the left of this figure we can apply transformations to produce in succession from left to right a spiral a helix and a random twisting pattern A tree modeled with random twists is shown in  The tree bark in this display is mod eled using bump mapping and fractal Brownian variations on the bump patterns as discussed in the following section  A fractal forest created with multiple instances of leaves pine needles grass and tree bark  Courtesy of John C Hart Washington State University   Modeling tree branches with spiral helical and random twists  Courtesy of Peter Oppentteimer Computer Graphics Lab New York Institute of Technology   An example of Brownian motion random walk in the xy plane    Tree branches modeled with random squiggles  Courtesy of Peter Oppenheimer Computer Graphics Lab New York Institute of Technology  Affine Fractal Construction Methods We can obtain highly realistic representations for terrain and other natural objects using affine fractal methods that model object features as fractional Brownian mo tion This is an extension of standard Brownian motion a form of random walk  that describes the erratic zigzag movement of particles in a gas or otner fluid  illustrates a random walk path in the xy plane Starting from a given position we generate a straight line segment in a random direction and with a random length We then move to the endpoint of the first line segment and repeat the process This procedure is repeated for any number of line seg ments and we can calculate the statistical properties of the line path over any time interval  Fractional Brownian motion is obtained by adding an additional parameter to the statistical distribution describing Brownian motion This addi tional parameter sets the fractal dimension for the motion path  A single fractional Brownian path can be used to model a fractal curve With a two dimensional array of random fractional Brownian elevations over a    A Brownian motion planet observed from the surface of a fractional Brownian motion pianet with added craters in the foreground  Courtesy of R V Voss and B B  Mandelbrot adapted from The Fractal Geometry of Nature by Benoit B Mandelbrot New York W H Freeman and Co   ground plane grid we can model the surface of a mountain by connecting the el evations to form a set of polygon patches If random elevations are generated on the surface of a sphere we can model the mountains valleys and oceans of a planet in  Brownian motion was used to create the elevation variations on the planet surface The elevations were then color coded so that lowest eleva tions were painted blue the oceans and the highest elevations white snow on the mountains  Fractional Brownian motion was used to create the terrain fea tures in the foreground Craters were created with random diameters and ran dom positions using affine fractal prc cedures that closely describe the distribu tion of observed craters river islands rain patterns and other similar systems of objects  By adjusting the fractal dimension in the fractional Brownian motion calcu lations we can vary the ruggedness of terrain features Values for the fractal di mension in the neighborhood of D  15 produce realistic mountain features while higher values close to can be used to create unusual looking extrater restrial landscapes We can also scale the calculated elevations to deepen the val leys and to increase the height of mountain peaks Some examples of terrain fea tures that can be modeled with fractal procedures are given in  A scene modeled with fractal clouds over a fractal mountain is shown in   Random Midpoint Displacement Methods Fractional Brownian motion calculations are time consuming because the eleva tion coordinates of the terrain above a ground plane are calculated with Fourier series which are sums of cosine and sine terms Fast Fourier transform FFT  methods are typically used but it is still a slow process to generate fractal moun   tain scenes Therefore faster random midpoint displacement methods similar to the random displacement methods used in geometric constructions have been developed to approximate fractional Brownian motion representations for terrain and other natural phenomena These methods were originally used to generate animation frames for science fiction films involving unusual terrain and planet features Midpoint displacement methods are now commonly used in many ap plications including television advertising animations  Although random midpoint displacement methods are faster than frac tional Brownian motion calculations they produce less realistic looking terrain features  illustrates the midpoint displacement method for generat ing a random walk path in the xy plane Starting with a straight line segment we calculate a displaced y value for the midposition of the line as the average of the endpoint y values plus a random offset  Yet   lua  y b  r 103  To approximate fractional Brownian motion we choose a value for r from a Gaussian distribution with a mean of and a variance proportional to lb al  where H  D and D  is the fractal dimension Another way to obtain a tandom offset is to take r  sr b  a  with parameter s as a selected surface toughness factor and r asa Gaussian random value with mean and variance Table lookups can be used to obtain the Gaussian values The process is then repeated by calculating a displaced y value for the midposition of each half of the subdivided line And we continue the subdivision until the subdivided line sec tions are less than some preset value At each step the value of the random vari  Section 18  Fractal Geometry Methods  cj   Variations in terrain features modeled with fractional Brownian motion  Courtesy of a R V Voss and B B Mandelbrot adapted from The Fractal Geometry of Nature by Benoit B Mandelbrot New York W H Freeman and Co   and b and c Ken Musgrave and Benoit B Mandelbrot Mathematics and Computer Science Yale University    A scene modeled with fractal clouds and mountains  Courtesy of Ken Musgrave and Benoit B Mandelbrot Mathematics and Computer Science Yale University   y  y b a  vib yta yta   Ymid a b x a a b b x  Random midpoint displacement of a straight line segment  able r decreases since it is proportional to the width b  a of the line section to be subdivided  shows a fractal curve obtained with this method Terrain features are generated by applying the random midpoint displace ment procedures to a rectangular ground plane   We begin by assign ing an elevation z value to each of the four corners a b c and d in  of the ground plane Then we divide the ground plane at the midpoint of each edge to obtain the five new grid positions e f gh and m Elevations at midpositions      y  a b a  b i   ground m plane hpoenne wee f  f  d cy d a cy a tb   A rectangular ground plane a is subdivided into four equal grid sections b for the first step in a random midpoint displacement procedure to calculate terrain elevations  Section 18 Fractal Geometry Methods   x  a     b   A random walk path generated from a straight line segment with four iterations of the random midpoint displacement procedure  Chapter Three Dimensiona Object Representations      Eight surface patches formed over a ground plane at the first step of a random midpoint displacement procedure for generating terrain features  e f g and h of the ground plane edges can be calculated as the average eleva tion of the nearest two vertices plus a random offset For example elevation z at midposition e is cal ulated using vertices a and b and the elevation at midposi tion f is calculated using vertices b and c  z        yp   Random values r and can be obtained from a Gaussian distribution with mean and variance proportional to the grid separation raised to the 2H power with H   D and where D  is the fractal dimension for the surface We could also calculate random offsets as the product of a surface roughness factor times the grid separation times a table lookup value for a Gaussian value with mean and variance The elevation z  of the ground plane midposition m can be calculated using positions e and g or positions f and h Alternatively we could calculate z  using the assigned elevations of the four ground plane corners  Zm   By   2p  This process is repeated for each of the four new grid sections at each step until the grid separation becomes smaller than a selected value  Triangular surface patches can be formed as the elevations are generated  shows the eight surface patches formed at the first subdivision step At each level of recursion the triangles are successively subdivided into smaller planar patches When the subdivision process is completed the patches are ren dered according to the position of the light sources the values for other illumina tion parameters and the selected color and surface texture for the terrain  The random midpoint displacement method can be applied to generate other components of a scene besides the terrain For instance we could use the same methods to obtain surface features for water waves or cloud patterns above a ground plane  Controlling Terrain Topography One way to control the placement of peaks and valleys in a fractal terrain scene modeled with a midpoint displacement method is to constrain the calculated ele vations to certain intervals over different regions of the ground plane We can ac complish this by setting up a set of control surfaces over the ground plane as illus trated in  Then we calculate a random elevation at each midpoint grid position on the ground plane that depends on the difference between the control elevation and the average elevation calculated for that position This procedure constrains elevations to be within a preset interval about the control surface ele vations   Control surfaces over a ground plane Control surfaces can be used to model existing terrain features in the Rocky Mountains or some other region by constructing the plane facets using the ele vations in a contour plot for a particular region Or we could set the elevations for the vertices of the control polygons to design our own terrain features Also control surfaces can have any shape Planes are easiest to deal with but we could use spherical surfaces or other curve shaj We use the random midpoint displacement method to calculate grid eleva tions but now we select random values from a Gaussian distribution where the mean  and standard deviation o are functions of the control elevations One way to set the values for  and o is to make them both proportional to the differ ence between the calculated average elevation and the predefined control eleva tion at each grid position For example for grid position e in  we set the mean and standard deviation as Me  2p    oe  S land where zc is the control elevation for ground plane position e and s 1isa preset scaling factor Small values for s say s  produce tighter conformity to the terrain envelope and larger values of s allow greater fluctuations in terrain height  To determine the values of the control elevations over a plane control sur face we first calculate the plane parameters A B C and D For any ground plane position x y  the elevation in the plane containing that control polygon is then calculated as zc   Ax By D C Incremental methods can then be used to calculate control elevations over posi tions in the ground plane grid To efficiently carry out these calculations we first subdivide the ground plane into a mesh of xy positions as shown in  Then each polygon control surface is projected onto the ground plane We can then determine which grid positions are within the projection of the control poly gon using procedures similar to those in scan line area filling That is for each y scan line in the ground plane mesh that crosses the polygon edges we calcu late scan line intersections and determine which grid positions are in the interior of the projection of the control polygon Calculations for the control elevations at those grid positions can then be performed incrementally as inj   AXCA C     AYB C  104   Projection of a triangular control x surface onto the ground plane grid  Section 18 Fractal Geometry Methods  Representations    A composite scene modeled with a random midpoint displacement method and planar control surfaces over a ground plane Surface features for the terrain water and clouds were modeled and rendered separately then combined to form the composite picture  Courtesy of Eng Kiat Koh Information Technology Institute Republic of Singapore  with Ax and Ay as the grid spacing in the x and y directions This procedure is particularly fast when parallel vector methods are applied to process the control plane grid positions   shows a scene constructed using control plar es to structure the surfaces for the terrain water and clouds above a ground plane Surface render ing algorithms were then applied to smooth out the polygon edges and to pro vide the appropriate surface colors  Self Squaring Fractals Another method for generating fractal objects is to repeatedly apply a transfor mation function to points in complex space In two dimensions a complex num ber can be represented as z  x  iy where x and y are real numbers and i   In three dimensional and four dimensional space points are represented with quatemnions A complex squaring function f z is one that involves the calcu lation of z  and we can use some self squaring functions to generate fractal shapes  P Depending on the initial position selected for the iteration repeated appli cation of a self squaring function will produce one of three possible results Fig 94   The transformed position can diverge to infinity   The transformed position can converge to a finite limit point called an at tractor   The transformed position remains on the boundary of some object  As an example the nonfractal squaring operation f z  z in the complex plane transforms points according to their relation to the unit circle   Any   Possible outcomes of a self squaring transformation f z in the complex plane depending on the position of the selected initial position   e attractor  Julia Set point z whose magnitude  zi is greater than is transformed through a sequence of positions that tend to infinity A point with z  is transformed toward the coordinate origin Points on the circle  z   remain on the circle For some functions the boundary between those points that move toward infinity and those that tend toward a finite limit is a fractal The boundary of the fracta object is called the Julia set  In general we can locate the fractal boundaries by testing the behavior of selected positions If a selected position either diverges to infinity or converges to an attractor point we can try another nearby position We repeat this process until we eventually locate a position on the fractal boundary Then iteration of the squaring transformation generates the fractal shape For simple transforma tions in the complex plane a quicker method for locating positions on the fractal curve is to use the inverse of the transformation function An initial point chosen on the inside or outside of the curve will then converge to a position on the frac tal curve    A function that is rich in fractals is the squaring transformation z f z  Az  z  103 where A is assigned any constant complex value For this function we can use the inverse method to locate the fractal curve We first rearrange terms to obtain the quadratic equation  zt27 aA 106  The inverse transformation is then the quadratic formula     z fl  sft  VI 47A  107  Using complex arithmetic operations we solve this equation for the real and imaginary parts of z as   Locating the fractal boundary with the inverse self squaring function      A unit circle in the complex plane The nonfractal  complex squaring function f z   moves points that are inside the circle toward the origin while points outside the circle are moved farther away from the circle Any initia point on the circle remains on the circle  Chapter Three Dimensional Object x  Re z  x  V Representations  Im    yo im   a Idiser Re discr   diser  Retain  108  with the discriminant of the quadratic formula as discr   A A few initial values for x and y say  can be calculated and discarded before we begin to plot the fractal curve Also since this function yields two possible transformed x y positions we can randomly choose either the plus or the minus sign at each step of the iteration as long as Im discr   Whenever Im discr  the two possible positions are in the second and fourth quadrants In this case x and y must have opposite signs The following procedure gives an implementation of this self squaring function and two example curves are plotted in    include math h  include values h  include graphics h  typedef struct  float x y  Complex   float lambdaMagSq discrMag Complex discr  Static firstPoint  TRUE  if ArstPoint    Compute divided by lambda    ry ry pet aren a a     Ma wo ra rt   e met  a4 iL af x  x  ow roe i ve Me L f  a  f dp  eres nur ek i ns   Two fractal curves generated with the inverse of the function f z  Az z by procedure sel fSquare  a A  3and b A   Each curve is plotted with 000 points static Complex fourOverLambda     void calculatePoint Complex lambda Complex   lamodaMagSq  lambda x  lamb a x  lambda y   ambda fourOverLambda x   lambda x  lambdaMagSq  yi fouroverLambda y   lambda y  lambdaMagSq firstPoin  FALSE  discr x   z x  fourOverLambda x  z y  fourOver Lambda y  diser y  z x  fourOverLambda y  z y  fourOverLambda x discrMag  sqrt discr x  discr x  diser y  discr y    Update z checking to avoid the sqrt of a negative number  if discrMag  discr x   z x  else zZ x  sqrt  discrMag  discr x   if discrMag  discr x  g y  QO else z y   sqrt  discrMag  discr x     Por half the points use negative root placing point in quad rant  if random   MAXINT  Z K   K Z y  Z y    When imaginary part of discriminant is negative point should lie in quadrant or so reverse sign of x   if discr y  z x  z x   Pinish up calculation for the real part of z   ze x     x    void selfSquare Complex lambda Complex z int count  int k   Skip the first few points  fcr k k l0 k  calculatePoint lambda  z  for k k count k   calculatePoint lambda  z   Scale point to fit window and draw  i pPoint z x WINDOW_WIDTH WINDOW_HEIGHT z y wWINDOW_HEIGHT     _I A three dimensional plot in variables x y and A of the self squaring func tion f z  Az z  with Al  is given in  Each cross sectional slice of this plot is a fractal curve in the complex plane  A very famous fractal shape is obtained from the Mandelbrot set which is the set of complex values z that do not diverge under the squaring transforma tion  m  109    k   That is we first select a point z in the complex plane then we compute the trans formed position z  z At the next step we square this transformed position and add the original z value We repeat this procedure until we can determine    The function f z  Az plotted in three dimensions with normalized A values plotted as the vertical axis  Courtesy of Alan Norton 1BM Research  whether or not the transformation is diverging The boundary of the convergence region in the complex plane is a fractal  To implement transformation 109 we first choose a window in the com plex plane Positions in this window are then mapped to color coded pixel posi tions in a selected screen viewport   The pixel colors are chosen ac cording to the rate of divergence of the corresponding point in the complex plane under transformation 109 If the magnitude of a complex number is greater than then it will quickly diverge under this self squaring operation Therefore we can set up a loop to repeat the squaring operations until either the magnitude of the complex number exceeds or we have reached a preset number of itera tions The maximum number of iterations is usually set to some value between and although lower values can be used to speed up the calculations With lower settings for the iteration limit however we do tend to lose some de tail along the boundary Julia set of the convergence region At the end of the loop we select a color value according to the number of iterations executed by the loop For example we can color the pixel black if the iteration count is at the Viewport imaginary  Window  n        Mapping positions in the complex plane to color coded pixel positions ona video monitor maximum value and we can color the pixel red if the iteration count is near Other color values can then be chosen according to the value of the iteration count within the interval from to the maximum value By choosing different color mappings we can generate a variety of dramatic displays for the Mandel brot set One choice of color coding for the set is shown in  a  An algorithm for displaying the Mandelbrot set is given in the following procedure The major part of the set is contained within the following region of the complex plane  25  Re z  75  25  Im   25 We can explore the details along the boundary of the set by choosing successively smaller window regions so that we can zoom in on selected areas of the display   shows a color coded display of the Mandelbrot set and a series of zooms that illustrate some of the features of this remarkable set  Section 18 Fractat Geometry Methods   Zooming in on the Mandelbrot set Starting with a display of the Mandelbrot set a  we zoom in on selected regions b through f  The white box outline shows the window area selected for each successive Zoom  Courtesy of Brian Evans Vanderbilt University   include graphics h  typedef struct  flcat  y  Yomplex j Comp ex complexSquare lomplex c   Complex cSq  cSq x  eSq y   return  cSq   q xo  int iterate Complex cinit int maxIteri  complex z  int cnt  Quit when z z 77 wnile  z x  x  ty  z y     ent  maxiter    complexSquare Zz  Z x ZInit x z y  Zinit y  ontt   return cnt   void mandelbrot int nx int ny int maxIter float realMin float realMax float im gMin float imagMax  float realInc  realMax  realMin  nx float imagInc  imagMax  imagMin  ny Complex z  int x yi int cont  for x C x realMin x nx x  z x realInc for y z ysimagMin y ry yt  z y simagiInc  ent  iterate z maxIter af cnt  maxiter setColor BLACK  else setColor cnt  pPoint x y    Complex function transformations such as Eq 105 can be extended to produce fractal surfaces and fractal solids Methods for generating these objects use quaternion representations Appendix A for transforming points in three dimensional and four dimensional space A quaternion has four components one real part and three imaginary parts and can be represented as an extension of the concept of a number in the complex plane  q stiat jb ke 710  wherei  j  k   The real partsisalsoreferred toas the scalar part of the quater nion and the imaginary terms are called the quaternion vector part v  a b c  Using the rules for quaternion multiplication and addition discussed in Ap pendix A we can apply self squaring functions and other iteration methods to generate surfaces of fractal objects instead of fractal curves A basic procedure is to start with a position inside a fractal object and generate successive points from that position until an exterior diverging point is identified The previous inte rior point is then retained as a surface point Neighbors of this surface point are then tested to determine whether they are inside converging or outside diver rging  Any inside point that connects to an outside point is a surface point In this way the procedure threads its way along the fractal boundary without gen erating points that are too far from the surface When four dimensional fractals are generated three dimensional slices are projected onto the two dimensional surface of the video monitor  Procedures for generating self squaring fractals in four dimensional space require considerable computation time for evaluating the iteration function and for testing points Each point on a surface can be represented as a small cube giv ing the inner and outer limits of the surface Output from such programs for the three dimensional projections of the fractal typically contain over a million ver tices for the surface cubes Display of the fractal objects is performed by applying illumination models that determine the lighting and color for each surface cube Hidden surface methods are then applied so that only visible surfaces of the ob jects are displayed Figures 101 and 102 show examples of self squaring four dimensional fractals with projections into three dimensions  Self Inverse Fractals Various geometric inversion transformations can be used to create fractal shapes Again we start with an initial set of points and we repeatedly apply nonlinear inversion operations to transform the initial points into a fractal  As an example we consider a two dimensiona inversion transformation with respect to a circle with radius r and center at position Py  x5 yo  Any point P outside the circle will be inverted to a position P inside the circle Fig 103 with the transformation  P PXP P   111  Section 18 Fractal Geometry Methods    Three dimensional projections of four dimensional fractals generated with the self squaring quaternion function f q  Aq q  a A  475  9061i and b A  57   Courtesy of Alan Norton IBM Research   Chapter Three Dimensional Object Representations    A three dimensional surface projection of a four dimensional object generated with the self squaring quaternion function f q  q   Courtesy of Alan Norton IBM Research   Inverting point P to a position P inside a circle with radius r  Reciprocally this trasformation inverts any point inside the circle to a point out side the circle Both P and P lie on a straight line passing through the circle cen ter Po  If the coordinates of the two points are P  x y and P     y  we can write Eq 111 as  Ce xP  yp     yp   PF Also since the two points lie along a line passing through the circle center we have y  y x  x9   y    x  x9  Therefore the transformed coordi nate values are Px  xX ry  yo   x   _  y  o x  xp  y  H    Mo x  xo  y   112   illustrates the inversion of points along another circle bound ary As long as the circle to be inverted does not pass through P it will transform to another circle But if the circle circumference passes through Pp the circle _ Originat Circle Inverted a Circle     Inversion of a circle with respect to another circle  transforms to a straight line Conversely points along a straight line not passing through P invert to a circle Thus straight lines are invariant under the inversion transformation Also invariant under this transformation are circles that are or thogonal to the reference circle That is the tangents of the two circles are perpen dicular at the intersection points  We can create various fractal shapes with this inversion transformation by starting with a set of circles and repeatedly applying the transformation using different reference circles Similarly we can apply circle inversion to a set of straight lines Similar inversion methods can be developed for other objects And  we can generalize the procedure to spheres planes or other shapes in three di mensional space  A number of other procedural methads have been developed for generating ob ject details Shape grammars are sets of production rules that can be applied to an initial object to add layers of detail that are harmonious with the original shape Transformations can be applied to alter the geometry shape of the object or the transformation rules can be applied to add surface color or surface texture detail  Given a set of production rules a shape designer can then experiment by applying different rules at each step of the transformation from a given initial ob ject to the final structure  shows four geometric substitution rules for altering triangle shapes The geometry transformations for these rules can be B he Rule Rule    Four geometric substitution mules for subdividing and altering the shape of an equilateral triangle  Chapter10 written algorithmically by the system based on an input picture drawn with a Three Dimensional Object production rule editor That is each rule can be described graphically by show Representations ing the initial and final shapes Implementations can then be set up in Mathemat  ica or some other programming language with graphics capability An application of the geometric substitutions in  is given in Fig 106 where  d is obtained by applying the four rules in succession starting with the initial triangle in  a   shows another shape created with triangle substitution rules   b  c   An equilateral triangle a is converted to shape b using substitution mules and in Fig 105 Rule is then used to convert b into shape c  which in  turn is transformed to d using Tule  Copyright  Andrew Glassner Xerox PARC Palo Alto Research Center    A design created with geometric substitution rules for altering triangle shapes  Copyright  Andrew Glassner Xerox PARC Palo Alto Research Center   Ts   A design created with geometric substitution rules for altering prism shapes The initial shape for this design was a representation of Rubik s Snake  Copyright  Andrew Glassner Xerox PARC Palo Alto Research Center   Three dimensional shape and surface features are transformed with similar operations  shows the results of geometric substitutions applied to polyhedra The initial shape for the objects shown in  is an icosahe dron a polyhedron with faces Geometric substitutions were applied to the plane faces of the icosahedron and the resulting polygon vertices were projected to the surface of an enclosing sphere  Another example of using production rules to describe the shape of objects is L grammars or graftals These rules provide a method for describing plants For _ instance the topology of a tree can be described as a trunk with some attached branches and leaves A tree can then be modeled with rules to provide a particu lar connection of the branches and the leaves on the individual branches The geometrical description is then given by placing the object structures at particular coordinate positions   shows a scene containing various plants and trees con structed with a commercial plant generator package Procedures in the software for constructing the plants are based on botanical laws   Designs created on the surface of a sphere using triangle substitution Tules applied to the plane faces of an icosahedron followed by projections to the sphere surface  Copyright  Andrew Classner Xerox PARC Palo Alto Research Center     Realistic scenery generated with the TDI AMAP software package which can generate over varieties of plants and trees using procedures based on botanical laws  Courtesy of Thomson Digital Image  20  PARTICLE SYSTEMS A method for modeling natural objects or other irregularly shaped objects that exhibit fluid like properties is particle systems This method is particularly good for describing objects that change over time by flowing billowing spatter ing or expanding Objects with these characteristics include clouds smoke fire fireworks waterfalls water spray and clumps of grass For example particle sys tems were used to model the planet explosion and expanding wall of fire due to the genesis bomb in the motion picture Star Trek I The Wrath of Khan  Random processes are used to generate objects within some defined region of space and to vary their parameters over time Af some random time each ob ject is deleted During the lifetime of a particle its path and surface characteris tics may be color coded and displayed  Particle shapes can be small spheres ellipsoids boxes or other shapes The size and shape of particles may vary randomly over time Also other properties such as particle transparency color and movement all can vary randomly In some applications particle motion may be controlled by specified forces such as a gravity field  As each particle moves its path is plotted and displayed in a particular color For example a fireworks pattern can be displayed by randomly generating Particles within a spherical region of space and allowing them to move radially Section 21 Partical Systems   Modeling fireworks as a particle system with particles traveling radially outward from the center of the sphere  outward as in  The particle paths can be color coded from red to yel low for instance to simulate the temperature of the exploding particles Simi larly realistic displays of grass clumps have been modeled with trajectory par ticles   that are shot up from the ground and fall back to earth under gravity In this case the particle paths can originate within a tapered cylinder and might be color coded from green to yellow   illustrates a particle system simulation of a waterfall The water particles fall from a fixed elevation are deflected by an obstacle and then splash up from the ground Different colors are used to distinguish the particle   Modeling a clump of grass by firing particles upward within a tapered cylinder The particle paths are parabolas due to the downward force of gravity     Simulation of the behavior of a waterfall hitting a stone circle  The water particles are deflected by the stone and then splash up from the ground  Courtesy of M Brooks and T L   Howard Department of Computer Scrence University of Manchester   Chapter Three Dimensional Object Representations paths at each stage An example of an animation simulating the disintegration of an object is shown in  The object on the left disintegrates into the par ticle distribution on the right A composite scene formed with a variety of repre sentations is given in  The scene is modeled using particle system grass fractal mountains and texture mapping and other surface rendering pro cedures   An object disintegrating into a cloud of particles  Courtesy of Autodesk Inc    A scene entitled Road to Point Reyes showing particle system grass  fractal mountains and texture mapped surfaces  Courtesy of Pixar Copyright  Pixar   A nonrigid object such as a rope a piece of cloth or a soft rubber ball can be represented with physically based modeling methods that describe the behavior of the object in terms of the interaction of external and internal forces An accu rate discription of the shape of a terry cloth towel drapped over the back of a chair is obtained by considering the effect of the chair on the fabric loops in the cloth and the interaction between the cloth threads A common method for modeling a nonrigid object is to approximate the ob ject with a network of point nodes with flexible connections between the nodes One simple type of connection is a spring  shows a section of a two dimensional spring network that could be used to approximate the behavior of a sheet of rubber Similar spring networks can be set up in three dimensions to model a rubber ball or a block of jello For a homogeneous object we can use identical springs throughout the network If we want the object to have different properties in different directions we can use different spring properties in differ ent directions When external forces are applied tc a spring network the amount of stretching or compression of the individual springs depends on the value set for the spring constant k also called the force constant for the spring  Horizontal displacement x of a node position under the influence of a force F is illustrated in  If the spring is not overstretched we can closely approximate the amount of displacement x from the equilibrium position using Hooke s law  Fo  F  kx ifO T73  where F is the equal and opposite restoring force of the spring on the stretched node This relationship holds also for horizontal compression of a spring by an amount x and we have similar relationships for displacements and force compo nents in the y and z directions  If objects are completely flexible they return to their original configuration when the external forces are removed But if we want to model putty or some other deformable object we need to modify the spring characteristics so that the springs do not return to their original shape when the external forces are re moved Another set of applied forces then can deform the object ir some other way    k   i  unstretched pagsiton  F  An external force F pulling on ore end of a spring with the other end rigidly fixed   Figure Hed lo A two dimensional spring network constructed with identical spring constants k  Chapter Three Dimensional Object Representations  Instead of using springs we can also model the connections between nodes with elastic materials then we minimize strain energy functions to determine ob ject shape under the influence of external forces This method provides a better model for cloth and various energy functions have been devised to describe the behavior of different cloth materials  To model a nonrigid object we first set up the external forces acting on the object Then we consider the propagation of the forces throughout the network representing the object This leads to a set of simultaneous equations that we must solve to determine the displacement of the nodes throughout the network   shows a banana peel modeled with a spring network and the scene in  shows examples of cloth modeling using energy functions with a texture mapped pattern on one cloth By adjusting the parameters in a network using energy function calculations different kinds of cloth can be mod eled  illustrates models for cotton wool and polyester cotton mate rials draped over a table  Physically based modeling methods are also applied in animations to more accurately describe motion paths In the past animations were often specified using spline paths and kinematics where motion parameters are based only on  Modeling the flexible behavior of a banana peel with a spring network  Copyright  David Laidlaw John Snyder Adam Woodbury and Alan Barr Computer Graphics Lab California Institute of Technology    Modeling the flexible behavior of cloth draped over furniture using energy function minimization  Copyright  Gene Greger and David E Breen Design Research Center Rensselaer Polytechnic Institute  iba ud   Modeling the characteristics of a cotton  b wool and c polyester cotton using energy function minimization  Copyright  David E Breen and Donald H House Design Research Center Rensselaer Polytechnic Institute  position and velocity Physically based modeling describes motion using dynam ical equations involving forces and accelerations Animation descriptions based on the equations of dynamics produce more realistic motions than those based on the equations of kinematics  The use of graphical methods as an aid in scientific and engineering analysis is commonly referred to as scientific visualization This involves the visualization of data sets and processes that may be difficult or impossible to analyze without graphical methods For example visualization techniques are needed to deal with the output of high volume data sources such as supercomputers satellite and spacecraft scanners radio astronomy telescopes and medical scanners Mil lions of data points are often generated from numerical solutions of computer simulations and from observational equipment and it is difficult to determine trends and relationships by simply scanning the raw data Similarly visualization techniques are useful for analyzing processes that occur over a long time period or that cannot be observed directly such as quantum mechanical phenomena and special relativity effects produced by objects traveling near the speed of light Scientific visualization uses methods from computer graphics image pro cessing computer vision and other areas to visually display enhance and ma nipulate information to allow better understanding of the data Similar methods employed by commerce industry and other nonscientific areas are sometimes re ferred to as business visualization  Data sets are classified according to their spatial distribution and according to data type Two dimensional data sets have values distributed over a surface and three dimensional data sets have values distributed over the interior of a cube a sphere or some other region of space Data types include scalars vectors tensors and multivariate data  Visual Representations for Scalar Fields A scalar quantity is one that has a single value Scalar data sets contain values that may be distributed in time as well as over spatial positions Also the data Section 22 Visuatization of Data Sets  Chapter Three Dimensional Object Representations  values may be functions of other scalar parameters Some examples of physical scalar quantities are energy density mass temperature pressure charge resis tance reflectivity frequency and water content  A common method for visualizing a scalar data set is to use graphs or charts that show the distribution of data values as a function of other parameters such as position and time If the data are distributed over a surface we could plot the data values as vertical bars rising up from the surface or we can interpolate the data values to display a smooth surface Pseudo color methods are also used to distinguish different values in a scalar data set and color coding techniques can be combined with graph and chart methods To color code a scalar data set we choose a range of colors and map the range of data values to the color range For example blue could be assigned to the lowest scalar value and red could be assigned to the highest value  gives an example of a color coded surface plot Color coding a data set can be tricky because some color combina tions can lead to misinterpretations of the data  Contour plots are used to display isolines lines of constant scalar value for a data set distributed over a surface The isolines are spaced at some convenient interval to show the range and variation of the data values over the region of space A typical application is a contour plot of elevations over a ground plane Usually contouring methods are applied to a set of data values that is distributed over a regular grid as in  Regular grids have equally spaced grid lines and data values are known at the grid intersections Numerical solutions of computer simulations are usually set up to produce data distributions on a regu lar grid while observed data sets are often irregularly spaced Contouring meth ods have been devised for various kinds of nonregular grids but often nonregu lar data distributions are converted to regular grids A two dimensional contouring algorithm traces the isolines from cell to cell within the grid by check ing the four corners of grid cells to determine which cell edges are crossed by a   A financial surface plot showing stock growth potential during the October stock market crash Red indicates high returrs and the plot shows that low growth stocks performed better in the crash  Courtesy of Eng Kiat Koh Information Technology Institute Republic of Singapore  I rn nn nn ee ee  _  re rt  i A regular two dimensional grid yt a with data values at the intersection ee _ wa of the grid lines The x grid lines have a constant Ax spacing and the y grid lines have a constant Ay VO ett ea mettre ne nee spacing where the spacing in the x C wee  co a and y directions may not be the Ax same particular isoline The isolines are usually plotted as straight line sections across each cell as illustrated in  Sometimes isolines are plotted with spline curves but spline fitting can lead to inconsistencies and misinterpretation of a data set For example two spline isolines could cross or curved isoline paths might not be a true indicator of the data trends since data values are known only at the cell corners Contouring packages can allow interactive adjustment of iso lines by a researcher to correct any inconsistencies An example of three overlap ping color coded contour plots in the xy plane is given in  and Fig 125 shows contour lines and color coding for an irregularly shaped space  For three dimensional scalar data fields we can take cross sectional slices and display the two dimensional data distributions over the slices We could ei ther color code the data values over a slice or we could display isolines Visual ization packages typically provide a slicer routine that allows cross sections to be   Color coded contour plots for three data sets within the same region of the xy plane  Courtesy of the National Center for Supercomputing Applications University of Mlinois at Urbana Champaign     Color coded contour plots over the surface of an apple core shaped region of space  Courtesy of Greg Nielson Department of Computer Science and Engineering Arizona State University  Section 22 Visualization of Data Sets   e     The path of an isoline across five grid cells   Cross sectional slices of a three  dimensional data set  Courtesy of Spyglass inc  B ve eBar  taken at any angle  shows a display generated by a commercial slicer dicer package  Instead of looking at two dimensional cross sections we can plot one or more isesurfaces which are simply three dimensional contour plots Fig   When two overlapping isosurfaces are displayed the outer surface is made transparent so that we can view the shape of both isosurfaces Constructing an isosurface is similar to plotting isolines except now we have three dimensional grid cells and we need to check the values of the eight corners of a cell to locate sections of an isosurface  shows some examples of isosurface inter sections with grid cells Isosurfaces are modeled with triangle meshes then sur face rendering algorithms are applied to display the final shape   An isosurface generated from a set of watet content values obtained from a numerical model of a thunderstorm  Courtesy of Bob Withelmson Department of Atmospheric Sciences and National Center for Supercomputing Applications University of Hlinois at Urbana Champaign    Isosurface intersections with grid cells modeled with triangle patches  Volume rendering which is often somewhat like an X ray picture is an other method for visualizing a three dimensional data set The interior informa tion about a data set is projected to a display screen using the ray casting meth ods introduced in Section 15 Along the ray path from each screen pixel Fig 129  interior data values are examined and encoded for display Often data values at the grid positions are averaged so that one value is stored for each voxel of the data space How the data are encoded for display depends on the ap plication Seismic data for example is often examined to find the maximum and minimum values along each ray The values can then be color coded to give in formation about the width of the interval and the minimum value In medical ap plications the data values are opacity factors in the range from to for the tis sue and bone layers Bone layers are completely opaque while tissue is somewhat transparent low opacity  Along each ray the opacity factors are accu mulated until either the total is greater than or equal to or until the ray exits at the back of the three dimensional data grid The accumulated opacity value is then displayed as a pixel intensity level which can be gray scale or color Figure 130 shows a volume visualization of a medical data set describing the struc ture of a dog heart For this volume visualization a color coded plot of the dis tance to the maximum voxel value along each pixel ray was displayed  Data Volume   Volume visualization of a regular Cartesian data grid using ray casting to examine interior data values  Volume visualization of a data set for a dog heart obtained by      eeee   plotting the color coded distance to the maximum voxel value for each pixel  Courtesy of Patrick Moran and Clinton Potter National Center for Supercomputing Applications University of Himois at Urbana Champaign   Section 22 Visualization of Data Sets Three Dimensional Object  Chapter Representations Visual Representations for Vector Fields A vector quantity V in three dimensional space has three scalar values V  V  V  one for each coordinate direction and a two dimensional vector has two components V  V  Another way to describe a vector quantity is by giving its magnitude v  and its direction as a unit vector u As with scalars vector quan tities may be functions of position time and other parameters Some examples of physical vector quantities are velocity acceleration force electric fields magnetic fields gravitational fields and electric current  One way to visualize a vector field is to plot each data point as a small arrow that shows the magnitude and direction of the vector This method is most often used with cross sectional slices as in  since it can be difficult to see the data trends in a three dimensional region cluttered with overlapping ar rows Magnitudes for the vector values can be shown by varying the lengths of the arrows or we can make all arrows the same size but make the arrows differ ent colors according to a selected color coding for the vector magnitudes   Arrow representation for a vector field over cross sectional slices  Courtesy of the National Center for Supercomputing Applications University of Illinois at Urbana Champaign  We can also represent vector values by plotting field lines or streamlines Field lines are commonly used for electric magnetic and gravitational fields The magnitude of the vector values is indicated by the spacing between field lines and the direction is the tangent to the field as shown in  An example of a streamline plot of a vector field is shown in  Streamlines can be displayed as wide arrows particularly when a whirlpool or vortex effect is pre sent An example of this is given in  which displays swirling airflow patterns inside a thunderstorm For animations of fluid flow the behavior of the vector field can be visualized by tracking particles along the flow direction An ee hi I  t ther Field line representation for a lower vector data set  Section 22  Visualization of Data Sets   Visualizing airflow around a cylinder with a hemispherical cap that is tilted slightly relative to the incoming direction of the airflow  Courtesy of M Gerald Yamasaki J Huiltquist and Sam Uselton NASA Ames Research Center    Twisting airflow patterns visualized with wide streamlines inside a transparent isosurface plot of a thunderstorm  Courtesy of Bob Withelmson Department of Atmospheric Sciences and Nafional Center far Supercomputing Applications University of Hlinois at Urbana Champaign    Airflow patterns visualized with both streamlines and particle  motion inside a transparent isosurface plot of a thunderstorm Rising sphere particles are colored orange and falling sphere particles are blue  Courtesy of Bob Wilhelmson Department of Atmospheric Sciences and National Center for Supercomputing Applications University of llinots at Urbana Champaign   example of a vector field visualization using both streamlines and particles is shown in   Sometimes only the magnitudes of the vector quantities are displayed This is often done when multiple quantities are to be visualized at a single position or when the directions do not vary much in some region of space or when vector directions are of less interest  Visual Representations for Tensor Fields A tensor quantity in three dimensional space has nine components and can be represented with a by matrix Actually this representation is used for a sec ond order tensor and higher order tensors do occur in some applications particu larly general relativity Some examples of physical second order tensors are Chapter Three Dimensional Object Representations stress and strain in a material subjected to external forces conductivity or resis tivity of an electrical conductor and the metric tensor which gives the proper ties of a particular coordinate space The stress tensor in Cartesian coordinates for example can be represented as  Gy Ow Oy Op 114 Ox Ty oO  Tensor quantities are frequently encountered in anisotropic materials which have different properties in different directions The x xy and xz elements of the conductivity tensor for example describe the contributions of electric field components in the x y and z directions to the current in the x direction Usually physical tensor quantities are symmetric so that the tensor has only six distinct values For instance the xy and yx components of the stress tensor are the same  Visualization schemes for representing all six components of a second order tensor quantity are based on devising shapes that have six parameters One graphical representation for a tensor is shown in  The three diagonal elements of the tensor are used to construct the magnitude and direction of the arrow and the three off diagonal terms are used to set the shape and color of the elliptical disk  Instead of trying to visualize all six components of a tensor quantity we can reduce the tensor to a vector or a scalar Using a vector representation we can simply display a vector representation for the diagonal elements of the tensor And by applying tensor contraction operations we can obtain a scalar representa tion For example stress and strain tensors can be contracted to generate a scalar strain energy density that can be plotted at points in a material subject to external forces    Visual Representations for Multivariate Data Fields In some applications at each grid position over some region of space we may have multiple data values which can be a mixture of scalar vector and even ten    Representing stress and strain tensors with an elliptical disk and a rod over the surface of a stressed material  Courtesy of Bob Haber National Center for Supercomputing Applications Untversity of Ulinais at Urbana Champaign     Representing stress and strain tensors with a strain energy density plot in a visualization of crack propagation on the surface of a stressed material  Courtesy of Bob Haber National Center for Supercomputing Applications University of Illinois at Urbana Champaign  sor values As an example for a fluid flow problem we may have fluid velocity temperature and density values at each three dimensional position Thus we have five scalar values to display at each position and the situation is similar to displaying a tensor field  A method for displaying multivariate data fields is to construct graphical objects sometimes referred to as glyphs with multiple parts Each part of a glyph represents a physical quantity The size and color of each part can be used to display information about scalar magnitudes To give directional information for a vector field we can use a wedge a cone or some other pointing shape for the glyph part representing the vector An example of the visualization of a mul tivariate data field using a glyph structure at selected grid positions is shown in   Ligure 138 One frame from an animated visualization of a multivariate data field using glyphs The wedge shaped part of the glyph indicates the direction of a vector quantity at each point  Courtesy of the Natranal Center for Supercomputing Applications University of Hiunais at Lirbana Champaign  Visualization of Data Sets   Chapter Three Dimenstanal Object Representations  SUMMARY Many representations have been developed for modeling the wide variety of ob jects that might be displayed in a graphics scene  Standard graphics objects are those represented with a surface mesh of polygon facets Polygon mesh represen tations are tvpically derived from other representations  Surface functions such as the quadrics are used to describe spheres and other smooth surfaces For design applications we can use superquadrics splines or blobby objects to represent smooth surface shapes In addition can struction techniques such as CSG and sweep representations are useful for de signing compound object shapes that are built up from a set of simpler shapes And interior as well as surface information can be stored in octree representa tions Descriptions for natural objects such as trees and clouds and other irregu larly shaped objects can be specified with fractals shape grammars and particle systems Finally visualization techniques use graphic represcntations to display numerical or other types of data sets The various types of numerical data in clude sealar vector and tensor values Also many scientific visualizations require methods for representing multivariate data sets that contain a combination of the various data types  REFERENCES A detailed discuss on of superquadrics is Contained in Barr  For more information on blobby object niodeling see Blinn  The metaball mode is discussed in Nishimura  and the soft object model is discussed in Wyville Ww lle and McPheeters  Sources of information on parametric curve and surface representations include Bezie  Burt and Adelsor  Barsky   Kochanek and Bartels  Farouki and Hinds  Huitric and Nahas  Mortenson  Farin  and Ravers and Adams  Octrees and quadtrees cre discussed bys Doctor  Yamaguchi Kuni and Fujimura  and bs Carlbom Chakravarty and Vanderschel  Solic modeling references include Casate and Staten and Requicha and Rassizrac  For further information on iractal representations see Mandellrot 1982  Fournier Fussel and Carpenter  Noston  Peitgen and Richter  Peitgen and Saupe  Koh and Hearn  and Barnsley  Skape grammars are disc ussed in Glassner  and particle systems are discussed in Reeves  A discussion at physically based modeling is given in Barzel   A general introduction to visualization methods is given ir Hearn and Baker  Addi tonal intormation on specific v sualization methods can be found in Sabin  Lorensen and Cline  Drebin Carpenter and Hanrahan  Sabella Lipson and Keeler  Frenkel  Nielson Shriver and Rosenblum and Ntelson  Guidelines tor visual displays of information are given in Tute   EXERCISES HOV Set ap geomuetac data tables asin Fig for aun cube  Setup weanmetre data tables tora unit cube using only cas vertex and polygon tables and bia single poivgon table Compare the two methods for representing the unit cube with a representation using three data tables and esamate storage requirenrents tor Gach 10  11  12  13  14  15  16  17  18  19 20 21 22  23  24  25  26  27  28   Define an efficient polygon representation for a cylinder Justify your choice of repre  sentation   Set up a procedure for establishing polygon tables for any input set of data points defining an object  Devise routines for checking the data tables in  for consistency and com  pleteness   Write a program that calculates parameters A B C and BD for any set of three di  mensional plane surfaces defining an object   Given the plane parameters A B C and D for all surfaces of an object devise an al  gorithm to determine whether any specified point is inside or outside the object   How would the values for parameters A 8B C and D in the equation of a plane sur  face have to be altered if the coordinate reference is changed from a right handed system to a lefi handed system   Sel up an algorithm for converting any specified sphere ellipsoid or cylinder to a polygon mesh representation  Set up an algorithm for converting a specified superellipsoid to a polygon mesh rep resentation  Set up an algorithm for converting a metaball representation to a palygon mesh rep resentation Write a routine to display a two dimensional cardinal spline curve given an input set of contro points in the xy plane  Write a routine to display a two dimensional Kochanek Bartels curve given an input set of control points in the xy plane  Determine the quadratic B zier blending functions for three contro points Plot each function and label the maximum and minimum values  Determine the B zier blending functions for five control points Plot each function and label the maximum and minimum values   Write an efficient routine to display two dimensional cubic B zier curves given a set of four control points in the xy plane  Write a routine to design two dimensional cubic B zier curve shapes that have first order piecewise continuity Use an interactive technique for selecting control point positions in the xy plane for each section of the curve  Write a routine to design two dimensional cubic B zier curve shapes that have sec and order piecewise continuity Use an interactive technique for selecting control point positions in the xy plane for each section of the curve  Write a routine to display a cubic B zier curve using a subdivision method Determine the blending functions for uniform periodic B spline curves for d  Determine the blending functions for uniform periodic B spline curves for d  Write a program using forward differences to calculate points along a two dimen sional uniform periodic cubic B spline curve given an input set of control points Write a routine to display any specified conic in the xy plane using a rational B zier spline representation  Write a routine to display any specified conic in the xy plane using a rational B spline representation  Develop an algorithm for calculating the normal vector to a Bezier surface at the point P u Vv  Write a program to display any specified quadratic curve using forward differences to calculate points along the curve path  Write a program to display any specified cubic curve using forward differences lo calculate points along the curve path  Derive expressions for calculating the forward differences for any specified quadratic curve Exercises Chaater  Three Dimensianal Object Represertations 31  32  33 34 35  36  38  39  40  41 42 43  44  45  46 47  48  49  50 5t 52  Derive expressions tor calculaung the forward differences for any specified cubic CUrVe  Set up procedures ter generating the description of a three dimensional object from input parameters thai define the object in terms of a translational sweep  Develop procedures for generating the description of a three dimensiona object using input parameters that define the object in terms of a rotational sweep Devise an algorithm tar generating solid objects as cornbinations of three dimen sional primitive shapes each defined as a set of surfaces using cunstructive solid geometsy methods  Develop an algorithm for performing constructive solid geametry modeling using a Primitive set of solids defined in octree structures Develop an algorithm for encoding a two dimensional scene as a Quadiree represen tation  Set up an algorithm for loading a quadtree representation of a scene into a frame buffer for display of tne scene  Write a routine to convert the polygon definition of a three dimensional object into an octree representation   Using the random midpoint displacement method write a routine to create a moun  tain outline starting with a horizontal fine in the xy plane Write a routine to calculate elevations above a ground plane using the random mid point cisplacement method  Write a program for 2enerating a iractal snowtlake Koch curve for any given num ber of iteratians Write a program to generate a fractal curve far a specified number af iterations using one of the generators in  of 72 What is the fractal dimension of vour curve  Write a program to generate fractal curves using the self squaring function Mz   A where Vis any selected complex constant Write a program a generate fractal curves using the self squaring tunction fQX  Kz  Ti where ro W T  Write a routine to interactively select diflerent color combinatians for displaying the Mandelbrot set  Write program to interactively select ary rectangular egion of the Mandelbrot set and to 200m in on the selected region  Write a routine to implement point inversion Eq 112 tor any specified circle and any given point position  Devise a set of geometric substitution rules for altering the shape of an equilateral tri  angle  Write a program to display the stages in the conversion of an equilateral triangle into another shape given a set of geometric substitulion rules Write a program to model an exploding firecracker in the xy plane using a particle system  Devise an algorithm for modeling a rectangle as a nonrigid body using identical springs for the four sides of the rectangle  Write routine to visualize a two dimensional scalar data set using pseudo color methods Write a routine to visualize a two dimensional scalar data set using contour lines Write a routine to visualize a two dimensional vecto data set using an arrow repre sentation for the vector values Make a arrows the same length but display the ar rows with different cclors to represent the different vector magnitudes CHAPTER  Three Dimensional Geometric and Modeling Transformations  Bie sce E coli Thymidytate Synthase WM JFM RMS  Tren hg a ered     M ethods for geometric transformations and object modeling in three di mensions are extended from two dimensional methods by including considerations for the z coordinate We now translate an object by specifying a three dimensional translation vector which determines how much the object is to be moved in each of the three coordinate directions Similarly we scale an object with three coordinate scaling factors The extension for three dimensional rota tion is less straightforward When we discussed two dimensional rotations in the xy plane we needed to consider only rotations about axes that were perpendicu lar to the xy plane In three dimensional space we can now select any spatial ori entation for the rotation axis Most graphics packages handle three dimensional rotation as a composite of three rotations one for each of the three Cartesian axes Alternatively a user can easily set up a general rotation matrix given the orienta tion of the axis and the required rotation angle As in the two dimensional case we express geometric transformations in matrix form Any sequence of transfor mations is then represented as a single matrix formed by concatenating the ma trices for the individual transformations in the sequence  In a three dimensional homogeneous coordinate representation a point is trans  lated   from position P   y z to position P   x  y  z  with the ma trix operation x x efrfoor e   ae 000 or P T P  Parameters t  f  and t  specifying translation distances for the coordinate direc tions x y and z are assigned any real values The matrix representation in Eq is equivalent to the three equations xosut t yorytth  tH nN   t   axis etx yz     Te ltt tl  a xaxis Translating a point with translation axis vector T  t  ty t   y axis ei yi z Tait t t   Translating an object with Z axis translation vector T  An object is translated in three dimensions by transforming each of the defining points of the object For an object represented as a set of polygon sur faces we translate each vertex of each surface   and redraw the polygon facets in the new position  We obtain the inverse of the translation matrix in Eq by negating the translaticn distances t  ty and t  This produces a translation in the opposite di rection and the product of a translation matrix and its inverse produces the iden tity matrix   To generate a rotation transformation for an object we must designate an axis of rotation about which the object is to be rotated and the amount of angular rota tion Unlike two dimensional applications where all transformations are carried out in the xy plane a three dimensional rotation can be specified around any line in space The easiest rotation axes to handle are those that are parallel to the coor dinate axes Also we can use combinations of coordinate axis rotations along with appropriate translations to specify any general rotation  By convention positive rotation angles produce counterclockwise rotations about a coordinate axis if we are looking along the positive half of the axis to ward the coordinate origin   This agrees with our earlier discussion of rotation in two dimensions where positive rotations in the xy plane are counter clockwise about axes parallel to the z axis  Coordinate Axes Rotations The two dimensional z axis rotation equations are easily extended to three di mensions  Section Rotation Chapter b Three Dimensional Geometric and Modeling Transformatons   b    Positive rotation directions a about the coordinate axes are counterc ockwise when looking toward the origin from a positive io coordinate position on each axis  x  xcos  ysin  xsin ycos uil d    Ze Parameter specifies the rotation angle In homogeneous coordinate form the three dimensional z axis rotation equations are expressed as x cos sing O x   sa wey _  sin  cos Oy  y ES z 0 Zz  Section   pection ire Rotation x   Rotation of an object about the z Zz axis which we can write more compactly as P R  P   illustrates rotation of an object about the z axis  Transformation equations for rotations about the other two coordinate axes can be obtained with a cyclic permutation of the coordinate parameters x y and z in Eqs  That is we use the replacements KAYLA   as illustrated in  Substituting permutations in Eqs  we get the equations for an x axis rotation y  ycos  zsin e z ysind  z cost  v x which can be written in the homogeneous coordinate form  x 0 x yo _ cos sine y aren sin cos z  0 y x    SX x fy y z  a a Figure  Cyclic permutation of the Cartesian coordinate axes to produce the three sets of coordinate axis rotation equations Chapter ye Three Dimensional Geometric  and Modeling Transtormations   Rotation of an object about the x X axis or P  R   P 10  Rotation of an object around the x axis is demonstrated in Fig  Cyclically permuting coordinates in Eqs give us the transformation equations for a y axis rotation   zc0s  x sing x zsin xcos GT yr  The matrix representation for y axis rotation is    x cos QO sin O x   _  y eld z sind cosd z 0 or P R   P iyfeT3  An example of y axis rotation is shown in   Yy ba a ae x  axis   Rotation of an object about the  a An inverse rotation matrix is formed by replacing the rotation angle  by  Negative values for rotation angles generate rotations in a clockwise direc tion so the identity matrix is produced when any rotation matrix is multiplied by its inverse Since anly the sine function is affected by the change in sign of the ro tation angle the inverse matrix can also be obtained by interchanging rows and columns That is we can calculate the inverse of any rotation matrix R by evalu ating its transpose R7  R  This method for obtaining an inverse matrix holds also for any composite rotation matrix  General Three Dimensional Rotations A rotation matrix for any axis that does not coincide with a coordinate axis can be set up as a composite transformation involving combinations of translations and the coordinate axes rotations We obtain the required composite matrix by first setting up the transformation sequence that moves the selected rotation axis onto one of the coordinate axes Then we set up the rotation matrix about that co ordinate axis for the specified rotation angle The last step is to obtain the inverse transformation sequence that returns the rotation axis to its original position  In the special case where an object is to be rotated about an axis that is par allel to one of the coordinate axes we can attain the desired rotation with the fol lowing transformation sequence  Translate the object so that the rotation axis coincides with the parallel coor dinate axis  Perform the specified rotation about that axis  Translate the object so that the rotation axis is moved back to its original po sition  The steps in this sequence are illustrated in  Any coordinate position P on the object in this figure is transformed with the sequence shown as P T R   T P where the composite matrix for the transformation is R  T R T which is of the same form as the two dimensional transformation sequence for rotation about an arbitrary pivot point  When an object is to be rotated about an axis that is not parallel to one of the coordinate axes we need to perform some additional transformations In this case ve also need rotations to align the axis with a selected coordinate axis and to bring the axis back to its original orientation Given the specifications for the rotation axis and the rotation angle we can accomplish the required rotation in five steps   Translate the object so that the rotation axis passes through the coordinate origin No  Rotate the object so that the axis of rotation coincides with one of the caor dinate axes  Perform the specified rotation about that coordinate axis  Section Ratation Chaper Three Dimensianal Geometric and Modeling Transformations   a Qriginal Position o Object tc Rotate Object Through Angle    l0lgz  ilion  Axis  R may Be Z x N  b Translate Rotation Axis onto x Axis id Translate Rotation Axis to Original Position  Sequence of transformations for rotating an object about an axis that is parallel to the x axis  Apply inverse rotations to bring the rotation axis back to ts original orien tation  Apply the inverse translation to bring the rotation axis back to its original position  We can transform the rotation axis onto any of the three coordinate axes The z axis is a reasonable choice and the following discussion shows how to set up the transformation matrices for getting the rotation axis onto the z axis and returning the rotation axis to its original position    A rotation axis can be defined with two coordinate positions as in Fig  or with one coordinate point and direction angles er direction cosines be tween the rotation axis and two of the coordinate axes We will assume that the rotation axis is defined by two points as illustrated and that the direction of ro tation is to be counterclockwise when looking along the axis from P to P  An axis vector is then defined by the two points as V P P  Phd    Xy Y2 TY   A unit vector u is then defined along the rotation axis as Vv u TI  a b c  13 i  Pie   P f  a a  x Initial Step Positron Translate Step P to the Origin Rotate P onto the Axis x Step Step Step Ratate the Axis Translate the Rotate the to the Original Rotation Axis Object Around the Orientation to the Original z Axis Position   Five transformation steps for obtaining a composite matrix for rotation about an arbitrary axis with the rotation axis projected onto the z axis where the components a b and c of unut vector u are the direction cosines for the rotation axis  a BO p22 ce BOR 16 iv iv  v  If the rotation is to be in the opposite direction clockwise when viewing from P  to P  then we would reverse axis vector V and unit vector u so that they point from P to P  The first step in the transformation sequence for the desired rotation is to set up the translation matrix that repositions the rotation axis so that it passes through the coordinate origin For the desired direction of rotation   we accomplish this by moving point P to the origin  If the rotation direction had been specified in the opposite direction we would move P to the crigin  This translation matrix is  x   y T 91  which repositions the rotation axis and the object as shown in    Section Rotation   An axis of rotation dashed line defined with points P and P  The direction for the unit axis vector u is determined by the specified rotation direction   Translation of the rotation axis to the coordinate ongin    Rotation of u around the x axis into the xz plane is accomplished by rotating u  the projection of u in the yz plane through angle a onto the axis   a tb   Unit vector u is rotated about the x axis to bring it into the xz plane a  then it is rotated around the y axis to align it with the z axis b  Now we need the transformations that will put the rotation axis on the z axis We can use the coordinate axis rotations to accomplish this alignment in two steps There are a number of ways to perform the two steps We will first ro tate about the x axis to transform vector u into the xz plane Then we swing u around to the z axis using a y axis rotation These two rotations are illustrated in  for one possible orientation of vector u  Since rotation calculations involve sine and cosine functions we can use standard vector operations Appendix A to obtain elements of the two rotation matrices Dot product operations allow us to determine the cosine terms and vector cross products provide a means for obtaining the sine terms  We establish the transformation matrix for rotation around the x axis by de termining the values for the sine and cosine of the rotation angle necessary to get u into the xz plane This rotation angle is the angle between the projection of u in the yz plane and the positive z axis   lf we designate the projection of u in the yz plane as the vector u   b c  then the cgsine of the rotation angle a can be determined from the dot product of u and the unit vector u along the z axls  cosa  EE lu fu d thi Gs where d is the magnitude vf u  d Vb   iff Similarly we can determine the sine of a from the cross product of w and u  The coordinate independent form of this cross product is w Xu  u lu ju sine 269 and the Cartesian form for the cross product gives us wXu u Wie 2k Equating the right sides of Eqs  20 and 21 and noting that lu   and lu  d we have dsina h or Section Rotation sing  22  Ri Now that we have determined the values for cosa and sina in terms of the com ponents of vector u we can set up the matrix for rotation of u about the x axis  0 _ c d b d a2 RL  ya cfd 0 This matrix rotates unit vector u about the x axis into the xz plane  Next we need to determine the form of the transformation matrix that will swing the unit vector in the xz plane counterclockwise around the y axis onto the positive axis The orientation of the unit vector in the xz plane after rotation about the x axis is shown in  This vector labeled u  has the value a for its x component since rotation about the x axis leaves the x component un changed Its z component is d the magnitude of u  because vector u has been rotated onto the z axis And the y component of u is because it now lies in the xz plane Again we can determine the cosine of rotation angle from expres sions for the dot product of unit vectors u and u   uw u cosB   24   fu  lu   Rotation of unit vector u since lu   lu   Comparing the coordinate independent form of the cross vector u after rotation into product the xz plane about the y axis Positive rotation angle B wu Xu  uly lu sing 1f aligns u with vector u  with the Cartesian form u Xu  uy  a if 26 we find that sing  a 27  Thus the transformation matrix for rotation af u about the y axis is d a Qo _ R A a ad i t 2e 1 With transformation matrices 17 23 and 28 we have aligned the rotation axis with the positive axis The specified rotation angle  can now be applied as a rotation about the z axis Al Chapter Three Dimensianal Geometric and Modeling Transformations   Q  wosu uw a   Local coordinate system for a rotation axis defined by unit vector u  Al8 cos  sind sind cos   RC   29  1 To complete the required rotation abou the given axis we need to trans form the rotation axis back to its original position This is done by applying the inverse of transformations 17 23 and 28 The transformation matrix for ratation about an arbitrary axis then can be expressed as the composition of these seven individual transformations  R  TR Mab Ry 4A RLC8 RB Ra  T 30 A somewhat quicker but perhaps less intuitive method for obtaining the composite rotation matrix R  R a is to take advantage of the form of the composite matrix for any sequence of three dimensional rotations  Thoth Ma R my tm tm O 31 Tr Tm 0 The upper left by submatrix of this matrix is orthogonal This means that the tows or the columns of this submatrix form a set of orthogonal unit vectors that are rotated by matrix R onto the x y and z axes respectively  ry ry ry r     R  fa i  R   Ia  farsa na ra Pay 1 I Therefore we can consider a local coordinate system defined by the rotation axis and simply form a matrix whose columns are the loca unit coordinate vec tors Assuming that the rotation axis is not parallel to any coordinate axis we can form the following local set of unit vectors    uj u Ux uy 33  u Xu u  uy X uz If we express the elements of the local unit vectors for the rotation axis as uy  jy Ua Uys Uy  ty Wyay Kya  ur  uy Wap Hea  then the required composite matrix equal to the product R   R a  is Uy Uy May x2 Uy Win Hy  R  Se Me 35 Wy Uy Wy 0  This matrix transforms the unit vectors uz u  and u onto the x y and z axes re spectively Thus the rotation axis is aligned with the z axis since uy  u  Rotations with Quaternions A more efficient method for obtaining rotation about a specified axis is to use a quaternion representation for the rotation transformation In Chapter we dis cussed the usefulness of quaternions for generating three dirnensional fractals using self squaring procedures Quaternions are useful also in a number of other computer graphics procedures including three dimensional rotation calcula tions They require less storage space than by matrices and it is simpler to write quaternion procedures for transformation sequences This is particularly important in animations that require complicated motion sequences and motion interpolations between two given positions of an object  One way to characterize a quatemion Appendix A is as an ordered pair consisting of a scalar part and a vector part  q  s v We can also think of a quaternion as a higher order complex number with one real part the scalar part and three complex parts the elements of vector v  A rotation about any axis passing through the coordinate origin is performed by first setting up a unit quaternion with the following scalar and vector parts   cO85 v usin  36  where u is a unit vector along the selected rotation axis and  is the specified ro  tation angle about this axis   Any point position P to be rotated by this quaternion can be represented in quaternion notation as P W p  with the coordinates of the point as the vector part p  x y z  The rotation of the point is then carried out with the quaternion operation P qPq  37 where q   s v is the inverse of the unit quaternion q with the scalar and vec  tor parts given in Eqs 36 This transformation produces the new quaternion with scalar part equal to  P  p  38  and the vector part is calculated with dot and cross products as Section Rotation  Ge  a x Fa   Unit quaternion parameters and u for rotation about a specified axis  Chapter Three Dimensional Geametnc and Madeling Transformations  po spt vip ey  2slv Xp  v  vy x pl 39  Parameters s and v have the rotation values given in Eqs 36 Many computer graphics systems use ethcient hardware implementations of these vector calcula tions to perform rapid three dimensional object rotations Transformation 37 is equivalent to rotation about an axis that passes through the coordinate origin This is the same as the sequence of rotation trans formations in Eq 30 that aligns the rotation axis with the z axis rotates about z and then returns the rotation axis to its original position  Using the definition for quaternion multiplication given in Appendix A and designating the components of the vector part of g as v   b c  we can evaluate the terms in Eg 39 to obtain the elemen s for the composite rotation matrix R  a Ri  R    R   R a in a by form as  2b 2ab  2s 2ac  2sh Mp9  2ab  2se  2a  2c 2hc  2sa 40 2ac  2sb 2he  2sa  2a  2b  To obtain the complete general rotation equation 30 we need to include the translations that move the rotation axis to the coordinate axis and back to ils orig inal position That is  Ri   T  M  T 14n  As an example we can perform a rotation about the axis by setting the unit quaternion parameters as 2s g   sine  COS v   sin  where the quaternion vector elements are a  b  Q and c  sin   Substitut ing these values into matrix 40 and using the following trigonometric identi ties ee a   sings   2sin  cos sin  sing we get the by version of the z axis rotation matrix R   in transformation equation Similarly substituting the unit quaternion rotation values into the transformation equation 37 produces the rotated coordinate values in Eqs   The matrix expression tor the scaling transformation of a position P  x y z rel ative to the coordinate urigin can de written as Ae ee le  Doubling the size of an object with transformation 42 also moves the x a object farther from the origin x s x yyap   Ody 42 z 8s O Zz  o 1 or P S P 43  where scaling parameters s  s  and s are assigned any positive values Explicit expressions for the coordinate transformations for scaling relative to the origin are xX  xs yY y Sy Z  44  Scaling an object with transformation 42 changes the size of the object and repositions the object relative to the coordinate origin Also if the transfor mation parameters are not all equal relative dimensions in the object are changed We preserve the original shape of an object with a uniform scaling s   s     The result of scaling an object uniformly with each scaling parameter set to is shown in   Scaling with respect to a selected fixed position x y z  can be represented with the following sransformation sequence  Translate the fixed point to the origin  Scale the object relative to the coordinate origin using Eq 42 Translate the fixed point back to its original position  This sequence of transformations is demonstrated in  The matrix repre sentation for an arbitrary fixed point scaling can then be expressed as the con catenation of these translate scale translatc transformations as s O U sy x O s O U sy y O s GA s z   TO yu SGy Sy Ta xe yp   451 We form the inverse scaling matrix for either Eq 42 or Eq 45 by re placing the scaling parameters  and s with their reciprocals The inverse ma  Xp Yer Zp  fa  e Xp  e Ze  ic     xp Ver Ze x  d   Figure 1T Scaling an object relative to a selected fixed point is equivalent to the sequence of transformations shown Chapter Three Dinensional Geometric and Modeling Transformations  trix generates an opposite scaling transformation so the concatenation of any scaling matrix and its inverse produces the identity matrix  In addition to translation rotation and scaling there are various additional transformations that are often useful in three dimensional graphics appiications Two of these are reflection and shear  Reflections A three dimensional reflection can be performed relative to a selected reflection axis or with respect to a selected reflection plane In general three dimensional re flection matrices are set up similarly to those for two dimensions Reflections rel ative to a given axis are equivalent to rotations about that axis Reflections with respect to a plane are equivalent to rotations in four dimensional space When the reflection plane is a coordinate plane either xy xz or yz  we can think of the transformation as a conversion between left handed and right handed sys tems  An example of a reflection that converts coordinate specifications from a right handed system to a left handed system or vice versa is shown in Fig 19 This transformation changes the sign of the z coordinates leaving the x and y coordinate values unchanged The matrix representation for this reflection of points relative to the xy plane is 0 RF   46   0 Qo  Transformation matrices for inverting x and y values are defined similarly as reflections relative to the yz plane and xz plane respectively Reflections about other planes can be obtained as a combination of rotations and cvordinate plane reflections  Reflection y Relative to the  xy Plane woe a x x   Conversion of coordinate specifications from a right handed to a left handed system can be carried out with the reflection transformation 46 Shears Section     omposite Transformations Shearing transformations can be used to modify object shapes They are also use  ful in three dimensional viewing for obtaining general projection transforma tions In two dimensions we discussed tranformations relative to the x or y axes to produce distortions in the shapes of objects In three dimensions we can also generate shears relative to the z axis  Asan example of three dimensional shearing the following transformation produces a z axis shear  hi 4o  clo co co  oO Parameters a and b can be assigned any real values The effect of this transforma tion matrix is to alter x and y coordinate values by an amount that is propor tional to the z value while leaving the z coordinate unchanged Boundaries of planes that are perpendicular to the z axis are thus shifted by an amount propor tional to z An example of the effect of this shearing matrix on a unit cube is shown in  for shearing values a    Shearing matrices for the  axis and y axis are defined similarly   b woe  COMPOSITE TRANSFORMATIONS igure F120 unit cube a is sheared      b by transformation matrix As with two dimensional transformations we form a composite three dimen  47 witha b  sional transformation by multiplying the matrix representations for the individ ual operations in the transformation sequence  his concatenation is carried out from right to left where the rightmost matrix is the first transformation to be ap plied to an object and the leftmost matrix is the las transformation The follow ing program provides an example for implementing a composite transformation A sequence of basic three dimensional geometric transformations are combined to produce a single composite transformation which is then applied to the coor dinate definition of an object  include math h  include ygraph ics h  define PF  typedef fioat Matrixdxd 4a   Matrix4xd themMatrix  void matrixdx4Seridentity Matirixdx4 m  mirj c   r       Multiplies matrix a times b putting result in b  void matrix4x4PreMultiply Matrix4x4 a Matrix4x4 b   int r   Matrix4 tmp  for r r r  for c c c  cmp rjlc  alr  O bid ci  alr  iitb t te  alr   b c  alr   b c  for r r r  for c eva crt blrife  tmelri ct    veic translate3 float tx float ty float tz   Matrix4x4d m  matrix4xdSetIdertity m  mf   tx m   ty ml2   tz matrix4x4PreMultiply m theMatrix  void scale3 float sx float sy float sz wcePt center   Matrix4x4 m  matrix4xdSetIdentity m  m O   sx  m    sx  center x m   sy  mfl    sy  center y  m z   sz  m    sz  center z matrix4x4PreMultiply m theMatrix    void rotace jwePt3 pl wePt3 p2 float radianAngle  float lenatn  sqrt  p2 x  pl x   pZ x  pl x   p2 y  pl y   p2 y  pl y   p2 z  pl 2z   p z  pl z  float cosAzZ  cosf radianAngle   float sinAZ  sinf radianAngle   float a  sinAZ  p2 x  pl x  length float b  simAZ  p2 y  pl y  length float c  sinA2  p2 z  pl z  length Matrix4x4 m  translate3  pl x  pl y  pl z  matrix4x4Setidentity im  m O   b b  ctc  m O l  atb  cosA2 c m Q   ate  cosA2 b m i jOl  a b  casA2 c m J  lL  L  ata  e te m l   b e  cosA2 a m    atc  cosA2 b  m   btc  cosA2 a m    a a  b b Matrix4xdPreMultiply m theMatrix  translate3 pl x pl y pl z    void transformPsints3 int nPts wcPt3  pts   int k Jj  float tmp  for ik k nPts k   for j j j   theMatrix j   pts k  theMatrix j   setWePt3  pts k  tmp  tmp  tmpiz    void main int argc char  argv   wePt3 pts    10 10 50 50 10  wePt3 pl   10  p2   10  wePt3 refPt   0   long windowID  openGraphics  argv   setBackground WHITE  setColor BLUE  pPolyline3  pts  matrix4x4SetIdentity theMatrix  rotate3 pl p2 PI  scale3 75 75  refPt  translate    transformPoints3  pts  setColor RED  pPolyline3  pts   sleep   closeGraphics windowID    We set up matrices for modeling and other transformations with functions sim  lar to those given in Chapter for two dimensional transformations The major difference is that we can now specify rotations around any coordinate axis These functions are translate  translateVector matrixTranslate rotateX ithetaX xMatrixRotate  rotateyY ithetaY yMatrixRotate  rotatezZ thetaZ zMatrixRotate  scale3 scaleVector matrixScale  tmp i  theMatrix j   pts k  x  theMatrix j   pts k y  Chapter Three Dimensianal Geometric and Modeling Transformations Each of these functions produces a by transformation matrix that can then be used to transform coordinate positions expressed as homogeneous column vec tors Parameter translateVector is a pointer to the list of translation distances t  and   Similarly parameter scaleVector specifies the three scaling para Meters S  Sy and s  Rotate and scale matrices transform objects with respect to the coordinate origin And we can constrict composite transformations with the functions composeMatrix3 buildTransformarionMatrix3 composeTransformrat 1onMatrix3 which have parameters similar to two dimensional transformation functions for setting up composite matrices except we can now specify three rotation angles The order of the transformation sequence for the buildTransformationMa trix3 and composeTransformationMatrix3 functions is the same as in two dimensions  scale  rotate and  translate  Once we have specified a transformation matrix we can apply the matrix to specified points with transformFoint  inPoint matrix outPo nt  In addition we can set the transformations for hierarchical constructions using structures with the function setLocalTranstormation3 matrix type  where parameter matrix specifies the elements of a by transformation ma trix and parameter type can be assigned one of the following three values pre concatenate postconcatenaty or replace  So far we have discussed three dimensignal transformations as operations that move objects from one position to another within a single reference frame There are many tines however when we are interested in switching coordinates from one system to anc ther General three dimensional viewing procedures for exam ple involve an initial transformation of world coordinate descriptions to a view ing oordinate system Then viewing coordinates are transformed to device coor dinates And in modeung objects are often described in a local modeling coordinate reference frame then the objects are repositioned into a wortd coordi nate scene For example tables chairs and other furniture each defined in a local modeling coordinate system can be placed into the description of a room defined in another reference frame by transforming the furniture coordinates to roam coordinates Then the room might be transformed into a larger scene con structed in world coordinates  An example of the use of multiple coordinate systems and hierarchical modeling with three dimensional objects is given in  This figure illus trates simulation of tractey movement As the tractor moves the tractor coordi nate svstem and front wheel coordinate system move in the world coordinate Tractor Yow Zz World xe  Zz Xt  Eront Wheel System   Possible coordinate systems used in simulating tractor movement Wheel rotations are described in the front wheel system Turning of the tractor is described by a rotation of the front wheel system in the tractor system Both the wheel and tractor reference frames move in the world coordinate system  system The front wheels rotate in the wheel system and the wheel system ro tates in the tractor system when the tractor turns  Three dimensiona objects and scenes are constructed using structure or segment operations similar to those discussed in Chapter Modeling transfor mation functions can be applied to create hierarchical representation for three di mensional objects We can define three dimensional object shapes in local mod eling coordinates then we construct a scene or a hierarchical representation with instances of the individual objects That is we transform object descriptions from modeling coordinates to world coordinates or tu another system in the hierarchy An example of a PHIGS structure hierarchy is shown in  This display was generated by the PHIGS Toolkit software developed at the University of A upper _tody ee  Sees esa   Pin ns  ef Displaying an object hierarchy chin met using the PHIGS Toolkit package a i developed at the University of Manchester The displayed object tree is itself a PHIGS structure  Courtesy of T L   Howard J G Williams and W T Hewitt Department of Computer Science University of Manchester United Kingdom   right leer _aren gt_em en  Section Modcling and Coordinate Transformations    Three dimensional modeling  a A ball and stick representation for key amino acid residues interacting with the natural substrate of Thymidylate Synthase modeled and rendered by Julie Newdull UCSF Computer Graphics Lab  b A CAD model showing individual engine components rendered by Ted Malone FTI 3D Magic  Courtesy of Silican Graphics Inc  Manchester to provide an editor windows menus and other interface tools for PHIGS applications  shows two example applications of three dimensional modeling  Coordinate descriptions of objects are transferred from one system to an other with the same procedures used to obtain two dimensional coordinate transformations We need to set up the transformation matrix that brings the two coordinate systems into alignment First we set up a translation that brings the new coordinate origin to the position of the other coordinate origin This is fol lowed by a sequence of rotations that corresponding coordinate axes If different scales are used in the two coordinate systems a scaling transformation may also be necessary to compensate or the differences in coordinate intervals  If a second coordinate system is defined with origin x9 Yo Zp arid unit axis vectors as shown in  relative to an existing Cartesian reference frame we first construct the translation matrix T  x9  o   Next we can use the unit axis vectors to form the coordinate rotation matrix   Transformation of an object description from one coordinate system to another    Ur a My  R  Me 48 uy up wy 01 which transforms unit vectors u  uy and u onto the x y and z axes respec tively The complete coordinate transformation sequence is then given by the composite matrix R  T This matrix correctly transforms coordinate descriptions from one Cartesian system to another even if one system is left handed and the other is right handed  SUMMARY Three dimensional transformations useful in computer graphics applications in clude geometric transformations within a single coordinate system and tranfor mations between different coordinate systems The basic geometric transforma tions are translation rotation and scaling Two additional object transformations are reflections and shears Transformations between different coordinate systems are common elements of modeling and viewing routines In three dimensions transformation operations are represented with by matrices As in two di mensional graphics methods a composite transformation in three dimensions is obtained by concatenating the matrix representations for the individual compo nents of the overall transformation  Representations for translation and scaling are straightforward extensions of two dimensional transformation representations For rotations however we need more general representations since objects can be rotated about any speci fied axis in space Any three dimensional rotation can be represented as a combi nation of basic rotations around the x y and axes And many graphics pack ages provide functions for these three rotations In general however it is more efficient to set up a three dimensional rotation using either a local rotation axis teference frame or a quaternion representation Quaternions are particularly use ful for fast generation of repeated rotations that are often required in animation sequences  Reflections and shears in three dimensions can be carried out relative to any teference axis in space Thus these transformations are also more involved than the corresponding transformations in two dimensions Transforming object de scriptions from one coordinate system to another is equivalent to a transforma tion that brings the two reference frames into coincidence Finally object model ing often requires a hierarchical transformation structure that ensures that the individual components of an object move in harmony with the overall structure  REFERENCES For additional techniques involving matrices modeling and three dimensional transforma tions see Glassner  Arvo  and Kirk  A detailed discussion of quater nion rotations is given in Shoemake  Three dimensional PHIGS and PHIGS  trans formation functions are discussed in Howard et al   Gaskins  and Blake   Summary Chapter Three Dimensional Geometric and Modeling Transformations  EXERCISES    10 14  12 13 14 15  it  Prove that the muitiplication of three dimensional transformation matrices for each of the follawing sequence of operations is commutative  a Any two successive translations  b Any two successive scaling operations  c Any two successive rotations about any one of the coordinate axes   Using either Eq 30 or q 41 prove that any two successive rotations about a given rotation axis js Commutative   By evaluating the terms in Eq 39 derive the e ements for general rotation matrix given in Eq 1t   Show that rotation matrix 35 is equal to the composite matrix R G8  Rta  Prove that the quaternion rotation matrix Eq 40 reduces to the matrix representa  tion in Eg when the rotation axis is the coordinate axis   Prove that Fy 41 i6 equivalent to the general rotation transformation given in Eq 30   Write a procedure to implement general rotation transformations using the rotation matix  Write a routine ta implement quaternion rotations fg 41 for any specified axis Derive the transturmation matrix for scaling an object by a sca ing factor sin a direc tion defined by the direction angles a B and y  Develop an alganthm for scaling an object defined in an octree representation Develop a procedure for animating an object by incrementally rotating it about any specified axis Js2 appropriate approximations to the Uigonometric equations to speed up the calculations and reset the object to ts initial position after each com plete revolution about the axis Devise a procedure for rotating an object that is represented in an octree structure Develop a routine reflect an object about an arbitrarily selected plane Write a program to shear an object with respect to any of the three coordinate axes using input yalues for the shearing parameters  Develop a procedure for coaverting an object definitior in one coordinate reference to any other coordinate system defined elative to the lirst system  Develop a complete algorithm for implementing the procecures for constructive sohd modeling by combining three dimensional primitives to generate new shapes Initially the prim tives can be Combined to form subassemblies then the subassem blies can be comb ned with each other and with primil ve shapes to form the final assembly Interactive input of translation and rotation parameters can be used to po sition the objects Output of the algorithm is to he ihe sequence af operatians needed to produce the final CSG object CHAPTER  Three Dimensional Viewing     I n two dimensional graphics applications viewing operations transfer posi tions from the world coordinate plane to pixel positions in the plane of the output device Using the rectangular boundaries for the world coordinate win dow and the device viewport a two dimensional package maps the world scene to device coordinates and clips the scene against the four boundaries of the view port For three dimensional graphics applications the situation is a bit more in volved since we now have more choices as to how views are to be generated First of all we can view an object from any spatial position from the front from above or from the back Or we could generate a view of what we would see if we were standing in the middle of a group of objects or inside a single object such as a building Additionally three dimensional descriptions of objects must be pro jected onto the flat viewing surface of the output device And the clipping boundaries now enclose a volume of space whose shape depends on the type of projection we select In this chapter we explore the general operations needed to produce views of a three dimensional scene and we also discuss specific viewing procedures provided in packages such as PHIGS and GL  The steps for computer generation of a view of a three dimensional scene are somewhat analogous to the processes involved in taking a photograph To take a snapshot we first need to position the camera at a particular point in space Then we need to decide on the camera orientation   Which way do we point the camera and how should we rotate it around the line of sight to set the up di rection for the picture Finally when we snap the shutter the scene is cropped to the size of the window  aperture of the camera and light from the visible sur   Photographing a scene involves selection of a camera position and Zz orientation  Modeling    Coordinates Cocrdinates Transtormation  World Viewing  Viewing  Coordinates   Projection Workstation   Coordinates Transformation Device Coordinates  General three dimensional transformation pipeline from modeling coordinates to final device coordinates  faces is projected onto the camera film We need to keep in mind however that the camera analogy can be carried only so far since we have more flexibility and many more options for generating views of a scene with a graphics package than we do with a camera   shows the general processing steps for modeling and convert ing a world coordinate description of a scene to device coordinates Once the scene has been modeled world coordinate positions are converted to viewing co ordinates The viewing coordinate system is used in graphics packages as a refer ence for specifying the observer viewing position and the position of the projec tion plane which we can think of in analogy with the camera film plane Next projection operations are performed to convert the viewing coordinate descrip tion of the scene to coordinate positions on the projection plane which will then be mapped to the output device Objects outside the specified viewing limits are clipped from further consideration and the remaining objects are processed through visible surface identification and surface rendering procedures to pro duce the display within the device viewport   Generating a view of an object in three dimensions is similar to photographing the object We can walk around and take its picture from any angle at various distances and with varying camera orientations Whatever appears in the viewfinder is projected onto the flat film surface The type and size of the camera lens determines which parts of the scene appear in the final picture These ideas are incorporated into three dimensional graphics packages so that views of a scene can be generated given the spatial position orientation and aperture size of the camera  Specifying the View Plane We choose a particular view for a scene by first establishing the viewing coordi nate system also called the view reference coordinate system as shown in  A view plane or projection plane is then set up perpendicular to the Ww Yn my z Py  Xp  pr Zo    A right handed viewing coordinate system with axes X  Yp and  relative toa warld coordinate scene  View Plane N  a  View Plane  b    Orientations of the view plane for specified norma vector coordinates relative to the world origin Position   orients the view plane as in a  while   gives the orientation in b    Specifying the view up vector with a twist angle   viewing z axis We can think of the view plane as the film plane in a camera that has been positioned and onented for a particular shot of the scene World coordi nate positions in the scene are transformed to viewing coordinates then viewing coordinates are projected onto the view plane  To establish the viewing coordinate reference frame we first pick a world coordinate position called the view reference point This point ts the origin of our viewing coordinate system The view reference point is often chosen to be close to or on the surface of some object in a scene But we could also choose a point that is at the center of an object or at the center of a group of objects or somewhere out in front of the scene to be displayed If we choose a point that is near to or on some object we can think of this point as the position where we might want to aim a camera to take a picture of the object Alternatively if we choose a point that is at same distance from a scene we could think of this as the camera position  Next we select the positive direction for the viewing z axis and the orien tation of the view plane by specifying the view plane normal vector N We choose a world coordinate position and this point establishes the direction for N relative either to the world origin or to the viewing coordinate origin Graphics packages such as GKS and PHIGS for example orient N relative to the world coordinate origin as shown in  The view plane normal N is then the di rected line segment from the world origin to the selected coordinate position In other words N is simply specified as a world coordinate vector Some other packages GL from Silicon Graphics for instance establish the direction for N using the selected coordinate position as a look at point relative to the view refer ence point viewing courdinate origin   illustrates this method for defining the direction of N which is from the look at point to the view reference point Another possibility is to set up a left handed viewing system and take N and the positive  axis trom the viewing origin to the look at point Only the di rection of N is nveded to establish the z direction the magnitude is irrelevant because N will be normalized tu a unit vector by the viewing calculations  Finally we choose the up direction for the view bv specifying a vector V called the view up vector This vector is used to establish the positive direction for the y axis Vector V also can be defined as a world coordinate vector or in some packages it is specified with a fis angle  about the z axis as shown in  For a general crientation of the normal vector it can be difficult or at least time consuming to deternune the direction for V that is precisely perpen dicular to N Therefore viewing procedures typically adjust the user defined ori entation of vector V as shown in  so that V is projected into a plane that is perpendicular to the normal vector We can choose the view up vector V to be in any convenient direction as long as it is not parallel to N As an example con mee Orientation of the view plane tora specified look at poin P relative ta the viewing coordinate origin Py  P ee  ee   a sider an interactive specification of viewing reference coordinates using PHIGS where the view reference point is often set at the center of an object to be viewed If we then want to view the object at the angled direction shown in  we can simply choose V as the world vector   and this vector will be projected into the plane perpendicular to N to establish the y axis This is much easier than trying to input a vector that is exactly perpendicular to N  Using vectors N and V the graphics package can compute a third vector U perpendicular to both N and V to define the direction for the x axis Then the di rection of V can be adjusted so that it is perpendicular to both N and U to estab lish the viewing y direction As we will see in the next section Transformation from World to Viewing Coordinates  these computations are conveniently car tied out with unit axis vectors which are also used to obtain the elements of the world to viewing coordinate transformation matrix The viewing system is then often described as a uvn system    Generally graphics packages allow users to choose the position of the view plane with some restrictions along the z axis by specifying the view plane dis tance from the viewing origin The view plane is always parallel to the x y plane and the projection of objects to the view plane correspond to the view of the scene that will be displayed on the output device  gives examples of view plane positioning If we set the view plane distance to the value the x y plane or uv plane af viewing coordinates becomes the view plane for the projec tion transformation Occasionally the term uv plane is used in reference to the viewing plane no matter where it is positioned in relation to the x y plane But we will only use the term uv plane to mean the x y plane which is not neces sarily the view plane  Left handed viewing coordinates are sometimes used in graphics packages so that the viewing direction is in the positive z direction But right handed viewing systems are more common because they have the same orientation as the world reference frame This allows graphics systems to deal with only one coordinate orientation for both world and viewing references We will follow the convention of PHIGS and GL and use a right handed viewing system for all al gorithm development  To obtain a series of views of a scene we can keep the view reference point fixed and change the direction of N as shown in  This corresponds to generating views as we move around the viewing coordinate origin In interac     Fiyure 10 View plane positioning along the z  axis  NA Adjusted vote P N   Adjusting the input position of the view up vector V toa position perpendicular to the normal vector N  Desired  yup Direction   Choosing V along the y  axis sets the up orientation for the view plane in the desired direction    A right handed viewing system defined with unit vectors u v and n  Chapter v Three Dimensiona Viewing    Viewing a scene from different directions with a fixed view reference point  tive applications the normal vector N is the viewing parameter that is most often changed By changing only the direction of N we can view a scene from any di rection except along the line of V To obtain either of the two possible views along the line of V we would need to change the direction of V If we want to simulate camera motion through a scene we can keep N fixed and move the view reference point around    Yo   Re  Po    Py  N    Moving around in a scerie by N  vo changing the position of the view reference point  Transtormation trom World to Viewing Coordinates Before object descriptions can be projected to the view plane they must be trans ferred to viewing coordinates Conversion of object descriptions from world to viewing coordinates is equivalent to a transformation that superimposes the viewing reference frame onto the world frame using the basic geometric trans late rotate operations discussed in Section This transformation sequence is J Translate the view reference point to the origin of the world coordinate sys tem  Apply rotations to align the x  y  and z axes with the world x  y  and z  axes respectively  Hf the view reference point is specified at world position x Yy Zo  this point is translated to the world origin with the matrix transformation  x  T   Z 1 The rotation sequence can require up to three coordinate axis rotations de pending on the direction we choose for N In general if N is not aligned with any world coordinate axis we can superimpose the viewing and world systems with the transformation sequence R  R  R  That is we first rotate around the world x axis to bring z into the x z  plane Then we rotate around the world y  axis to align the z  and z axes The final rotation is about the z  axis to align the y  and y axes Further if the view reference system is left handed a reflection of one of the viewing axes for example the z axis is also necessary  illus trates the general sequence of translate rotate transformations The composite transformation matrix is then applied to world coordinate descriptions to trans fer them to viewing coordinates  Another method for generating the rotation transformation matrix is to cal culate unit uum vectors and form the composite rotation matrix directly as dis   Aligning a viewing system with the world coordinate axes using a sequence of translate rotate transformations  Section Viewmg Coordinales Chapter cussed in Section Given vectors N and V these unit vectors are calculated Three Dimensional Viewing as n TNT  ny Mg M3 VXxN  u Tv xn  iy a u   ven X u  vy   This method also automatically adjusts the direction for V so that v is perpendic ular to n The composite rotation matrix for the viewing transformation is then Uy My iy Dv U9 D4 a My My Ny Oo Foc fc which transforms u onto the world x axis v onto the y  axis and n onto the axis In addition this matrix automatically performs the reflection necessary to transform a left handed viewing svstem onto the right handed world system  The complete world to viewing coordinate transformation matrix is ob tained as the matrix product Mwycye  R T This transformation is then applied to coordinate descriptions of objects in the scene to transfer them to the viewing reference frame  Once world coordinate descriptions of the objects in a scene are converted to viewing coordinates we can project the three dimensional objects onto the two dimensional view plane There are two basic projection methods In parallel projection coordinate positions are transformed to the view plane along parallel lines as shown in the example of  For a perspective projection Fig 15  object positions are transformed to the view plane along lines that con verge to a point called the projection reference point or center of projection  The projected view of an object is determined bv calculating the intersection of the projection lines with the view plane  View Plane  Parallel projectian cf an abject to the view plane View P Plane   Projection Reference Point   Perspective projection of an object to the view plane  A parallel projection preserves relative proportions of objects and this is the method used in drafting to produce scale drawings of three dimensional ob jects Accurate views of the various sides of an object are obtained with a parallel projection but this does not give us a realistic representation of the appearance of a three dimensiona object A perspective projection on the other hand produces realistic views but does not preserve relative proportions Projections of distant objects are smaller than the projections of objects of the same size that are closer to the projection plane    Parallel Projections We can specify a parallel projection with a projection vector that defines the di rection for the projection lines When the projection is perpendicular to the view plane we have an orthographic parallel projection Otherwise we have an oblique parallel projection  illustraces the two types of parallel pro jections Some graphics packages such as GL on Silicon Graphics workstations do not provide for oblique projections In this package for example a parallel projection is specified by simply giving the boundary edges of a rectangular par allelepiped  Projection Reference Point View Plane Tigure 16 Perspective projection of equal sized objects at different distances from the view plane  Section Projections vhapier  hree Pimens caal Wew ing Ortnographie Proyction Ablique Projection fas th  Figure 17 Orientation of the projection vector V to produce an orthographic projection a and an oblique projection b  Orthographic projections are most often used to produce the front side and top views of an object as shown in  Front side and rear orthographic projections of an object are called elevations and a top orthographic projection is called a plan view Engineering and architectural drawings commonly employ these orthographic projections because lengths anc angles are accurately de picted and can be measured from the drawings  We can also form orthographic projections that display more than one face of an object Such views are called axonometric orthographic projections The most commonly used axonometric projection is the isometric projection We gen erate an isometric project on by aligning the projection plane so that it intersects each coordinate axis in which the object is defined called the principal axes at the same distance fram the origin  shows an isometric projection for a Pian View  Side Elevation View  Front Elevation View Fagyre 18 Orthographte projections 1m object displaying plan and clevation views    Isometric projection for a cube  cube The isometric projection is obtained by aligning the projection vector with the cube diagonal There are eight positions one in each octant for obtaining an isometric view All three principal axes are foreshortened equally in an isometric projection so that relative proportions are maintained This is not the case in a general axonometric projection where scaling factors may be different for the three principal directions  Transformation equations for an orthographic parallel projection are straightforward If the view plane is placed at position z  along the z axis Fig 20  then any point x y  in viewing coordinates is transformed to projection coordinates as pa Y y  where the original z coordinate value is preserved for the depth information needed in depth cueing and visible surface determination procedures  An oblique projection is obtained by projecting points along parallel lines that are not perpendicular to the projection plane In some applications packages an oblique projection vector is specified with two angles a and  as shown in  Point x y z is projected to position x  y  on the view plane Ortho graphic projection coordinates on the plane are x y  The oblique projection line from x y z to x  y  makes an angle a with the line on the projection plane that joins x  y  and x y  This line of length L is at an angle  with the horizontal direction in the projection plane We can express the projection coordinates in terms of x y L and  as    x   Z    Orthographic projection of a point onto a viewing plane  Zz  Section Projections Chapter Three Dimensional Viewing   Oblique projection of coordirate position x y z to position x  p on the view plane  x x Leosd   y  Lsing Length L depends on the angle a and the z coordinate of the point to be pro jected  tana   i  L Thus i  tana rosy  2h  where L is the inverse of tana which is also the value of L when z  We can then write the oblique projection equations as x  x  cos d a yp y  sing  The transformation matrix for producing any parallel projection onto the Xy y plane can be written as Leos O Lysing Ma arate  iD In parallel a a  o An orthographic projection is obtained when L   which occurs at a projection angle a of  Oblique projections are generated with nonzero values for Ly Projection matrix 10 has a structure similar to that of a z axis shear matrix In fact the effect of this projection matrix is to shear planes of constant and project them onto the view plane The x and y coordinate values within each plane of constant z are shifted by an amount proportional to the  value of the plane so that angles distances anc parallel lines in the plane are projected accurately This effect is shown in  where the back plane of the box is sheared and over lapped with the front plane in the projection to the viewing surface An edge of the box connecting the front and back planes is projected into a line of length L that makes an angle  with a horizontal line in the projection plane  Common choices for angle  are and  which display a combination view of the front side and top or front side and bottom of an object Two com monly used values for a are those for which tana  and tana  For the first case a  and the views obtained are called cavalier projections All lines per pendicular to the projection plane are projected with no change in length Exam ples of cavalier projections for a cube are given in   When the projection angle a is chosen so that tana  the resulting view is called a cabinet projection For this angle   lines perpendicular to the viewing surface are projected at one half their length Cabinet projections appear more realistic than cavalier projections because of this reduction in the length of perpendiculars  shows examples of cabinet projections for a cube  Perspective Projections To obtain a perspective projection of a three dimensional object we transform points along projection lines that meet at the projection reference point Suppose we set the projection reference point at position z  along the z axis and we         oe a pO   oa  vr ta   b Original Coordinete Projection on the Description of Object Viewing Plane  Figure 22 Oblique projection of a box onto the z  plane   tal ib    Cavalier projections of a cube onto a view plane for two values of angle   Note Depth of the cube is projected equal to the width and height  Section Projections      ta th   Cabinet projections of a cube onto a view plane for two values of angle  Depth is projected as one half that of the width and height  place the view plane at z  as shown in  We can write equations de scribing coordinate positions along this perspective projection line in parametric form as xo x xu yo yrye 19 zv   u Parameter u takes values from to and coordinate position x  y   repre sents any point along the projection line When u  we are at position P  x y z  At the other end of the line u  and we have the projection reference point coordinates  z   On the view plane z  z  and we can solve the z equation for parameter u at this position along the projection line  Zip  Z ys PF 12 Zprp  Z Substituting this value of u into the equations for x and y  we obtain the per spective transformation equations  P ix y  en OTs Lp Vor Zea     2p orp Perspective projection of a point Vi P with coordinates x y z to Plano position x  Yp 2p on the view plane   s e te    Zyep  Zope    i  13 vp  y 2e 2pm  2pm  where d  2p  Z pisthe distance of the view plane from the projection refer ence point  Using a three dimensional homogeneous coordinate representation we can write the perspective projection transformation 13 in matrix form as Xp 0 x Yr ool y   14 zy 2yg dy  2gZprp dp z h I d Zoe A Jn this representation the homogeneous factor is n er  15  d P and the projection coordinates on the view plane are calculated from the homo geneous coordinates as x  x h Vy  val 16  where the original z coordinate value would be retained in projection coordinates for visible surface and other depth processing  In general the projection reference point does not have to be along the z  axis We can select any coordinate position    Ypq Zpry on either side of the view plane for the projection reference point and we discuss this generalization in the next section  There are a number of special cases for the perspective transformation equations 13 If the view plane is taken to be the uv plane then z   and the projection coordinates are anes r Zpp   17 Ac Ye  e y z Pee  And in some graphics packages the projection reference point is always taken to be at the viewing coordinate origin In this case    and the projection coor  ee dinates on the viewing plane are  78  Section Projections Chapter  Three Dimensional Viewing When a three dimensional object is projected onto a view plane using per spective transformation equations any set of parallel lines in the object that are not parallel to the plane are projected into converging lines Parallel lines that are parallel to the view plane will be projected as parallel lines The point at which a set of projected parallel lines appears to converge is called a vanishing point Each such set of projected parallel lines will have a separate vanishing point and in general a scene can have any number of vanishing points depending on how many sets of parallel lines there are in the scene  The vanishing point for any set of lines that are parallel to one of the princi pal axes of an object is referred to as a principal vanishing point We control the number of principal vanishing points one two or three with the orientation of the projection plane and perspective projections are accordingly classified as one point two point or three puint projections The number of principal vanish ing points in a projection is determined by the number of principal axes intersect ing the view plane  illustrates the appearance of one point and two point perspective projections for a cube In  b  the view plane is aligned parallel to the xy object plane so that only the object z axis is intersected  Vanishing Point ft       a  Coordinate ib wnt Description ne Poin Perspective Projection    e raxis x axis   Vanishing Vanishing Point  Point  Two Point Perspective Projection   Perspective views and principal vanishing points of a cube for various orientations of the view plane relative to the principal axes of the object This orientation produces a one point perspective projection with a z axis vanish ing point For the view shown in  c  the projection plane intersects both the x and z axes but not the y axis The resulting two point perspective projection contains both x axis and z axis vanishing points  TRANSIT ORMATIONS  In the camera analogy the type of lens used on the camera is one factor that de termines how much of the scene is caught on film A wide angle lens takes in more of the scene than a regular lens In three dimensional viewing a rectangu lar view window or projection window in the view plane is used to the same effect Edges of the view window are parallel to the x y axes and the window boundary positions are specified in viewing coordinates as shown in  The view window can be placed anywhere on the view plane  Given the specification of the view window we can sel up a view volume using the window boundaries Only those objects within the view volume will appear in the generated display on an output device all others are clipped from the display The size of the view volume depends on the size of the window while the shape of the view volume depends on the type of projection to be used to generate the display in any case four sides of the volume are planes that pass through the edges of the window For a parallel projection these four sides of the view volume form an infinite parallelepiped as in  For a perspective projection the view volume is a pyramid with apex at the projection reference point   A finite view volume is obtained by limiting the extent of the volume in the direction This is done by specifying positions for one or two additional boundary planes These z boundary planes are referred to as the front plane and back plane or the near plane and the far plane of the viewing volume The front and back planes are parallel to the view plane at specified positions Zon and Both planes must be on the same side of the projection reference point and the back plane must be farther from the projection point than the front plane Including the front and back planes produces a view volume bounded by six planes as shown in  With an orthographic parallel projection the six planes form a rectangular parallelepiped while an oblique parallel projection produces an oblique parallelepiped view volume With a perspective projection the front and back cupping planes truncate the infinite pyramidal view volume to form a frustum  Front and back clipping planes allow us to eliminate parts of the scene from the viewing operations based on depth We can then pick out parts of a scene that we would like to view and exclude objects that are in front of or behind the parl that we want to look at Also in a perspective projection we can use the front clipping plane to take out large objects close to the view plane that can project into unrecognizable sections within the view window Similarly the back clip ping plane can be used to cut out objects far from the projection reference point that can project to small blots on the output device Relative placement of the view plane and the front and back clipping planes depends on the type of view we want tc generate and the limitations of a particu lar graphics package With PHIGS the view plane can be positioned anywhere along the z axis except that it cannot contain the projection reference point And View Plane   Projection Window  xine YW  Frenre 27 Window specification on the view plane with minimum and maximum coordinates given in the viewing reference system  Chapter Three Dimensional Viewing xy plane  zy window Orthographic Orthographic Projection Projection View Volume View Volume a  bb x Plane a zy window Oblique Projection Oblique View Volume Projection e View Volume  d   View volume for a parallel projection In a and b  the side and top views of the view volume for an orthographic projection are shawn and in c and d  the side and top views of an oblique view volume are shown Projection Reference Point nt x  Plane xy plane window s z A window Projaction Reference  Point a tb  c   Examples of a perspective projection view valume for various positions of the projection reference point Parallelpiped  Section View Volume  View Volumes and General Projection Transformations Back Plane   Frustum View Volume Front Plane Parallel Projection ta  Back SN  Plane  sa Projection Reference Front Point Plane Perspective Projection b   View volumes bounded by front and back planes and by top bottom and side planes Front and back planes are parallel to the view plane at positions Zpon ANd Zac along the z  axis  the front and back planes can be in any position relative to the view plane as long as the projection reference point is not between the front and back planes Figure 31 illustrates possible arrangements of the front and back planes in relation to the view plane The default view volume in PHIGS is formed as a unit cube using a parallel projection with Z  Zpack  the view plane coincident with the back plane and the projection reference point at position   on the front plane  Orthographic paralle projections are not affected by view plane position ing because the projection lines are perpendicular to the view plane regardless of  Back Plane View Plane   Back Plane  View Plane _  View Plane   Back Piane   _ Front Plane ch  xy  Front Plane   Front Plane a  b   Possible arrangements of the front and back clipping planes relative to the view plane   window   View Plane View Plane window Projection Projection Reference Reference Point Point a ib    Changing the shape of the oblique projection view volume by moving the window position when the projection vector V is determined by the projection reference point and the window position  its location Oblique projections may be affected by view plane positioning de pending on how the projection direction is to be specified In PHIGS the oblique projection direction is parallel to the line from the projection reference point to the center of the window Therefore moving the position of the view plane with out moving the projection reference point changes the skewness of the sides of the view volume as shown in  Often the view plane is positioned at the view reference point or on the front clipping plane when generating a parallel projection  Perspective effects depend on the positioning of the projection reference point relative to the view plane as shown in  If we place the projec  View View View  Window a  Window  Window  _  OOH    t i aN ao   i l  Ny o    t  a    Vue     Z I I att   Il U Projection  i i Reference   Point       a   i  f       i t    vt I     I   I     Projection Projection Reference Reference Point Very Far bi from window c   Changing perspective effects by moving the projection reference point away from the view plane window  window  ul ul a  s Projection Projection Reference Reference Point Point a  b   Projected object size depends on whether the view plane is positioned in front of the object or behind it relative to the position of the projection reference point  tion reference point close to the view plane perspective effects are emphasized that is closer objects will appear much larger than more distant objects of the same size Similarly as we move the projection reference point farther from the view plane the difference in the size of near and far objects decreases In the limit as we move the projection reference point infinitely far from the view plane a perspective projection approaches a parallel projection  The projected size of an object in a perspective view is also affected by the relative position of the object and the view plane   If the view plane is in front of the object nearer the projection reference point  the projected size is smaller Conversely object size is increased when we project onto a view plane in back of the object  View plane positioning for a perspective projection also depends on whether we want to generate a static view or an animation sequence For a static view of a scene the view plane is usually placed at the viewing coordinate ori gin which is at some convenient point in the scene Then it is easy to adjust the size of the window to include al parts of the scene that we want to view The projection reference point is positioned to obtain the amount of perspective de sired In an animation sequence we can place the projection reference point at the viewing coordinate origin and put the view plane in front of the scene Fig 35  This placement simulates a camera reference frame We set the field of view lens angle by adjusting the size of the window relative to the distance of the view plane from the projection reference point We move through the scene by moving the viewing reference frame and the projection reference point will move with the view reference point  Section View Volumes and General Projection Transformations  View Volume   Fignre 37 Regular parallelepiped view volume obtained by shearing the view volume in   Scene motion window View I Plane   View plane positioning to simulate a camera reference frame for an animation sequence  General Parallel Projection Transformations In PHIGS the direction of a parallel projection is specified with a projection vec tor from the projection reference point to the center of the view window Figure 36 shows the general shape of a finite view volume for a given projection vec tor and projection window in the view plane We obtain the oblique projection transformation with a shear operation that converts the view volume in Fig 36 to the regular parallelepiped shown in   The elements of the shearing transformation needed to generate the view volume shown in  are obtained by considering the shear transformation of the projection vector If the projection vector is specified in world coordinates it must first be transformed to viewing coordinates using the rotation matrix dis cussed in Section  The projection vector is unaffected by the translation since it is simply a direction with no fixed position  For graphics packages that allow specification of the projection vector in viewing coordinates we apply the shear directly to the input elements of the projection vector  Suppose the elements of the projection vector in viewing coordinates are Vp  Py Py Pa  19 We need to determine the elements of a shear matrix that will align the projection vector V with the view plane normal vector N   This transformation can be expressed as window Vv  a Figure 36 Oblique projection vector and associated view volume   29  where My raue is equivalent to the parallel projection matrix 10 and represents a z axis shear of the form Mparattet  21  occ  ooro oergra me OOO The explicit transformation equations from 20 in terms of shear parameters a and b are O p  ap O p  bp  22  so that the values for the shear parameters are a Ph ope 23  Thus we have the general parallel projection matrix in terms of the ele ments of the projection vector as  p p  p p 0 Myaraiel   24  ooo This matrix is then concatenated with transformation R  T from Section to produce the transformation from world coordinates to parallel projection coordi nates For an orthographic parallel projection p  p   and Myanne is the iden tity matrix From  we can relate the components of the projection vec tor to parameters L a and  Section  By similar triangles we see that L cos _ Px  Ps 25 Lsind _ _Py Zz Pz which illustrates the equivalence of the elements of transformation matrices and 24 In Eqs 25 z and p are of opposite signs and for the positions il lustrated in    Section View Volumes and General Projection Transformations  We   X51 Vp OF   Relationship between the parallel projection vector V and parameters z L a and   General Perspective Projection Transformations With the PHIGS programming standard the projection reference point can be lo cated at any position in the viewing system except on the view plane or between the front and back clipping planes  shows the shape of a finite view volume for an arbitrary position of the projection reference point We can obtain the general perspective projection transformation with the following two opera tions  Shear the view volume so that the centerline of the frustum is perpendicu lar to the view plane  Scale the view volume with a scaling factor that depends on z  The second step scaling the view volume is equivalent to the perspective trans formation discussed in Section A shear operation to align a general perspective view volume with the pro    Frustum Centerline View Volume View Plane  Xprae Vero Zoro   General shape for the perspective view volume witha projection reference point that is not on the  axis    Frustum Centerline ux ya  iy   PF  z t  I i f ifs  fa A d  r  if   uy View Plane z  z   te  i  Vd c twi  Vd enter of Window Shearing a general perspective view   volume to center it on the projection pep Ypeo pep window  jection window is shown in  This transformation has the effect of shift ing all positions that lie along the frustum centerline including the window cen ter to a line perpendicular to the view plane With the projection reference point at a general position x   Vprp Zerp e the transformation involves a combination 2z axis shear and a translation  A Alyy  Pep 26  Mohear  oo of oor where the shear parameters are Xpp  XWapin  XWmax  torn 27  p  Yew  YOmn  Ylma  Zoey Points within the view volume are transformed by this operation as xe x  az  Y Yy Ft WZ Zpy  28  z When the projection reference point is on the Z axis Xpj9  Vprp  Once we have converted a position x y  in the onal view volume to position x  y   in the sheared frustum we then appiy a scaling transformation to produce a regular parallelepiped   The transformation for this con version is Section  View Volumes and General Projection Transformations  Chapter Zz  Zz we  tr Uj Three Dimensional Viewing x x me   onl e    Zprp 2p 29 we y   Yon Zap  Zprp  2pm  and the homogeneous matrix representation is pry Aopen Zyrp  Sup Zoep Zup t TY pry Yory2up Zomp Z Zo   pry op pry up  Macaie  0 30 a te tom orp  Zep 2pm  2vp Therefore the general perspective projection transformation can be ex pressed in matrix form as Mperspective  Mycate  Mshea  31 The complete transformation from world coordinates to perspective projection coordinates is obtained by right concatenating Myerspenve With the composite viewing transformation R  T from Section  In this section we first explore the general ideas involved in three dimensional clipping by considering how clipping could be performed using the view vol ume clipping planes directly Then we discuss more efficient methods using nor malized view volumes and homogeneous coordinates   An algorithm for three dimensional clipping identifies and saves all surface segments within the view volume for display on the output device All parts of objects that are outside the view volume are discarded Clipping in three dimen sions can be accomplished using extensions of two dimensional clipping meth ods Instead of clipping against straight line window boundaries we now clip objects against the boundary planes of the view volume  To clip a line segment against the view volume we would need to test the relative position of the line using the view volume s boundary plane equations By substituting the line endpoint coordinates into the plane equation of each boundary in turn we could determine whether the endpoint is inside or outside that boundary An endpoint x y z of a line segment is outside a boundary plane if Ax  By  Cz  D O where A B C and D are the plane parameters for that boundary Similarly the point is inside the boundary if Ar  By  Cz  D  Lines with both endpoints outside a boundary plane are discarded and those with both endpoints inside all boundary planes are saved The intersection of a line with a boundary is found using the line equations along with the plane equation Intersection coordinates x y  Z  are values that are on the line and that satisfy the plane equation Ax  By  Cz  D  To clip a polygon surface we can clip the individual polygon edges First we could test the coordinate extents against each boundary of the view volume to determine whether the object is completely inside or completely outside that boundary If the coordinate extents of the object are inside all boundaries we save it If the coordinate extents are outside all boundaries we discard it Other wise we need to apply the intersection calculations We could do this by deter mining the polygon edge intersection positions with the boundary planes of the view volume as described in the previous paragraph  As in two dimensional viewing the projection operations can take place be fore the view volume clipping or after clipping All objects within the view vol ume map to the interior of the specified projection window The last step is to transform the window contents to a two dimensional viewport which specifies the location of the display on the output device  Clipping in two dimensions is generally performed against an upright rec tangle that is the dip window is aligned with the x and y axes This greatly sim plifies the clipping calculations because each window boundary is defined by one coordinate value For example the intersections of all lines crossing the left boundary of the window have an x coordinate equal to the left boundary  View volume clipping boundaries are planes whose orientations depend on the type of projection the projection window and the position of the projection reference point Since the front and back clipping planes are parallel to the view plane each has a constant z coordinate value The z coordinate of the intersec tions of lines with these planes is simply the z coordinate of the corresponding plane But the other four sides of the view volume can have arbitrary spatial ori entations To find the intersection of a line with one of the view volume bound aries means that we must obtain the equation for the plane containing that boundary polygon This process is simplified if we convert the view volume be fore clipping to a rectangular parallelepiped In other words we first perform the projection transformation which converts coordinate values in the view volume to orthographic parallel coordinates then we carry out the clipping calculations  Clipping against a regular parallelepiped is much simpler because each sur face is now perpendicular to one of the coordinate axes As seen in  the top and bottom of the view volume are now planes of constant y the sides are planes of constant x and the front and back are planes of constant z A line cut ting through the top plane of the parallelepiped for example has an intersection point whose y coordinate value is that of the top plane  In the case of an orthographic parallel projection the view volume is al ready a rectangular parallelepiped As we have seen in Section cblique pro jection view volumes are converted to a rectangular parallelepiped by the shear ing operation and perspective view volumes are converted in general with a combination shear scale transformation  y View Volume   An object intersecting a rectangular parallelepiped view volume   Section Clipping  Modeling Coordinates  Normalized View Volumes  shows the expanded PHIGS transformation pipeline At the first step a scene is constructed by transforming object descriptions from modeling coordinates to world coordinates Next a view mapping converts the world de scriptions to viewing coordinates At the projection stage the viewing coordi nates are transformed to projection coordinates which effectively converts the view volume into a rectangular parallelepiped Then the parallelepiped is mapped into the unit cube a normalized view volume called the normalized projection coordinate system The mapping to normalized projection coordi nates is accomplished by transforming points within the rectangular paral lelepiped into a position within a specified three dimensional viewport which occupies part or all of the unit cube Finally at the workstation stage normalized projection coordinates are converted to device coordinates for display The normalized view volume is a region defined by the planes x x y y z z  32  A similar transformation sequence is used in other graphics packages with indi vidual variations depending on the system The GL package for example maps the rectangular parallelepiped into the interior of a cube with boundary planes at positions in each coordinate direction  There are several advantages to clipping against the unit cube instead of the original view volume or even the rectangular parallelepiped in projection coordi nates First the normalized view volume provides a standard shape for repre senting any sized view volume This separates the viewing transformations from any workstation considerations and the unit cube then can be mapped to a workstation of any size Second clipping procedures are simplified and stan dardized with unit clipping planes or the viewport planes and additional clip ping planes can be specified within the normalized space before transforming to Modeling World Viewing Viewing Transformatian Coordinates Transformation Coordinates    age  ge Normalized Projection Prajection Normalization    c  canes Transformation Coordinates Transformation Projection Coordinates Device L Workstation  i  Transformation Coardinates eel Frenre 42   Expanded PHIGS transformation pipeline device coordinates Third depth cueing and visible surface determination are simplified since the z axis always points toward the viewer the projection refer ence point has now been transformed to the z axis  Front faces of objects are those with normal vectors having a component along the positive z direction and back surfaces are facing in the negative z direction  Mapping positions within a rectangular view volume to a three dimen sional rectangular viewport is accomplished with a combination of scaling and translation similar to the operations needed for a two dimensional window to viewport mapping We can express the three dimensional transformation matrix for these operations in the form   33  Factors D  D and D are the ratios of the dimensions of the viewport and regu lar parallelepiped view volume in the x y and z directions   XV imax  XV min  XWrin Ymax YOmin YWrrax  Yeni 34  20max  20min D  Zack  Ztront  Wm  YW nax Peace  Wrinr F min Zon m mn  Hon xVinine Y inine IVinin  Parallelepiped Section Clipping View Volume Unit Cube  a  b   Dimensions of the view volume and three dimensional viewport  Onan W maxe zy  3D Viewport mon   where the view volume boundaries are established by the window limits 7W pin XWeays Yin YWmox aNd the positions Zpont ANA Zpacx Of the front and back planes Viewport boundaries are set with the coordinate values x_n Umax YOmins Ymax ZWmine ANd ZUpax The additive translation factors K  K  and K in the transforma tion are Ky  XU pmin  XWmninD K  Ymin  YminDy K  20min  2front Dy 35  Viewport Clipping Lines and polygon surfaces in a scene can be clipped against the viewport boundaries with procedures similar to those used for two dimensions except that objects are now processed against clipping planes instead of clipping edges Curved surfaces are processed using the defining equations for the surface boundary and locating the intersection lines with the parallelepiped planes  The two dimensional concept of region codes can be extended to three di mensions by considering positions in front and in back of the three dimensional viewport as well as positions that are left right below or above the volume For two dimensional clipping we used a four digit binary region code to identify the position of a line endpoint relative to the viewport boundaries For three dimen sional points we need to expand the region code to six bits Each point in the de scription of a scene is then assigned a six bit region code that identifies the rela tive position of the point with respect to the viewport For a line endpoint at position x y z  we assign the bit positions in the region code from right to left as bit  ifx  xv   left bit  if   XVmax right bit  ify yv  below bit  if y  yupna above bit5  if z0_  front  bit  if z  2v   back  For example a region code of identifies a point as above and behind the viewport and the region code indicates a point within the volume  A line segment can be immediately identified as completely within the viewport if both endpoints have a region code of If either endpoint of a line segment does not have a region cade of we perform the logical and operation on the two endpoint codes The result of this and operation will be nonzero for any line segment that has both endpoints in one of the six outside re gions For example a nonzero value will be generated if both endpoints are be hind the viewport or both endpoints are above the viewport If we cannot iden tify a line segment as completely inside or completely outside the volume we test for intersections with the bounding planes of the volume  As in two dimensional line clipping we use the calculated intersection of a line with a viewport plane to determine how much of the line can be thrown away The remaining part of the fine is checked against the other planes and we continue until either the line is totally discarded or a section is found inside the volume  Equations for three dimensional line segments are conveniently expressed in parametric form The two dimensional parametric clipping methods of Cyrus Beck or Liang Barsky can be extended to three dimensional scenes For a line segment with endpoints P   x  y   and P    yz  we can write the parametric line equations as x x t  x u Osfuel ym t Y wu Z    u 36  Coordinates x y z represent any point on the line between the two endpoints At u  we have the point P  and u  puts us at Py  To find the intersection of a line with a plane of the viewport we substitute the coordinate value for that plane into the appropriate parametric expression of Eq 36 and solve for u For instance suppose we are testing a line against the 2Umin Plane of the viewport Then 20min  w mn  37  When the calculated value for u is not in the range from to the line segment does not intersect the plane under consideration at any point between endpoints P and P  line A in   If the calculated value for u in Eq 37 is in the interval from to we calculate the intersection s x and y coordinates as  x    n  Hae  2y  way  Oh vl an  38   If either x or y is not in the range of the boundaries of the viewport then this line intersects the front plane beyond the boundaries of the volume line B in Fig 44  Clipping in Homogeneous Coordinates Although we have discussed the clipping procedures in terms of three dimen sional coordinates PHIGS and other packages actually represent coordinate posi tions in homogeneous coordinates This allows the various transformations to be Tepresented as by matrices which can be concatenated for efficiency After all viewing and other transformations are complete the homogeneous coordinate positions are converted back to three dimensional points  As each coordinate position enters the transformation pipeline it is con verted to a homogeneous coordinate representation  x y   x y z  Section Clipping  Y nex  en    Side view of two line segments that are to be clipped against the ZU nip plane of the viewport For line A Eq 37 produces a value of u that is outside the range from to For line B Eqs 38 produce intersection coordinates that are outside the range from yup to Yrax  The various transformations are applied and we obtain the final homogeneous point  Xb ay M2 Aya Aye x h Ay An Ax Ary y y 39 zy Ax Ay Ay z h Aa 4qn gn ay where the homogeneous parameter h may not be In fact can have any real value Clipping is then performed in homogeneous coordinates and clipped ho mogeneous positions are converted to nonhomogeneous coordinates in three dimensional normalized projection coordinates   vet 40  We will of course have a problem if the magnitude of parameter H is very small or has the value but normally this will not occur if the transformations are car ried out properly At the final stage in the transformation pipeline the normal ized point is transformed to a three dimensional device coordinate point The xy position is plotted on the device and the z component is used for depth informa tion processing  Setting up clipping procedures in homogeneous coordinates allows hard ware viewing implementations to use a single procedure for both parallel and perspective projection transformations Objects viewed with a parallel projection could be correctly clipped in three dimensional normalized coordinates pro vided the value  has not been altered by other operations But perspective projections in general produce a homogeneous parameter that no longer has the value Converting the sheared frustum to a rectangular parallelepiped can change the value of the homogeneous parameter So we must clip in homoge neous coordinates to be sure that the clipping is carried out correctly Also ratio nal spline representations are set up in homogeneous coordinates with arbitrary values for the homogeneous parameter including h  Negative values for the homogeneous parameter can also be generated in perspective projections when coordinate positions are behind the projection reference point This can occur in applications where we might want to move inside of a building or other object to view its interior  To determine homogeneous viewport clipping boundaries we note that any homogeneous coordinate position x  y   is inside the viewport if it sat isfies the inequalities  XVnin  S Waa Ymin  Ym ZUmin   S2Vepax 41  2h h Thus the homogeneous clipping limits are WxVmin SX_ SMa AYVmin S Vn ShYVmax Zin SZ SANZ if  O AVenax SX_ SAVg in HYP max SV_ SHYV pine NZ pax SZ HAW ain ifh  42  And clipping is carried out with procedures similar to those discussed in the pre vious section To avoid applying both sets of inequalities in 42 we can simply negate the coordinates for any point with h and use the clipping inequalities for h   Most graphics processes are now implemented in hardware Typically the view ing visible surface identification and shacing algorithms are available as graph ics chip sets employing VLSI very large scale integration circuitry techniques Hardware systems are now designed to transform clip and project objects to the output device for either three dimensional or two dimensional applications   illustrates an arrangement of components in a graphics chip set to implement the viewing operations we have discussed in this chapter The chips are organized into a pipeline for accomplishing geometric transformations coordinate system transformations projections and clipping Four initial chips are provided for matrix operations involving scaling translation rotation and the transformations needed for converting world coordinates to projection coor dinates Each of the next six chips performs clipping against one of the viewport boundaries Four of these chips are used in two dimensional applications and the other two are needed for clipping against the front and back planes of the three dimensional viewport The last two chips in the pipeline convert viewport coordinates to output device coordinates Components for implementation of vis ible surface identification and surface shading algorithms can be added to this set to provide a complete three dimensional graphics system  Section Hardware Implementations World Coordinate Object Descriptions   Transformation Operations      Conversion to Device Coordinates    A hardware implementation of three dimensional viewing operations using chips for the coordinate transformations and clipping operations  Other specialized hardware implementations have been developed These include hardware systems for processing octree representations and for display ing three dimensional scenes using ray tracing algorithms Chapter   Several procedures are usually provided in a three dimensional graphics library to enable an application program to set the parameters for viewing transforma tions There are of course a number of different methods for structuring these procedures Here Wwe discuss the PHIGS functions for three dimensional view ing  With parameters specified in world coordinates elements of the matrix for transforming world coordinate descriptions to the viewing reference frame are calculated using the function evaluateViewOrientationMatrix3 x0 yO  xN yN ZN xV yV 2V error viewMatrix  This function creates the viewMatrix from input coordinates defining the view ing system as discussed in Section Parameters x0 y0 and specify the origin view reference point of the viewing system World coordinate vector xN yN ZN defines the normal to the view plane and the direction of the positive z  viewing axis And world coordinate vector xV yV zV gives the elements of the view up vector The projection of this vector perpendicular to xN yN zN estab lishes the direction for the positive y axis of the viewing system An integer error code is generated in parameter error if input values are not specified correctly For example an error will be generated if we set xv yV zV parallel to x YN ZN  To specify a second viewing coordinate system we can redefine some or all of the coordinate parameters and invoke evaluateViewOrientationMa trix3 with a new matrix designation In this way we can set up any number of world to viewing coordinate matrix transformations  The matrix projMatrix for transforming viewing coordinates to normal ized projection coordinates is created with the function evaluateViewMappingMatrix3 xwmin xwmax ywmnin ywmax xvmin xvmax yvmin yvmax zvmin zvmax projType xprojRef yprojkef zprojRef zview zback zfront error projMatrix  Window limits on the view plane are given in viewing coordinates with parame ters xwmin xwmax ywmin and ywmax Limits of the three dimensional viewport within the unit cube are set with normalized coordinates xvmin xvmax yvmin yvmax zyvmin and zvmax Parameter projType is used to choose the projec tion type as either parallel or perspective Coordinate position xprojRef yproj Ref zprojRef sets the projection reference point This point is used as the cen ter of projection if projType is set to perspective otherwise this point and the center of the view plane window define the parallel projection vector The posi tion of the view plane along the viewing z axis is set with parameter zview Po sitions along the viewing z axis for the front and back planes of the view volume are given with parameters zfront and zback And the error parameter re turns an integer error code indicating erroneous input data Any number of pro jection matrix transformations can be created with this function to obtain various three dimensional views and projections  A particular combination of viewing and projection matrices is selected on a specified workstation with setViewRepresentation3 ws viewIndex viewMatrix projMatrix xclipmin xclipmax yelipmin yclipmax zclipmin zclipmax clipxy clipback clipfroent  Parameter ws is used to select the workstation and parameters viewMatrix and projMatrix select the combination of viewing and projection matrices to be used The concatenation of these matrices is then placed in the workstation view table and referenced with an integer value assigned to parameter viewIndex Limits given in normalized projection coordinates for clipping a scene are set with parameters xclipmin xclipmax yclipmin yclipmax zclipmin and zclipmax These limits can be set to any values but they are usually set to the limits of the viewport Values of clip or noclip are assigned to parameters cl ipxy clipfront and clipback to turn the clipping routines on or off for the xy planes or for the front or back planes of the view volume or the defined clipping limits  Section Three Dimensionat Viewing Functions   There are several times when it is convenient to bypass the clipping rou tines For initial constructions of a scene we can disable clipping so that trial placements of objects can be displayed quickly Also we can eliminate one or more of the clipping planes if we know that all objects are inside those planes  Once the view tables have been set up we select a particular view represen tation on each workstation with the function setViewIndex viewIndex  The view index number identifies the set of viewing transformation parameters that are to be applied to subsequently specified output primitives for each of the active workstations  Finally we can use the workstation transformation functions to select sec tions of the projection window for display on different workstations These oper ations are similar to those discussed for two dimensional viewing except now our window and viewport regions aia three dimensional regions The window function selects a region of the unit cube and the viewport function selects a dis play region for the output device Limits in normalized projection cvordinates for the window are set with setWorkstationWindow3 ws xwsWindmin xwsWindmax ywsWindmin ywsWindmax zwsWindmin zwsWindmax  and limits in device coordinates for the viewport are set with setWorkstationViewport3 ws xwsVPortmin xwsVPortmax ywsVPortmin ywsVPortmax zwsVPortmin zwsVPortmax   shows an example of interactive selection of viewing parameters in the PHIGS viewing pipeline using the PHIGS Toolkit software This software was developed at the University of Manchester to provide an interface to PHIGS with a viewing editor windows menus and other interface tools  For some applications composite methods are used to create a display con sisting of multiple views using different camera orientations  shows   Using the PHIGS Toolkit developed at the University of Manchester to interactively control parameters in the viewing pipeline  Courtesy of T L J Howard   G Williams and W T Hewit  Department of Computer Science University of Manchester United Kingdom   A wide angle view for a virtual reality display generated with seven sections each from a slightly different viewing direction  Cauriesy of the National Center for Supercomputing Applications University of Illinois at Urbana Chamrpaign   a wide angle perspective display produced for a virtual reality environment The wide viewing angle is attained by generating seven views of the scene from the same viewing position but with slight shifts in the viewing direction  SUMMARY Viewing procedures for three dimensional scenes follow the general approach used in two dimensional viewing That is we first create a world coordinate scene from the definitions of objects in modeling coordinates Then we set up a viewing coordinate reference frame and transfer object descriptions from world coordinates to viewing coordinates Finally viewing coordinate descriptions are transformed to device coordinates  Unlike two dimensional viewing however three dimensional viewing re quires projection routines to transform object descriptions to a viewing plane be fore the transformation to device coordinates Also three dimensional viewing operations involve more spatial parameters We can use the camera analogy to describe three dimensional viewing parameters which include camera position and orientation A viewing coordinate reference frame is established with a view reference point a view plane normal vector N and a view up vector V View plane position is then established along the viewing z axis and object descrip tions are projected to this plane Either perspective projection or parallel projec tion methods can be used to transfer object descriptions to the view plane  Parallel projections are either orthographic or oblique and can be specified with a projection vector Orthographic parallel projections that display more than one face of an object are called axonometric projections An isometric view of an object is obtained with an axonometric projection that foreshortens each principal axis by the same amount Commonly used oblique projections are the cavalier projection and the cabinet projection Perspective projections of objects are ob tained with projection lines that meet at the projection reference point  Objects in three dimensional scenes are clipped against a view volume The top bottom and sides of the view volume are formed with planes that are paral lel to the projection lines and that pass through the view plane window edges Front and back planes are used to create a closed view volume For a parallel pro jection the view volume is a parallelepiped and for a perspective projection the view volume is a frustum Objects are clipped in three dimensional viewing by testing object coordinates against the bounding planes of the view volume Clip ping is generally carried out in graphics packages in homogeneous coordinates  Summary  after all viewing and other transformations are complete Then homogeneous co ordinates are converted to three dimensional Cartesian coordinates  REFERENCES For additional information on three dimensional viewing and clipping operations in PHIGS and PHIGS  see Howard et al   Gaskins  and Blake  Discussions of three dimensional clipping and viewing algorithms can be found in Blinn and Newell  Cyrus and Beck  Riesenfeld  Liang and Barsky  Arvo  and Blinn   EXERCISES       10  11  12  13  14 15  16  17  Write a procedure to implement the evaluateViewOrientationMatrix3 func tion using Eqs through  Write routines to implement the set ViewRepresentation3 and set ViewIndex functions  Write a procedure to transform the vertices of a polyhedron to projection coordinates using a parallel projection with a specified projection vector  Write a procedure to obtain different parallel projection views of a polyhedron by first applying a specified rotation  Write a procedure to perform a one point perspective projection of an object  Write a procedure to perform a two point perspective projection of an object Develop a routine to perform a three point perspective projection of an object  Write a routine to convert a perspective projection frustum to a regular paral lelepiped  Extend the Sutherland Hodgman polygon clipping algorithm to clip three dimen sional planes against a regular parallelepiped  Devise an algorithm to clip objects in a scene against a defined frustum Compare the operations needed in this algorithm to those needed in an algorithm that clips against a regular parallelepiped Modify the two dimensional Liang Barsky tine clipping algorithm to clip three di mensional lines against a specified regular parallelepiped  Modify the two dimensional Liang Barsky line clipping algorithm to clip a given polyhedron against a specified regular parallelepiped  Set up an algorithm for clipping a polyhedron against a parallelepiped  Write a routine to perform clipping in homogeneous coordinates  Using any clipping procedure and orthographic parallel projections write a program to perform a complete viewing transformation from world coordinates to device co ordinates  Using any clipping procedure write a program to perform a complete viewing trans formation from world coordinates to device coordinates for any specified parallel projection vector  Write a program to perform all steps in the viewing pipeline for a perspective trans formation  CHAPTER Visible Surface Detection Methods      A major consideration in the generation of realistic graphics displays is identifying those parts of a scene that are visible from a chosen viewing position There are many approaches we can take to solve this problem and nu merous algorithms have been devised for efficient identification of visible objects for different types of applications Some methods require more memary some in volve more processing time and some apply only to special types of objects De ciding upon a method for a particular application can depend on such factors as the complexity of the scene type of objects to be displayed available equipment and whether static or animated displays are to be generated The various algo rithms are referred to as visible surface detection methods Sometimes these methods are also referred to as hidden surface elimination methods although there can be subtle differences between identifying visible surfaces and eliminat ing hidden surfaces For wireframe displays for example we may not want to actually eliminate the hidden surfaces but rather to display them with dashed boundaries or in some other way to retain information about their shape In this chapter we explore some of the most commonly used methods for detecting visi ble surfaces in a three dimensional scene  ALGORITHMS Visible surface detection algorithms are broadly classified according to whether they deal with object definitions directly or with their projected images These two approaches are called object space methods and image space methods re spectively An object space method compares objects and parts of objects to each other within the scene definition to determine which surfaces as a whole we should label as visible In an image space algorithm visibility is decided point by point at each pixel position on the projection plane Most visible surface algo rithms use image space methods although object space methods can be used ef fectively to locate visible surfaces in some cases Line display algorithms on the other hand generally use object space methods to identify visible lines in wire frame displays but many image space visible surface algorithms can be adapted easily to visible line detection  Although there are major differences in the basic approach taken by the var ious visible surface detection algorithms most use sorting and coherence meth ods to improve performance Sorting is used to facilitate depth comparisons by ordering the individual surfaces in a scene according to their distance from the view plane Coherence methods are used to take advantage of regularities in a scene An individual scan line can be expected to contain intervals runs of con stant pixel intensities and scan line patterns often change little from one line to the next Animation frames contain changes only in the vicinity of moving ob jects And constant relationships often can be established between objects and surfaces in a scene  A fast and simple object space method for identifying the back faces of a polyhe dron is based on the inside outside tests discussed in Chapter A point x y  is inside a polygon surface with plane parameters A B C and D if Ax  By Cz D  When an inside point is along the line of sight to the surface the polygon must be a back face we are inside that face and cannot see the front of it from our viewing position  We can simplify this test by considering the normal vector N toa polygon surface which has Cartesian components A B C  In general if V is a vector in the viewing direction from the eye or camera  position as shown in  then this polygon is a back face if V N O  Furthermore if object descriptions have been converted to projection coordinates and our viewing direction is parallel to the viewing z axis then V   V  and V N V C so that we only need to consider the sign of C the z component of the normal vector N In a right handed viewing system with viewing direction along the nega tive z axis   the polygon is a back face if C  Also we cannot see any face whose normal has component C  since our viewing direction is grazing that polygon Thus in general we can label any polygon as a back face if its nor mal vector has a z component value  C 33 N ABC    Vector  in the viewing direction and a back face normal vector N of a polyhedron  Section Back Face Detection     View of a concave polyhedron with one face partially hidden by other faces    N A B C  A polygon surface with plane parameter C  Oina right handed  viewing coordinate system is Vv identified as a back face when the viewing direction is along the     negative axis  Similar methods can be used in packages that employ a left handed view ing system In these packages plane parameters A B C and D can be calculated from polygon vertex coordinates specified in a clockwise direction instead of the counterclockwise direction used in a right handed system  Inequality then remains a valid test for inside points Also back faces have normal vectors that point away from the viewing position and are identified by C  when the viewing direction is along the positive z axis  By examining parameter C for the different planes defining an object we can immediately identify all the back faces For a single convex polyhedron such as the pyramid in  this test identifies all the hidden surfaces on the ob ject since each surface is either completely visible or completely hidden Also if a scene contains only nonoverlapping convex polyhedra then again all hidden surfaces are identified with the back face method  For other objects such as the concave polyhedron in  more tests need to be carried out to determine whether there are additional faces that are to tally or partly obscured by other faces And a general scene can be expected to contain overlapping objects along the line of sight We then need to determine where the obscured objects are partially or completely hidden by other objects In general back face removal can be expected to eliminate about half of the polygon surfaces in a scene from further visibility tests  A commonly used image space approach to detecting visible surfaces is the depth buffer method which compares surface depths at each pixel position on the projection plane This procedure is also referred to as the z buffer method since object depth is usually measured from the view plane along the z axis of a viewing system Each surface of a scene is processed separately one point at a time across the surface The method is usually applied to scenes containing only polygon surfaces because depth values can be computed very quickly and the method is easy to implement But the method can be applied to nonplanar sur faces  With object descriptions converted to projection coordinates each x y  position on a polygon surface corresponds to the orthographic projection point x y on the view plane Therefore for each pixel position x y on the view plane object depths can be compared by comparing z values  shows three surfaces at varying distances along the orthographic projection line from position x y in a view plane taken as the x y plane Surface is closest at this position so its surface intensity value at x y is saved  We can implement the depth buffer algorithm in normalized coordinates so that z values range from at the back clipping plane to   at the front clip At view plane position x y  surface S has the smallest depth from the view plane and so is visible at that position  d  A a ok  ping plane The value of z   can be set either to for a unit cube or to the largest value that can be stored on the system  As implied by the name of this method two buffer areas are required A depth buffer is used to store depth values for each x y position as surfaces are processed and the refresh buffer stores the intensity values for each position Ini tially all positions in the depth buffer are set to minimum depth  and the re fresh buffer is initialized to the background intensity Each surface listed in the polygon tables is then processed one scan line at a time calculating the depth z value at each x y pixel position The calculated depth is compared to the value previously stored in the depth buffer at that position If the calculated depth is greater than the value stored in the depth buffer the new depth value is stored and the surface intensity at that position is determined and placed in the same xy location in the refresh buffer  We summarize the steps of a depth buffer algorithm as follows     Initialize the depth buffer and refresh buffer so that for all buffer posi tions x y  depth x y   refresh x    lbackgnd previously stored values in the depth buffer to determine visibility   Calculate the depth z for each x y position on the polygon  Ifz  depth x y  then set depth y y z  refresh x y  Jun xy  where lpackgna is the value for the background intensity and qu  x y is the projected intensity value for the surface at pixel position x y  After all surfaces have been processed the depth buffer contains depth values for the visible surfaces and the refresh buffer contains   For each position on each polygon surface compare depth values to    I the corresponding intensity values for those surfaces   Depth values for a surface position x y are calculated from the plane equation for each surface  _ Ax  By D C  Zz Section Depth Butfer Method y Axis  y x x4 yaxis   From position x y on a scan line the next position across the line has coordinates  x  y  and the position immediately below on the next line has coordinates  iy  1D  For any scan line   adjacent horizontal positions across the line differ by and a vertical y value on an adjacent scan line differs by If the depth of posi tion x y has been determined to be z then the depth z of the next position x  y along the scan line is obtained from Eq as     AGw  By D TO Cc  03 N il N  O  The ratio A C is constant for each surface so succeeding depth values across a scan line are obtained from preceding values with a single addition  On each scan line we start by calculating the depth on a left edge of the polygon that intersects that scan line   Depth values at each successive position across the scan line are then calculated by Eq  We first determine the y coordinate extents of each polygon and process the surface from the topmost scan line to the bottom scan line as shown in Fig  Starting at a top vertex we can recursively calculate x positions down a left edge of the polygon as x  x  m where m is the slope of the edge   Depth values down the edge are then obtained recursively as zZ  A m B our Cc If we are processing down a vertical edge the slope is infinite and the recursive calculations reduce tu goszt Ol An alternate approach is to use a midpoint method or Bresenham type al gorithm for determining x values on left edges for each scan line Also the method can be applied to curved surfaces by determining depth and intensity values at each surface projection point  For polygon surfaces the depth buffer method is very easy to implement and it requires no sorting of the surfaces in a scene But it does require the avail ability of a second buffer in addition to the refresh buffer A system with a resolu  top scan tine  y scan lire left edge intersection  bottom scan line  Scan lines intersecting a polygon surface tion of by  Section A Buffer Method y scan line y scan line   it xx   Intersection positions on successive scan lines along a left polygon edge  for example would require over a million positions in the depth buffer with each position containing enough bits to represent the number of depth increments needed One way to reduce storage requirements is to process one sec tion of the scene at a time using a smaller depth buffer After each view section is processed the buffer is reused for the next section  An extension  ETHOD f the ideas in the depth buffer method is the A buffer method at the other end of the alphabet from z buffer  where z represents depth  The A  buffer method developed by Tepresents an antialiased area averaged accumulation buffer method Lucasfilm for implementation in the surface rendering system called REYES an acronym for Renders Everything You Ever Saw  A drawback of the depth buffer method is that it can only find one visible surface at each pixel position In other words it deals only with opaque surfaces and cannot accumulate intensity values for more than one surface as is necessary if transparent surfaces are to be displayed   The A buffer method ex  pands the dept list of surfaces ation at each pi h buffer so that each position in the buffer can reference a linked Thus more than one surface intensity can be taken into consider xel position and object edges can be antialiased  Each position in the A buffer has two fields   depth field  stores a positive or negative real number  intensity field  stores surface intensity information or a pointer value  background opaque  surface  foreground  transparent oo surface Viewing an opaque surface through a transparent surface requires multiple surface intensity contributions for pixel positions   flo Gea  depth intensiry depth intensity field field field field a  b   Organization of an A buffer pixel position  a single surface overlap of the corresponding pixel area and b multiple surface overlap  If the depth field is positive the number stored at that position is the depth of a single surface overlapping the corresponding pixel area The intensity field then stores the RGB components of the surface color at that point and the percent of pixel coverage as illustrated in  a  If the depth field is negative this indicates multiple surface contributions to the pixel intensity The intensity field then stores a pointer to a linked list of sur face data as in  b  Data for each surface in the linked list includes  RGB intensity components  opacity parameter percent of transparency  depth  percent of area coverage  surface identifier  other surface rendering parameters  pointer to next surface The A buffer can be constructed using methods similar to those in the depth buffer algorithm Scan lines are prucessed to determine surface overlaps of pixels across the individual scanlines Surfaces are subdivided into a polygon mesh and clipped against the pixel boundaries Using the opacity factors and percent of surface overlaps we can calculate the intensity of each pixel as an av erage of the contributions from the overlapping surfaces  This image space method for removing hidden surfaces is an extension of the scan line algorithm for filling polygon interiors Instead of filling just one surface we now deal with multiple surfaces As each scan line is processed ail polygon surfaces intersecting that line are examined to determine which are visible Across each scan line depth calculations are made for each overlapping surface to determine which is nearest to the view plane When the visible surface has been determined the intensity value for that position is entered into the refresh buffer  We assume that tables are set up for the various surfaces as discussed in Chapter which include both an edge table and a polvgon table The edge table contains coordinate endpoints for each line in the scene the inverse slope of each line and pointers into the polygon table to identify the surfaces bounded by each line The polygon table contains coefficients of the plane equation for each sur face intensity information for the surfaces and possibly pointers into the edge table To facilitate the search for surfaces crossing a given scan line we can set up an active list of edges from information in the edge table This active list will con tain only edges that cross the current scan line sorted in order of increasing x In addition we define a flag for each surface that is set on or off to indicate whether a position along a scan line is inside or outside of the surface Scan lines are processed from left to right At the leftmost boundary of a surface the surface flag is turned on and at the rightmost boundary it is tumed off   illustrates the scan line method for locating visible portions of surfaces for pixel positions along the line The active list for scan line contains information from the edge table for edges AB BC EH and FG For positions along this scan line between edges AB and BC only the flag for surtace S is on Therefore no depth calculations are necessary and intensity information for sur face S is entered from the polygon table into the refresh buffer Similarly be tween edges EH and FG only the flag for surface S is on No other positions along scan line intersect surfaces so the intensity values in the other areas are set to the background intensity The background intensity can be loaded through out the buffer in an initialization routine  For scan lines and in  the active edge list contains edges AD EH BC and FG Along scan line from edge AD to edge EH only the flag for surface S is on But between edges EH and BC the flags for both surfaces are on In this interval depth calculations must be made using the plane coefficients for the two surfaces For this example the depth of surface is assumed to be less than that of S2 so intensities for surface are loaded into the refresh buffer until boundary BC is encountered Then the flag for surface S goes off and intensities for surface S2 are stored until edge FG is passed  We can take advantage of coherence along the scan lines as we pass from one scan line to the next In  scan line has the same active list of edges as scan line Since no changes have occurred in line intersections it is unneces sary again to make depth calculations between edges EH and BC The two sur    Scan lines crossing the projection of two surfaces   and S  in the view plane Dashed lines indicate the boundaries of hidden surfaces  Section Scan Line Method   Intersecting and cyclically overlapping surfaces that alternately obscure one another  Subdividing  Line  Subdividing  faces must be in the same orientation as determined on scan line so the intensi ties for surface S can be entered without further calculations  Any number of overlapping polygon surfaces can be processed with this scan line method Flags for the surfaces are set to indicate whether a position is inside or outside and depth calculations are performed when surfaces overlap When these coherence methods are used we need to be careful to keep track of which surface section is visible on each scan line This works only if surfaces do not cut through or otherwise cyclically overlap each other   lf any kind of cyclic overlap is present in a scene we can divide the surfaces to eliminate the overlaps The dashed lines in this figure indicate where planes could be subdi vided to form two distinct surfaces so that the cyclic overlaps are eliminated  Using both image space and object space operations the depth sorting method performs the following basic functions  Surfaces are sorted in order of decreasing depth  Surfaces are scan converted in order starting with the surface of greatest depth  Sorting operations are carried out in both image and object space and the scan conversion of the polygon surfaces is performed in image space  This method for solving the hiddew surface problem is often referred to as the painter s algorithm In creating an oil painting an artist first paints the back ground colors Next the most distant objects are added then the nearer objects and so forth At the final step the foreground objects are painted on the canvas over the background and other objects that have been painted on the canvas Each layer of paint covers up the previous layer Using a similar technique we first sort surfaces according to their distance from the view plane The intensity values for the farthest surface are then entered into the refresh buffer Taking each succeeding surface in turn in decreasing depth order  we paint the sur face intensities onto the frame buffer over the intensities of the previously processed surfaces  Painting polygon surfaces onto the frame buffer according to depth is carried out in several steps Assuming we are viewing along the z direction surfaces are ordered on the first pass according to the smallest z value on each surface Surface S with the greatest depth is then compared to the other sur faces in the list to determine whether there are any overlaps in depth If no depth overlaps occur S is scan converted  shows two surfaces that overlap in the xy plane but have no depth overlap This process is then re peated for the next surface in the list As long as no overlaps occur each sur face is processed in depth order until ail have been scan converted If a depth overlap is detected at any point in the list we need to make some additional comparisons to determine whether any of the surfaces should be reordered  We make the following tests for each surface that overlaps with S If any one of these tests is true no reordering is necessary for that surface The tests are listed in order of increasing difficulty   The bounding rectangles in the xy plane for the two surfaces da not over lap  Surface S is completely behind the overlapping surface relative to the view ing position  The overlapping surface is completely in front of S relative to the viewing position  The projections of the two surfaces onto the view plane do not overlap  We perform these tests in the order listed and proceed to the next overlapping surface as soon as we find one of the tests is true If all the overlapping surfaces pass at least one of these tests none of them is behind S No reordering is then necessary anc is scan converted  Test is performed in two parts We first check for overlap in the x direc tion then we check for overlap in the y direction If either of these directions show no overlap the two planes cannot obscure one other An example of two  Two surfaces with no depth overlap  Section Depth Sorting Method  surfaces that overlap in the z direction but not in the x direction is shown in Fig 13  We can perform tests and with an inside outside polygon test That is we substitute the coordinates for all vertices of S into the plane equation for the overlapping surface and check the sign of the result If the plane equations are set up so that the outside of the surface is toward the viewing position then  is be hind S if all vertices of S are inside S    Similarly S is completely in front of S if all vertices of S are outside of S   shows an overlap ping surface S that is completely in front of S but surface S is not completely inside S  test is not true  If tests through have all failed we try test by checking for intersections between the bounding edges of the two surfaces using line equations in the xy plane As demonstrated in  two surfaces may or may not intersect even though their coordinate extents overlap in the x y and z directions  Should all four tests fail with a particular overlapping surface S  we inter change surfaces S and S in the sorted list An example of two surfaces that   Lt Xun Xinax X min X mac My Fryure 13 Two surfaces with depth overlap  but no overlap in the x direction  x  Surface Sis completely behind  inside  the overlapping surface   Zz S s _ Ss _   x Overlapping surface S is completely in front  outside  of surface S but S is not completely Z behind    Two surfaces with overlapping bounding rectangles in the xy plane  would be reordered with this procedure is given in  At this point we still do not know for certain that we have found the farthest surface from the view plane  illustrates a situation in which we would first inter change S and S  But since S obscures part of S  we need to interchange S and S to get the three surfaces into the correct depth order Therefore we need to repeat the testing process for each surface that is reordered in the list  It is possible for the algorithm just outlined to get into an infinite loop if two or more surfaces alternately obscure each other as in  In such sit uations the algorithm would continually reshuffle the positions of the overlap ping surfaces To avoid such loops we can flag any surface that has been re ordered to a farther depth position so that it cannot be moved again If an attempt is made to switch the surface a second time we divide it into two parts to eliminate the cyclic overlap The original surface is then replaced by the twe new surfaces and we continue processing as before   A binary space partitioning BSP tree is an efficient method for determining object visibility by painting surfaces onto the screen from back to front as in the painter s algorithm The BSP tree is particularly useful when the view reference point changes but the objects in a scene are at fixed positions  Applying a BSP tree to visibility testing involves identifying surfaces that are inside and outside the partitioning plane at each step of the space sub division relative to the viewing direction  illustrates the basic con cept in this algorithm With plane P  we first partition the space into two sets of objects One set of objects is behind or in back of plane P relative to the view ing direction and the other set is in front of P  Since one object is intersected by plane P  we divide that object into two separate objects labeled A and B Ob jects A and C are in front of P  and objects B and 2D are behind P  We next parti tion the space again with plane  and construct the binary tree representation shown in  b  In this tree the objects are represented as terminal nodes with front objects as left branches and back objects as right branches  s  zy  Surface  has greater depth but obscures surface     Three surfaces entered into the sorted surface list in the order   S  shauld be reordered S    S  Chapter Visible Surface Detection Methods  Pp  P P _ oe front pack von back  A region of space a is partitioned A c B D with two planes P and  to form ty the BSP tree representation in b  For objects described with polygon facets we chose the partitioning planes to coincide with the polygon planes The polygon equations are then used to identify inside and outside polygons and the tree is constructed with one partitioning plane for each polygon face Any polygon intersected by a partition ing plane is split nto two parts When the BSP tree is complete we process the tree by selecting the surfaces for display in the order back to front so that fore ground objects are painted over the background objects Fast hardware imple mentations for sonstructing and processing BSP trees are used in some systems  This technique for hidden surface removal is essentially an image space method but object space operations can be used to accomplish depth ordering of surfaces The area subdivision method takes advantage of area coherence ina scene by lo cating those view areas that represent part of a single surface We apply this method by successively dividing the total viewing area into smaller and smaller rectangles until each smail area is the projection of part of a single visible surface or no surface at all  To implement this method we need to establish tests tnat can quickly iden tify the area as part of a single surface or tell us that the area is too complex to an alyze easily Starting with the total view we apply the tests to determine whether we should subdivide the total area into smaller rectangles If the tests indicate that the view is sufficiently complex we subdivide it Next we apply the tests to each of the smaller areas subdividing these if the tests indicate that visibility of a Section single surface is still uncertain We continue this process until the subdivisions  Area Subdivision Method are easily analyzed as belonging to a single surface or until they are reduced to  the size of a single pixel An easy way to do this is to successively divide the area into four equal parts at each step as shown in  This approach is similar to that used in constructing a quadtree A viewing area with a resolution of by could be subdivided ten times in this way before a subarea is reduced to a point  Tests to determine the visibility of a single surface within a specified area are made by comparing surfaces to the boundary of the area There are four pos sible relationships that a surface can have with a specified area boundary We can  describe these relative surface characteristics in the following way      Surrounding surface One that completely encloses the area   Overlapping surface One that is partly inside and partly outside the area  Inside surface One that is completely inside the area      Dividing a square area into Outside surface One that is completely outside the area equal sized quadrants at each step The tests for determining surface visibility within an area can be stated in P terms of these four classifications No further subdivisions of a specified area are needed if one of the following conditions is true  All surfaces are outside surfaces with respect to the area  Only one inside overlapping or surrounding surface is in the area  A surrounding surface obscures all other surfaces within the area bound aries  Test can be carried out by checking the bounding rectangles of all surfaces against the area boundaries Test can also use the bounding rectangles in the xy plane to identify an inside surface For other types of surfaces the bounding rec tangles can be used as an initial check If a single bounding rectangle intersects the area in some way additional checks are used to determine whether the sur face is surrounding overlapping or outside Once a single inside overlapping or surrounding surface has been identified its pixel intensities are transferred to the appropriate area within the frame buffer  One method for implementing test is to order surfaces according to their minimum depth from the view plane For each surrounding surface we then compute the maximum depth within the area under consideration If the maxi   Surrounding Overlapping inside Outside Surtece Surface Surface Surtece  Possible relationships between polygon surfaces and a rectangular area   lange Within a specified area a   Surrounding surrounding surface with a Surface   U I KO  Te  i  i maximum depth of   obscures all x surfaces that have a minimum depth beyond Zax  Zi Area mum depth of one of these surrounding surfaces is closer to the view plane than the minimum depth of all other surfaces within the area test is satisfied Figure 22 shows an example of the conditions for this method  Another method for carrying out test that does not require depth sorting is to use plane equations to calculate depth values at the four vertices of the area for all surrounding overlapping and inside surfaces If the calculated depths for one of the surrounding surfaces is less than the calculated depths for all other surfaces test is true Then the area can be filled with the intensity values of the surrounding surface  For some situations both methods of implementing test will fail to iden tify correctly a surrounding surface that obscures all the other surfaces Further testing could be carried out to identify the single surface that covers the area but it is faster to subdivide the area than to continue with more complex testing Once outside and surrounding surfaces have been identified for an area they will remain outside and surrounding surfaces for all subdivisions of the area Furthermore some inside and overlapping surfaces can be expected to be elimi nated as the subdivision process continues so that the areas become easier to an alyze In the limiting case when a subdivision the size of a pixel is produced we simply calculate the depth of each relevant surface at that point and transfer the intensity of the nearest surface to the frame buffer   Area A is subdivided into and  using the boundary of surface S on the view plane As a variation on the basic subdivision process we could subdivide areas along surface boundaries instead of dividing them in half If the surfaces have been sorted according to minimum depth we can use the surface with the small est depth value to subdivide a given area  illustrates this method for subdividing areas The projection of the boundary of surface S is used to parti tion the original area into the subdivisions A and A Surface S is then a sur rounding surface for A and visibility tests and can be applied to determine whether further subdividing is necessary In general fewer subdivisions are re quired using this approach but more processing is needed to subdivide areas and to analyze the relation of surfaces to the subdivision boundaries  When an octree representation is used for the viewing volume hidden surface elimination is accomplished by projecting octree nodes onto the viewing surface in a front to back order In  the front face of a region of space the side toward the viewer is formed with octants  and Surfaces in the front of these octants are visible to the viewer Any surfaces toward the rear of the front octants or in the back octants   and may be hidden by the front surfaces  Back surfaces are eliminated for the viewing direction given in  by processing data elements in the octree nodes in the order     This results in a depth first traversal of the octree so that nodes representing oc tants  and for the entire region are visited before the nodes representing octants  and Similarly the nodes for the front four suboctants of actant are visited before the nodes for the four back suboctants The traversal of the oc tree continues in this order for each octant subdivision  When a color value is encountered in an octree node the pixel area in the frame buffer corresponding to this node is assigned that color value only if no values have previously been stored in this area In this way only the front colors are loaded into the buffer Nothing is loaded if an area is void Any node that is found to be completely obscured is eliminated from further processing so that its subtrees are not accessed   Different views of objects represented as octrees can be obtained by apply ing transformations to the octree representation that reorient the object according   oo Numbered  Octants A   Objects in octants  and of a Region   obscure objects in the back octants Viewing   when the viewing Direction direction is as shown  Section Octree Methods  a  Octants in Space      Quadrants for tha View Plane   Octant divisions for a region of space and the corresponding quadrant plane   to the view selected We assume that the octree representation is always set up so that octants  and of a region form the front face as in   A method for displaying an octree is first to map the octree onto a quadtree of visible areas by traversing octree nodes from front to back in a recursive proce dure Then the quadtree representation for the visible surfaces is loaded into the frame buffer  depicts the octants in a region of space and the corre sponding quadrants on the view plane Contributions to quadrant come from octants and Color values in quadrant are obtained from surfaces in octants and and values in each of the other two quadrants are generated from the pair of octants aligned with each of these quadrants  Recursive processing of octree nodes is demonstrated in the following proce dure which accepts an octree description and creates the quadtree representation for visible surfaces in the region In most cases both a front and a back octant must be considered in determining the correct color values for a quadrant But if the front octant is homogeneously filled with some color we do not process the back octant For heterogeneous regions the procedure is recursively called pass ing as new arguments the child of the heterogeneous octant and a newly created quadtree node If the front is empty the rear octant is processed Otherwise two recursive calls are made one for the rear octant and one for the front octant  typedef enum  SOLID MIXED  Status  define EMPTY  typedef struct tOctree  int id Status status union  int color struct tOctree  children   data  Octree  typedef struct tQuadtree  int id Status status union  int color struct tQuadtree  children   data  Quadtree  int nQuadtree   void octreeToQuadtree Octree  oTree Quadtree  qTree   Octree  front  back  Quadtree  newQuadtree  int i  if oTree status  SOLID  QTree status  SOLID qTree data color  oTree data color return   qTree status  MIXED   Fill in each quad of the quadtree   for i i d i   front  oTree data children i   back  oTree data children i  newQuadtree  Quadtree  malloc sizeof Quadtree  newQuadtree id  nQuadtree   newQuadtree status  SOLID  qTree data children i  newQuadtree  if front status  SOLID if front data color  EMPTY qfree data children i  data color  Front data color else if back status  SOLID if back data color  EMPTY qTree data children i  data color  back data color else qTree data children i  data color  EMPTY else   back node is mixed  newQuadtree status  MIXED octreeToQuadtree back newQuadtree   else   front node is mixed  newQuadtree status  MIXED octreeToQuadtree back newQuadtree  octreeToQuadtree front newQuadtree     If we consider the line of sight from a pixel position on the view plane through a scene as in  we can determine which objects in the scene if any inter sect this line After calculating all ray surface intersections we identify the visi ble surface as the one whose intersection point is closest to the pixel This visibil ity detection scheme uses ray casting procedures that were introduced in Section 15 Ray casting as a visibility detection tool is based on geometric optics methods which trace the paths of light rays Since there are an infinite number of light rays in a scene and we are interested only in those rays that pass through   A ray along the line of sight from a pixel position through a scene  Section 10 Ray Casting Method Chapter  Visible Surface Detection Methods  pixel positions we can trace the light ray paths backward from the pixels through the scene The ray casting approach is an effective visibility detection method for scenes with curved surfaces particularly spheres  We can think of ray casting as a variation on the depth buffer method Sec tion  In the depth buffer algorithm we process surfaces one at a time and calculate depth values for all projection points over the surface The calculated surface depths are then compared to previously stored depths to determine visi ble surfaces at each pixel In ray casting we process pixels one at a time and cal culate depths for all surfaces along the projection path to that pixel  Ray casting is a special case of ray tracing algorithms Section that trace multiple ray paths to pick up global reflection and refraction contributions from multiple objects in a scene With ray casting we only follow a ray out from each pixel to the nearest object Efficient ray surface intersection calculations have been developed for common objects particularly spheres and we discuss these intersection methods in detail in Chapter  Effective methods for determining visibility for objects with curved surfaces in clude ray casting and octree methods With ray casting we calculate ray surface intersections and locate the smallest intersection distance along the pixel ray With octrees once the representation has been established from the input defini tion of the objects all visible surfaces are identified with the same processing pro cedures No special considerations need be given to different kinds of curved surfaces  We can also approximate a curved surface as a set of plane polygon sur faces In the list of surfaces we then replace each curved surface with a polygon mesh and use one of the other hidden surface methods previously discussed With some objects such as spheres it can be more efficient as well as more accu rate to use ray casting and the curved surface equation  Curved Surface Representations We can represent a surface with an implicit equation of the form f x y z  or with a parametric representation Appendix A  Spline surfaces for instance are normally described with parametric equations In some cases it is useful to ob tain an explicit surface equation as for example a height function over an xy ground plane  z fix y  Many objects of interest such as spheres ellipsoids cylinders and cones have quadratic representations These surfaces are commonly used to model molecu lar structures roller bearings rings and shafts  Sean line and ray casting algorithms often involve numerical approxima tion techniques to solve the surface equation at the intersection point with a scan line or with a pixel ray Various techniques including parallel calculations and fast hardware implementations have been developed for solving the curved sur face equations for commonly used objects Surface Contour Plots For many applications in mathematics physical sciences engineering and other fields it is useful to display a surface function with a set of contour lines that show the surface shape The surface may be described with an equation or with data tables such as topographic data on elevations or population density With an explicit functional representation we can plot the visible surface contour lines and eliminate those contour sections that are hidden by the visible parts of the surface  To obtain an xy plot of a functional surface we write the surface representa tion in the form y  fx   A curve in the xy plane can then be platted for values of z within some selected range using a specified interval Az Starting with the largest value of z we plot the curves from front to back and eliminate hidden sections We draw the curve sections on the screen by mapping an xy range for the function into an xy pixel screen range Then unit steps are taken in x and the corresponding y value for each x value is determined from Eq for a given value of z  One way to identify the visible curve sections on the surface is to maintain a list of Yam and Ypu Values previously calculated for the pixel x coordinates on the screen As we step from one pixel x position to the next we check the calculated y value against the stored range yin ANd Yas for the next pixel If yrin y  Youx that point on the surface is not visible and we do not plot it But if the calcu lated y value is outside the stored y bounds for that pixel the point is visible We then plot the point and reset the bounds for that pixel Similar procedures can be used to project the contour plot onto the xz or the yz plane  shows an example of a surface contour plot with color coded contour lines  Similar methods can be used with a discrete set of data points by determin ing isosurface lines For example if we have a discrete set of z values for an n by n grid of xy values we can determine the path of a line of constant over the surface using the contour methods discussed in Section 21 Each selected con tour line can then be projected onto a view plane and displayed with straight line   A color coded surface contour plot  Courtesy of Los Alamos National Laboratory  Section 11 Curved Surfaces  a  ib   Hidden line sections dashed fora line that a passes behind a surface and b penetrates a surface  segments Again lines can be drawn on the display device in a front to back depth order and we eliminate contour sections that pass behind previously drawn visible contour lines   When only the outline of an object is to be displayed visibility tests are applied to surface edges Visible edge sections are displayed and hidden edge sections can either be eliminated or displayed differently from the visible edges For ex ample hidden edges could be drawn as dashed lines or we could use depth cue ing to decrease the intensity of the lines as a linear function of distance from the view plane Procedures for determining visibility of object edges are referred to as wireframe visibility methods They are also called visible line detection methods or hidden line detection methods Special wireframe visibility proce dures have been developed but some of the visible surface methods discussed in preceding sections can also be used to test for edge visibility  A direct approach to identifying the visible lines in a scene is to compare each line to each surface The process involved here is sifnilar to clipping lines against arbitrary window shapes except that we now want to determine which sections of the lines are hidden by surfaces For each line depth values are com pared to the surfaces to determine which line sections are not visible We can use coherence methods to identify hidden line segments without actually testing each coordinate position If both line intersections with the projection of a surface boundary have greater depth than the surface at those points the line segment between the intersections is completely hidden as in  a  This is the usual situation in a scene but it is also possible to have lines and surfaces inter secting each other When a line has greater depth at one boundary intersection and less depth than the surface at the other boundary intersection the line must penetrate the surface interior as in  b  In this case we calculate the in tersection point of the line with the surface using the plane equation and display only the visible sections  Some visible surface methods are readily adapted to wireframe visibility testing Using a back face method we could identify all the back surfaces of an object and display only the boundaries for the visible surfaces With depth sort ing surfaces can be painted into the refresh buffer so that surface interiors are in the background color while boundaries are in the foreground color By process ing the surfaces from back to front hidden lines are erased by the nearer sur faces An area subdivision method can be adapted to hidden line removal by dis playing only the boundaries of visible surfaces Scan line methods can be used to display visible lines by setting points along the scan line that coincide with boundaries of visible surfaces Any visible surface method that uses scan conver sion can be modified to an edge visibility detection method in a similar way  Often three dimensional graphics packages accommodate several visible surface detection procedures particularly the back face and depth buffer methods A particular function can then be invoked with the procedure name such as back Face or depthBuf fer In general programming standards such as GKS and PHIGS visibility methods are implementation dependent A table of available methods is listed at each installation and a particular visibility detection method is selected with the hidden line hidden surface removal HLHSR function  setHLHSRidentifier visibilityPunctionIndex  Parameter visibilityFunctionIndex is assigned an integer code to identify the visibility method that is to be applied to subsequently specified output primi tives  SUMMARY Here we give a summary of the visibility detection methods discussed in this chapter and a comparison of their effectiveness Back face detection is fast and ef fective as an initial screening to eliminate many polygons from further visibility tests For a single convex polyhedron back face detection eliminates all hidden surfaces but in general back face detection cannot completely identify all hid den surfaces Other more involved visibility detection schemes will correctly produce a list of visible surfaces  A fast and simple technique for identifying visible surfaces is the depth buffer or z buffer method This procedure requires two buffers one for the pixel intensities and one for the depth of the visible surface for each pixel in the view plane Fast incremental methods are used to scan each surface in a scene to calcu late surface depths As each surface is processed the two buffers are updated An improvement on the depth buffer approach is the A buffer which provides addi tional information for displaying antialiased and transparent surfaces Other visi ble surface detection schemes include the scan line method the depth sorting method painter s algorithm  the BSP tree method area subdivision octree methods and ray casting  Visibility detection methods are also used in displaying three dimensional line drawings With curved surfaces we can display contour plots For wireframe displays of polyhedrons we search for the various edge sections of the surfaces in a scene that are visible from the view plane  The effectiveness of a visible surface detection method depends on the characteristics of a particular application If the surfaces in a scene are spread out in the z direction so that there is very little depth overlap a depth sorting or BSP tree method is often the best choice For scenes with surfaces fairly well sepa rated horizontally a scan line or area subdivision method can be used efficiently to locate visible surfaces  As a general rule the depth sorting or BSP tree method is a highly effective approach for scenes with only a few surfaces This is because these scenes usually have few surfaces that overlap in depth The scan line method also performs well when a scene contains a small number of surfaces Either the scan line depth sorting or BSP tree method can be used effectively for scenes with up to several thousand polygon surfaces With scenes that contain more than a few thousand surfaces the depth buffer method or octree approach performs best The depth buffer method has a nearly constant processing time independent of the number of surfaces in a scene This is because the size of the surface areas decreases as the number of surfaces in the scene increases Therefore the depth buffer method ex hibits relatively low performance with simple scenes and relatively high perfor  Summary  mance with complex scenes BSP trees are useful when multiple views are to be generated using different view reference points  When octree representations are used in a system the hidden surface elimi nation process is fast and simple Only integer additions and subtractions are used in the process and there is no need to perform sorting or intersection calcu lations Another advantage of octrees is that they store more than surfaces The entire solid region of an object is available for display which makes the octree representation useful for obtaining cross sectional slices of solids  If a scene contains curved surface representations we use octree or ray casting methods to identify visible parts of the scene Ray casting methods are an integral part of ray tracing algorithms which allow scenes to be displayed with global illumination effects  It is possible to combine and implement the different visible surface detec tion methods in various ways In addition visibility detection algorithms are often implemented in hardware and special systems utilizing parallel processing are employed to increase the efficiency of these methods Special hardware sys tems are used when processing speed is an especially important consideration as in the generation of animated views for flight simulators  REFERENCES Additional sources of information on visibility algorithms include Elber and Cohen  Franklin and Kankanhalli  Glassner  Naylor Amanatides and Thibault  and Segal   EXERCISES  Develop a procedure based on a back face detection technique for identifying all the visible faces of a convex polyhedron that has different colored surfaces Assume that the object is defined in a right handed viewing system with the xy plane as the viewing surface  Implement a back face detection procedure using an orthographic parallel projection to view visible faces of a convex polyhedron Assume that all parts of the object are in front of the view plane and provide a mapping onto a screen viewport for display  Implement a back face detection procedure using a perspective projection to view visible faces of a convex polyhedron Assume that all parts of the object are in front of the view plane and provide a mapping onto a screen viewport for display  Write a program to produce an animation of a convex polyhedron The object is to be rotated incrementally about an axis that passes through the object and is parallel to the view plane Assume that the object lies completely in front of the view plane Use an orthographic parallel projection to map the views successively onto the view plane  Implement the depth buffer method to display the visible surfaces of a given polyhe dron How can the storage requirements for the depth buffer be determined from the definition of the objects to be displayed  tmplement the depth buffer method to display the visible surfaces in a scene contain ing any number of polyhedrons Set up efficient methods for storing and processing the various objects in the scene  Implement the A buffer algorithm to display a scene containing both opaque and transparent surfaces As an optional feature your algorithm may be extended to in clude antialiasing   10  11  12  13  14  15  16  17  18  19 20  21 22  23  24  Develop a program to implement the scan line algorithm for displaying the visible surfaces of a given polyhedron Use polygon and edge tables to store the definition of the object and use coherence techniques to evaluate points along and between scan lines  Write a program to implement the scan line algorithm for a scene containing several polyhedrons Use polygon and edge tables to store the definition of the object and use coherence techniques to evaluate points along and between scan lines  Set up a program to display the visible surfaces of a convex polyhedron using the painter s algorithm That is surfaces are to be sorted on depth and painted on the screen from back to front  Write a program that uses the depth sorting method to display the visible surfaces of any given object with plane faces  Develop a depth sorting program to display the visible surfaces in a scene containing several polyhedrons  Write a program to display the visible surfaces of a convex polyhedron using the  BSP tree method  Give examples of situations where the two methods discussed for test in the area subdivision algorithm will fail to identify correctly a surrounding surface that ob scures all other surfaces  Develop an algorithm that would test a given plane surface against a rectangular area to decide whether it is a surrounding overlapping inside or outside surface Develop an algorithm for generating a quadtree representation for the visible sur faces of an object by applying the area subdivision tests to determine the values of the quadtree elements  Set up an algorithm to load a given quadtree representation of an object into a frame buffer for display  Write a program on your system to display an octree representation for an object so that hidden surfaces are removed  Devise an algorithm for viewing a single sphere using the ray casting method Discuss how antialiasing methods can be incorporated into the various hidden sur face elimination algorithms  Write a routine to produce a surface contour plot for a given surface function f x y  Develop an algorithm for detecting visible line sections in a scene by comparing each line in the scene to each surface  Discuss how wireframe displays might be generated with the various visible surface detection methods discussed in this chapter  Set up a procedure for generating a wireframe display of a polyhedron with the hid den edges of the object drawn with dashed lines  Exercises  CHAPTER Ilumination Models and  Surface Rendering Methods     R ealistic displays of a scene are obtained by generating perspective projec tions of objects and by applying natural lighting effects to the visible sur faces An illumination model also called a lighting model and sometimes re ferred to as a shading model is used to calculate the intensity of light that we should see at a given point on the surface of an object A surface rendering algo rithm uses the intensity calculations from an illumination model to determine the light intensity for all projected pixel positions for the various surfaces in a scene Surface rendering can be performed by applying the illumination model to every visible surface point or the rendering can be accomplished by interpolating in tensities across the surfaces from a small set of illumination model calculations Scan line image space algorithms typically use interpolation schemes while ray tracing algorithms invoke the illumination model at each pixel position Some times surface rendering procedures are termed surface shading methods To avoid confusion we will refer to the model for calculating light intensity at a single sur face point as an ilumination model or a lighting model and we will use the term surface rendering to mean a procedure for applying a lighting model to obtain pixel intensilies for all the projected surface positions in a scene  Photorealism in computer graphics involves two elements accurate graphi cal representations of objects and good physical descriptions of the lighting ef fects in a scene Lighting effects include light reflections transparency surface texture and shadows   Modeling the colors and lighting effects that we see on an object is a com plex process involving principles of both physics and psychology Fundamen tally lighting effects are described with models that consider the interaction of electromagnetic energy with object surfaces Once light reaches our eyes it trig gers perception precesses that determine what we actually see in a scene Phys ical illumination models involve a number of factors such as object type object position relative to light sources and other objects and the light source condi tions that we set for a scene Objects can be constructed of opaque materials or they can be more or jess transparent In addition they can have shiny or dull sur faces and they can have a variety of surface texture patterns Light sources of varying shapes colors and positions can be used to provide the illumination ef fects for a scene Given the parameters for the optical properties of surfaces the relative positions of the surfaces in a scene the color and positions of the light sources and the position and orientation of the viewing plane illumination mod els calculate the intensity projected from a particular surface point ina specified viewing direction  Mjumination models in computer graphics are often loosely derived from the physical laws that describe surface light intensities To minimize intensity cal Chapter  Numination Models and Surface Rendering Methods Light Source  Reflecting Surface    Light viewed from an opaque nonluminous surface is in general a combination of reflected light from a light source and reflections of light reflections from other surfaces  Ne  N Diverging ray paths froma paint light source  culations most packages use empirical models based on simplified photometric calculations More accurate models such as the radiosity algorithm calculate light intensities by considering the propagation of radiant energy between the surfaces and light sources in a scene In the following sections we first take a look at the basic illumination models often used in graphics packages then we discuss more accurate but more time consuming methods for calculating sur face intensities And we explore the various surface rendering algorithms for ap plying the lighting models to obtain the appropriate shading over visible sur faces in a scene  When we view an opaque nonluminous object we see reflected light from the surfaces of the object The total reflected light is the sum of the contributions from light sources and other reflecting surfaces in the scene   Thus a surface that is not directly exposed to a light source may still be visible if nearby objects are illuminated Sometimes light sources are referred to as light emitting sources and reflecting surfaces such as the walls of a room are termed light re flecting sources We will use the term light source to mean an object that is emitting radiant energy such as a light bulb or the sun  A luminous object in general can be both a light source and a light reflec tor For example a plastic globe with a light bulb inside both emits and reflects light from the surface of the globe Emitted light from the globe may then illumi nate other objects in the vicinity  The simplest model for a light emitter is a point source Rays from the source then follow radially diverging paths from the source position as shown in  This light source model is a reasonable approximation for sources whose dimensions are smal compared to the size of objects in the scene Sources such as the sun that are sufficiently far from the scene can be accurately modeled as point sources A nearby source such as the long fluorescent light in  is more accurately modeled as a distributed light source In this case the illumi nation effects cannot be approximated realistically with a point source because the area of the source is not small compared to the surfaces in the scene An accu rate model for the distributed source is one that considers the accumulated illu mination effects of the points over the surface of the source  When light is incident on an opaque surface part of it is reflected and part is absorbed The amount of incident light reflected by a surface depends on the type of material Shiny materials reflect more of the incident light and dull sur faces absorb more of the incident light Similarly for an illuminated transparent   An object illuminated with a distributed light source surface some of the incident light will be reflected and some will be transmitted through the material  Surfaces that are rough or grainy tend to scatter the reflected light in al di rections This scattered light is called diffuse reflection A very rough matte sur face produces primarily diffuse reflections so that the surface appears equally bright from all viewing directions  illustrates diffuse light scattering from a surface What we call the color of an object is the color of the diffuse re flection of the incident light A blue object illuminated by a white light source for example reflects the blue component of the white light and totally absorbs all other components If the blue object is viewed under a red tight it appears black since all of the incident light is absorbed  In addition to diffuse reflection light sources create highlights or bright spots called specular reflection This highlighting effect is more pronounced on shiny surfaces than on dul surfaces An illustration of specular reflection is shown in   Here we discuss simplified methods for calculating light intensities The empiri cal models described in this section provide simple and fast methods for calculat ing surface intensity at a given point and they produce reasonably good results for most scenes Lighting calculations are based on the optical properties of sur faces the background lighting conditions and the light source specifications Optical parameters are used to set surface properties such as glossy matte opaque and transparent This controls the amount of reflection and absorption of incident light All light sources are considered to be point sources specified with a coordinate position and an intensity value color  Ambient Light A surface that is not exposed directly to a light source still will be visible if nearby objects are illuminated In our basic illumination model we can set a gen eral level of brightness for a scene This is a simple way to model the combina tion of light reflections from various surfaces to produce a uniform illumination called the ambient light or background light Ambient light has no spatial or di rectional characteristics The amount of ambient light incident on each object is a constant for all surfaces and over all directions  We can set the level for the ambient light in a scene with parameter I  and each surface is then illuminated with this constant value The resulting reflected light is a constant for each surface independent of the viewing direction and the spatial orientation of the surface But the intensity of the reflected light for each surface depends on the optical properties of the surface that is how much of the incident energy is to be reflected and how much absorbed  Diffuse Reflection Ambient light reflection is an approximation of global diffuse lighting effects Diffuse reflections are constant over each surface in a scene independent of the viewing direction The fractional amount of the incident light that is diffusely re  Section Basic Illumination Models   Diffuse reflections from a surface   Specular reflection superimposed on diffuse reflection vectors  Chapter Illumination Models and Surface Rendering Methods  i  a    b   A surface perpendicular to the direction of the incident light a is more illuminated than an equal sized surface at an oblique angle b to the incoming light direction    Radiant energy from a surface area dA in direction y relative to the surface normal direction   flected can be set for each surface with parameter k  the diffuse reflection coeffi cient or diffuse reflectivity Parameter k is assigned a constant value in the in terval to according to the reflecting properties we want the surface to have If we want a highly reflective surface we set the value of ky near This produces a bright surface with the intensity of the reflected light near that of the incident light To simulate a surface that absorbs most of the incident light we set the re flectivity to a value near Actually parameter k is a function of surface color but for the time being we will assume k is a constant  If a surface is exposed only to ambient light we can express the intensity of the diffuse reflection at any point on the surface as Tambaitt  Kale  Since ambient light produces a flat uninteresting shading for each surface Fig 19 b  scenes are rarely rendered with ambient light alone At least one light source is included in a scene often as a point source at the viewing position  We can model the diffuse reflections of illumination from a point source in a similar way That is we assume that the diffuse reflections from the surface are scattered with equal intensity in all directions independent of the viewing direc tion Such surfaces are sometimes referred to as ideal diffuse reflectors They are also called Lambertian reflectors since radiated light energy from any point on the surface is governed by Lambert s cosine law This law states that the radiant energy from any small surface area dA in any direction dy relative to the surface normal is proportional to cos y   The light intensity though depends on the radiant energy per projected area perpendicular to direction y which is dA cosdy Thus for Lambertian reflection the intensity of light is the same over all viewing directions We discuss photometry concepts and terms such as radiant energy in greater detail in Section  Even though there is equal light scattering in all directions from a perfect diffuse reflector the brightness of the surface does depend on the orientation of the surface relative to the light source A surface that is oriented perpendicular to the direction of the incident light appears brighter than if the surface were tilted at an oblique angle to the direction of the incoming light This is easily seen by holding a white sheet of paper or smooth cardboard parallel to a nearby window and slowly rotating the sheet away from the window direction As the angle be tween the surface normal and the incoming light direction increases less of the incident light falls on the surface as shown in  This figure shows a beam of light rays incident on two equal area plane surface patches with different spa tial orientations relative to the incident light direction from a distant source par   Acos  An illuminated area projected perpendicular to the path of the incoming light rays  incident allel incoming rays  If we denote the angle of incidence between the incoming light direction and the surface normal as    then the projected area of a surface patch perpendicular to the light direction is proportional to cos  Thus the amount of illumination or the number of incident light rays cutting across the projected surface patch depends on cos  If the incoming light from the source is perpendicular to the surface at a particular point that point is fully illu minated As the angle of illumination moves away from the surface normal the brightness of the point drops off If J is the intensity of the point light source then the diffuse reflection equation for a point on the surface can be written as lag  Kal cos  A surface is illuminated by a point source only if the angle of incidence is in the range to  cos  is in the interval from to  When cos is negative the light source is behind the surface  If N is the unit normal vector to a surface and L is the unit direction vector to the point light source from a position on the surface   then cos  N  Land the diffuse reflection equation for single point source illumination is Tain  Kal N  L  Reflections for point source illumination are calculated in world coordinates or viewing coordinates before shearing and perspective transformations are ap plied These transformations may transform the orientation of normal vectors so that they are no longer perpendicular to the surfaces they represent Transforma tion procedures for maintaining the proper orientation of surface normals are discussed in Chapter   illustrates the application of Eq to positions over the sur face of a sphere using various values of parameter kz between and Each pro jected pixel position for the surface was assigned an intensity as calculated by the diffuse reflection equation for a point ight source The renderings in this figure illustrate single point source lighting with no other lighting effects This is what we might expect to see if we shined a small light on the object in a completely darkened room For general scenes however we expect some background light ing effects in addition to the illumination effects produced by a direct light source  We can combine the ambient and point source intensity calculations to ob tain an expression for the total diffuse reflection In addition many graphics packages introduce an ambient reflection coefficient k to modify the ambient light intensity for each surface This simply provides us with an additional pa rameter to adjust the light conditions in a scene Using parameter k  we can write the total diffuse reflection equation as Tay  Kyla  kylN  L  lf To Light N Source   Angle of incidence  between the unit light source direction vector L and the unit surface normal N  Chapter Iilumination Models and Surface  Rendering Methods kd with ka  2 o  8   Diffuse reflections from a spherical surface illuminated by a point light source for values of the diffuse reflectivity coefficient in the interval O kS1    Diffuse reflections from a spherical surface illuminated with ambient light and a single point source for values of k and k in the interval   where both k and k depend on surface material properties and are assigned val ues in the range from to  shows a sphere displayed with surface intensitities calculated from Eq for values of parameters k and ky between and  Specular Reflection and the Phong Model When we look at an illuminated shiny surface such as polished metal an apple or a person s forehead we see a highlight or bright spot at certain viewing di rections This phenomenon called specular reflection is the result of total or near total reflection of the incident light in a concentrated region around the specular reflection angle  shows the specular reflection direction at a point on the illuminated surface The specular reflection angle equals the angle of the incident light with the two angles measured on opposite sides of the unit normal surface vector N In this figure we use R to represent the unit vector in the direc tion of ideal specular reflection L to represent the unit vector directed toward the point light source and V as the unit vector pointing to the viewer from the sur face position Angle  is the viewing anyle relative to the specular reflection di rection R For an ideal reflector perfect mirror  incident light is reflected only in the specular reflection direction In this case we would only see reflected light when vectors V and R coincide     Objects other than ideal reflectors exhibit specular reflections over a finite range of viewing positions around vector R Shiny surfaces have a narrow specu lar reflection range and dull surfaces have a wider reflection range An empirical model for calculating the specular reflection range developed by Phong Bui Tuong and called the Phong specular reflection model or simply the Phong model sets the intensity of specular reflection proportional to cos  Angle  can be assigned values in the range to  so that cos varies from to The value assigned to specular reflection parameter n is determined by the type of sur face that we want to display A very shiny surface is modeled with a large value for n  say or more  and smaller values down to are used for duller sur faces For a perfect reflector n is infinite For a rough surface such as chalk or cinderblock n would be assigned a value near Figures 13 and 14 show the effect of n on the angular range for which we can expect to see specular re flections  The intensity of specular reflection depends on the material properties of the surface and the angle of incidence as well as other factors such as the polar ization and color of the incident light We can approximately model monachro matic specular intensity variations using a specular reflection coefficient WO  for each surface  shows the general variation of W over the range  to   for a few materials In general W  tends to increase as the angle of incidence increases At    WC  and all of the incident light is reflected The variation of specular intensity with angle of incidence is described by Fresnel s Laws of Reflection Using the spectral reflection function W   we can write the Phong specular refiection model as Fooee  WCB cos    where J is the intensity of the light source and  is the viewing angle relative to the specular reflection direction R  oa oN oR Shiny Surface Oull Surface Large a  Small n   Modeling specular reflections shaded area with parameter    Specular reflection angle equals angle of incidence  5at   we   S S beer S co  a  Ra  i  cost  8 06 4 2 at l  oo  3ar 45  QO  45  oO  cos  db Oo    o   cast cos   os  2 Nt td L oa lL   ce   QO    Plots of cos s for several values of specular parameter  As seen in  transparent materials such as glass only exhibit ap preciable specular reflections as  approaches  At    about percent of the incident light on a glass surface is reflected And for most of the range of the reflected intensity is less than percent of the incident intensity But for many opaque materials specular reflection is nearly constant for all incidence an gles In this case we can reasonably model the reflected light effects by replacing W  with a constant specular reflection coefficient k  We then simply set k equal to some value in the range to for each surface  Since V and R are unit vectcrs in the viewing and specular reflection direc tions we can calculate the value of cos with the dot product V R Assuming the specular reflection coefficient is a constant we can determine the intensity of the specular reflection at a surface point with the calculation Lpec  KeliV  RY silver   Approximate variation of the specular reflection coefficient as a function of angle of incidence for  different materials   dielectric glass   Vector R in this expression can be calculated in terms of vectors L and N As seen in  the projection of L onto the direction of the normal vector is ob tained with the dot product N  L Therefore from the diagram we have R L N L N and the specular reflection vector is obtained as R  N DL N L   illustrates specular reflections for various values of k and n un a sphere illuminated with a single point light source  A somewhat simplified Phong model is obtained by using the halfway vector H between L and V to calculate the range of specular reflections If we replace V  R in the Phong model with the dot product N  H this simply replaces the empir ical cos  calculation with the empirical cosa calculation   The halfway vector is obtained as L V H TL vl  COLE We Ra ey attrition toms os spherical surface for varying specular parameter values and a single light source  Section Basic tllumination Models   Calculation of vector R by considering projections onto the direction of the normal vector N   Halfway vector H along the bisector of the angle between Land V  Illumination Models and Surface  Chapter Rendering Methods If both the viewer and the light source are sufficiently far from the surface both V and L are constant over the surface and thus H is also constant for all surface points For nonplanar surfaces N  H then requires less computation than V R since the calculation of R at each surface point involves the variable vector N  For given light source and viewer positions vector H is the orientation di rection for the surface that would produce maximum specular reflection in the viewing direction For this reason H is sometimes referred to as the surface ori entation direction for maximum highlights Also if vector V is coplanar with vectors L and R and thus N  angle  has the value  When V L and N are not coplanar a   depending on the spatial relationship of the three vectors  Combined Diffuse and Specular Reflections with Multiple Light Sources For a single point light source we can mode the combined diffuse and specular reflections from a point on an illuminated surface as Tat  Topec  kl  kyl ON L  kd N  HY    illustrates surface lighting effect wioduced by the various terms in Eq  If we place more than one point sour  scene we obtain the light re flection at any surface point by summing the vontributions from the individual sources  T kl  S KQN L  k N  I 10  trl To ensure that any pixel intensity does not exceed the maximum allowable value we can apply some type of normalization procedure A simple approach is to set a maximum magnitude for each term in the intensity equation If any cal culated term exceeds the maximum we simply set it to the maximum value An other way to compensate for intensity overflow is to normalize the individual terms by dividing each by the magnitude of the largest term A more compli cated procedure is first to calculate all pixel intensities for the scene then the cal culated intensities are scaled onto the allowable intensitv range  Warn Model So far we have considered only point light sources The Warn model provides a method for simulating studio lighting effects by controlling light intensity in dif ferent directions  Light sources are modeled as points on a reflecting surface using the Phong model for the surface points Then the intensity in different directions is con trolled by selecting values for the Phong exponent In addition light controls such as barn doors ane spotlighting used by studio photographers can be sim ulated in the Warn model Flaps are used to control the amount of light emitted by a source in various directions Two flaps are provided for each of the x y and z directions Spotlights are used to control the amount of light emitted within a cone with apex at a point source position The Warn model s implemented in c id    A wireframe scene a is displayed only with ambient lighting in b  and the surface of each object is assigned a different color Using ambient light and diffuse reflections due to a single source with k  for all surfaces we obtain the lighting effects shown in c  Using ambient light and both diffuse and specular reflections due to a single light source  we obtain the lighting effects shown in d  PHIGS  and  illustrates lighting effects that can be produced with this model  Intensity Attenuation As radiant energy from a point light source travels through space its amplitude is attenuated by the factor d  where d is the distance that the light has traveled This means that a surface close to the light source small d receives a higher inci dent intensity from the source than a distant surface large   Therefore to pro duce realistic lighting effects our illumination mode should take this intensity attenuation into account Otherwise we are illuminating all surfaces with the same intensity no matter how far they might be from the light source If two par allel surfaces with the same optical parameters overlap they would be indistin guishable from each other The two surfaces would be displayed as one surface  Chapter Illumination Models and Surface  Rendering Methods   Studio lighting effects produced with the Warn model using five light sources to illuminate a Chevrolet Camaro  Courtesy of David R Warm General Motors Research Laboratories  Our simple point source illumination model however does not always produce realistic pictures if we use the factor d to attenuate intensities The factor d produces too much intensity variations when d is small and it pro duces very little variation when d is large This is because real scenes are usually not illuminated with point light sources and our illumination model is too sim ple to accurately describe real lighting effects  Graphics packages have compensated for these problems by using inverse linear or quadratic functions of d to attenuate intensities For example a general inverse quadratic attenuation function can be set up as f d  a tad  ad  11  A user can then fiddle with the coefficients a9 a  and a to obtain a variety of lighting effects for a scene The value of the constant term ay can be adjusted to prevent f d from becoming too large when d is very small Also the values for the coefficients in the attenuation function and the optical surface parameters for a scene can be adjusted to prevent calculations of reflected intensities from ex ceeding the maximum allowable value This is an effective method for limiting intensity values when a single light source is used to illuminate a scene For mul tiple light source illumination the methods described in the preceding section are more effective for limiting the intensity range  With a given set of attenuation coefficients we can limit the magnitude of the attenuation function to with the calculation fa  min pv ad vad  12  Using this function we can then write our basic illumination model as a T kd  fd hK N L   N HY  13  i l where d is the distance light has traveled from light source i   Light reflections from the surface of a black nylon cushion modeled as woven cloth patterns and rendered using Monte Carlo ray tracing methods  Courtesy of Stephen H Westin Program of Computer Graphics Comell University   Color Considerations Most graphics displays of realistic scenes are in color But the illumination model we have described so far considers only monochromatic lighting effects To incor porate color we need to write the intensity equation as a function of the color properties of the light sources and object surfaces  For an RGB description each color in a scene is expressed in terms of red green and blue components We then specify the RGB components of light source intensities and surface colors and the illumination model calculates the RGB components of the reflected light One way to set surface colors is by speci fying the reflectivity coefficients as three element vectors The diffuse reflection coefficient vector for example would then have RGB components kag Kac kag  Uf we want an object to have a blue surface we select a nonzero value in the range from to for the blue reflectivity component k g while the red and green reflec tivity components are set to zero kyg  kig   Any nonzero red or green com ponents in the incident light are absorbed and only the blue component is re flected The intensity calculation for this example reduces to the single expression Ip  Kyplig   fi Liplkas N  L   k p N  HY  14  i l Surfaces typically are illuminated with white light sources and in general we can set surface color so that the reflected light has nonzero values for all three RGB components Calculated intensity levels for each color component can be used to adjust the corresponding electron gun in an RGB monitor  In his original specular reflection model Phong set parameter k to a con stant value independent of the surface color This produces specular reflections that are the same color as the incident light usually white  which gives the sur face a plastic appearance For a nonplastic material the color of the specular re flection is a function of the surface properties and may be different from both the color of the incident light and the color of the diffuse reflections We can approxi mate specular effects on such surfaces by making the specular reflection coeffi cient color dependent as in Eq 14  illustrates color reflections from a matte surface and Figs 22 and 23 show color reflections from metal   Light reflections from a teapot with Teflectance parameters set to simulate brushed aluminum surfaces and rendered using Monte Carlo ray tracing methods  Courtesy of Stephen H Westin Program of Computer Graphics Cornell University   Section Basic IIumination Models Chapter Wumination Models and Surface  Rendering Methods   Light reflections from trombones with reflectance parameters set to simulate shiny brass surfaces  Courtesy of SOFTIMAGE Inc   surfaces Light reflections from object surfaces due to multiple colored light sources is shown in   Another method for setting surface color is to specify the components of diffuse and specular color vectors for each surface while retaining the reflectivity coefficients as single valued constants For an RGB color representation for in stance the components of these two surface color vectors can be denoted as S4a  Sac Sag and Sg S5c  The blue component of the reflected light is then calcu lated as Ig  KeSaales   fild ligdkySaa N  L  k S g N  H  15  i This approach provides somewhat greater flexibility since surface color parame ters can be set independently from the reflectivity values  Other color representations besides RGB can be used to describe colors in a scene And sometimes it is convenient to use a color model with more than three components for a color specification We discuss color models in detail in the next chapter For now we can simply represent any component of a color specifi cation with its spectral wavelength A Intensity calculations can then be ex pressed as    kSaalen   fAdMyilkaSalN L  k S  N H  16  imd Transparency A transparent surface in general produces both reflected and transmitted light The relative contribution of the transmitted light depends on the degree of trans   Light reflections due to multiple light sources of various colors  Courtesy of Sun Microsystems  parency of the surface and whether any light sources or illuminated surfaces are behind the transparent surface  illustrates the intensity contributions to the surface lighting for a transparent object  When a transparent surface is to be modeled the intensity equations must be modified to include contributions from light passing through the surface In most cases the transmitted light is generated from reflecting objects in back of the surface as in  Reflected light from these objects passes through the transparent surface and contributes to the total surface intensity  Both diffuse and specular transmission can take place at the surfaces of a transparent object Diffuse effects are important when a partially transparent sur face such as frosted glass is to be modeled Light passing through such materials is scattered so that a blurred image of background objects is obtained Diffuse re fractions can be generated by decreasing the intensity of the refracted light and spreading intensity contributions at each point on the refracting surface onto a fi nite area These manipulations are time comsuming and most lighting models employ only specular effects   Realistic transparency effects are modeled by consideririg light refraction When light is incident upon a transparent surface part of it is reflected and part is refracted   Because the speed of light is different in different materi als the path of the refracted light is different from that of the incident light The direction of the refracted light specified by the angle of refraction is a function of the index of refraction of each material and the direction of the incident light Index of refraction for a material is defined as the ratio of the speed of light in a vacuum to the speed of light in the material Angle of refraction is calculated from the angle of incidence the index of refraction n of the incident material usually air  and the index of refraction of the refracting material according to Snell s law  17  sin   sin h   A ray traced view of a transparent glass surface showing both light transmission from objects behind the glass and light reflection from the glass surface  Courtesy of Eric Haines 3D EYE tc  incident light transparent object   Light emission from a transparent surface is in general a combination of reflected and transmitted light  To Light N Source L    teflection direction tefraction direction   Reflection direction R and refraction direction T fora ray of light incident upon a surface with index of refraction  incident fight   Refraction of light through a glass object The emerging refracted ray travels along a path that is parallel to the incident light path dashed line  Background Object P  Transparent ae Object Projection Plane   The intensity of a background object at point P can be combined with the reflected intensity off the surface of a transparent object along a perpendicular projection line dashed   Actually the index of refraction of a material is a function of the wave length of the incident light so that the different color components of a light ray will be refracted at different angles For most applications we can use an average index of refraction for the different materials that are modeled in a scene The index of refraction of air is approximately and that of crown glass is about Using these values in Eq 17 with an angle of incidence of yields an angle of refraction of about   illustrates the changes in the path direc tion for a light ray refracted through a glass object The overall effect of the re fraction is to shift the incident light to a parallel path Since the calculations of the trigonometric functions in Eq 17 are time consuming refraction effects could be modeled by simply shifting the path of the incident light a small amount  From Snell s law and the diagram in  we can obtain the unit transmission vector T in the refraction direction  as  r  ir cos  cos a  FL 18  where N is the unit surface normal and L is the unit vector in the direction of the light source Transmission vector T can be used to locate intersections of the re fraction path with objects behind the transparent surface Including refraction ef fects in a scene can produce highly realistic displays but the determination of re fraction paths and object intersections requires considerable computation Most scan line image space methods model light transmission with approximations that reduce processing time We return to the topic of refraction in our discussion of ray tracing algorithms Section  A simpler procedure for modeling transparent objects is to ignore the path shifts altogether In effect this approach assumes there is no change in the index of refraction from one material to another so that the angle of refraction is always the same as the angle of incidence This method speeds up the calculation of in tensities and can produce reasonable transparency effects for thin polygon sur faces  We can combine the transmitted intensity Ian through a surface from a background object with the reflected intensity Iq from the transparent surface   using a transparency coefficient k We assign parameter k a value between and to specify how much of the background light is to be transmit ted Total surface intensity is then calculated as T  K lea  Kilerans  The term  k  is the opacity factor  For highly transparent objects we assign k a value near Nearly opaque objects transmit very little light from background objects and we can set k to a value near for these materials opacity near  It is also possible to allow k to be a function of position over the surface so that different parts of an object can transmit more or less background intensity according to the values assigned to k  Transparency effects are often implemented with modified depth buffer z buffer algorithms A simple way to do this is to process opaque objects first to determine depths for the visible opaque surfaces Then the depth positions of the transparent objects are compared to the values previously strored in the depth buffer If any transparent surface is visible its reflected intensity is calcu lated and combined with the opaque surface intensity previously stored in the frame buffer This method can be modified to produce more accurate displays by using additional storage for the depth and other parameters of the transparent ow Incident Light froma Distant Source   Objects modeled with shadow regions  surfaces This allows depth values for the transparent surfaces to be compared to each other as well as to the depth values of the opaque surfaces Visible transpar ent surfaces are then rendered by combining their surface intensities with those of the visible and opaque surfaces behind them  Accurate displays of transparency and antialiasing can be obtained with the A buffer algorithm For each pixel position surface patches for all overlapping surfaces are saved and sorted in depth order Then intensities for the transparent and opaque surface patches that overlap in depth are combined in the proper vis ibility order to produce the final averaged intensity for the pixel as discussed in Chapter  A depth sorting visibility algorithm can be modified to handle transparency by first sorting surfaces in depth order then determining whether any visible surface is transparent If we find a visible transparent surface its reflected surface intensity is combin d with the surface intensity of objects behind it to obtain the pixel intensity at each projected surface point  Shadows Hidden surface methods can be used to locate areas where light sources produce shadows By applying a hidden surface method with a light source at the view position we can determine which surface sections cannot be seen from the light source These are the shadow areas Once we have determined the shadow areas for all Jisht sources the shadows could be treated as surface patterns and stored in pattern arrays  illustrates the generation of shading pat terns for two objects on a table and a distant light source All shadow areas in this figure are surfaces that are not visible from the position of the light source The scene in  shows shadow effects produced by multiple light sources  Shadow patterns generated by a hidden surface method are valid for any selected viewing position as long as the light source positions are not changed Surfaces that are visible from the view position are shaded according to the light ing model which can be combined with texture patterns We can display shadow areas with ambient light iniensity only or we can combine the ambient light with specified surface textures   Values of intensitv calculated by an illumination model must be converted to one of the allowable intersitv levels for the particular graphics system in use Some Section Displaying Light Intensities  Rendering Methods systems are capable of displaying several intensity levels while others are capa ble of only two levels for each pixel on or off  In the first case we convert inten sities from the lighting model into one of the available levels for storage in the frame buffer For bilevel systems we can convert intensities into halftone pat terns as discussed in the next section  Assigning Intensity Levels We first consider how grayscale values on a video monitor can be distributed over the range between and so that the distribution corresponds to our per ception of equal intensity intervals We perceive relative light intensities the same way that we perceive relative sound intensities on a logarithmic scale This means that if the ratio of two intensities is the same as the ratio of two other in tensities we perceive the difference between each pair of intensities to be the same As an example we perceive the difference between intensities 20 and 22 to be the same as the difference between 80 and 88 Therefore to display n successive intensity levels with equal perceived brightness the intensity levels on the monitor should be spaced so that the ratio of successive intensities is constant  h_k hobble  20 food Ba Here we denote the lowest level that can be displayed on the monitor as y and the highest as J  Any intermediate intensity can then be expressed in terms of Jy as  rly 21  We can calculate the value of r given the values of J and n for a particular sys tem by substituting k  in the preceding expression Since   we have Va r  t  22   Thus the calculation for  in Eq 21 can be rewritten as e  23  As an example if Ip  for a system with n  we have r  and the four intensity values are  and  The lowest intensity value Ij depends on the characteristics of the monitor and is typically in the range from 005 to around 025 As we saw in Chapter a black region displayed on a monitor will always have some intensity value above due to reflected light from the screen phosphors For a black and white monitor with bits per pixel   and I  01 the ratio of successive inten sities is approximately r  0182 The approximate values for the intensities on this system are 0100 0102 0104 0106 0107 0109     9821 and 0000  With a color system we set up intensity levels for each component of the color model Using the RGB model for example we can relate the blue compo nent of intensity at level k to the lowest attainable blue value as in Eq 21 cee intensity  A typical monitor response curve showing the displayed screen  0 intensity asa function of normalized electron gun voltage normalized electron gun voltage Igy  r ilpo 24 where un Tg     25 Tho and n is the number of intensity levels Similar expressions hold for the other color components  Gamma Correction and Video Lookup Tables Another problem associated with the display of calculated intensities is the non linearity of display devices Illumination models produce a linear range of inten sities The RGB color 25 25 25 obtained from a lighting model represents one half the intensity of the color   Usually these calculated intensi ties are then stored in an image file as integer values with one byte for each of the three RGB components This intensity file is also linear so that a pixel with the value   has one half the intensity of a pixel with the value   A video monitor however is a nonlinear device If we set the voltages for the electron gun proportional to the linear pixel values the displayed intensities will be shifted according to the monitor response curve shown in   To correct for monitor nonlinearities graphics systems use a video lookup table that adjusts the linear pixel values The monitor response curve is described by the exponential function l av  26  Parameter  is the displayed intensity and parameter V is the input voltage Val ues for parameters a and y depend on the characteristics of the monitor used in the graphics system Thus if we want to display a particular intensity value J the correct voltage value to produce this intensity is pvr Ve   27  a Section Displaying Light Intensities   normalized elect on gun valtage  pt 0 pixel intensity value   A video lookup correction curve for mapping pixel intensities to electron gun voltages using gamma correction with y  Values for both pixel intensity and monitor voltages are normalized on the interval to  This calculation is referred to as gamma correction of intensity Monitor gamma values are typically between and The National Television System Com mittee NTSC signal standard is y   shows a gamma correction curve using the NTSC gamma value Equation 27 is used to set up the video lookup table that converts integer pixel values in the image file to values that control the electron gun voltages  We can combine gamma correction with logarithmic intensity mapping to produce a lookup table that contains both conversions If  is an input intensity value from an illumination model we first locate the nearest intensity J from a table of values created with Eq 20 or Eq 23 Alternatively we could deter mine the level number for this intensity value with the calculation k round log  28 Q then we compute the intensity value at this level using Eq 23 Once we have the intensity value JI  we can calculate the electron gun voltage  L Vr V    29 a Values V can then be placed in the lookup tables and values for k would be stored in the frame buffer pixel positions If a particular system has no lookup table computed values for V can be stored directly in the frame buffer The com bined conversion to a logarithmic intensity scale followed by calculation of the V using Eq 29 is also sometimes referred to as gamma correction  a   A continuous tone photograph a printed with b two intensity levels  c four intensity levels and d eight intensity levels  If the video amplifiers of a monitor are designed to convert the linear input pixel values to electron gun voltages we cannot combine the two intensity con version processes In this case gamuma correction is built into the hardware and the logarithmic values J must be precomputed and stored in the frame buffer or the color table  Displaying Continuous Tone Images High quality computer graphics systems generally provide intensity levels for each color component but acceptable displays can be obtained for many ap plications with fewer levels A four level system provides minimum shading ca pability for continuous tone images while photorealistic images can be gener ated on systems that are capable of from to intensity levels per pixel   shows a continuous tone photograph displayed with various intensity levels When a small number of intensity levels are used to reproduce a continuous tone image the borders between the different intensity regions called contours are clearly visible In the two level reproduction the features of the photograph are just barely identifiable Using four intensity levels we begin to identify the original shading patterns but the contouring effects are glaring With eight intensity levels contouring effects are still obvious but we begin to have a better indication of the original shading At or more intensity levels contouring effects diminish and the reproductions are very close to the original Reproductions of continuous tone images using more than intensity levels show only very subtle differences from the original     An enlarged section of a photograph reproduced with a halftoning method showing how tones are represented with varying size dots  When an output device has a limited intensity range we can create an apparent increase in the number of available intensities by incorporating multiple pixel po sitions into the display of each intensity value When we view a small region con sisting of several pixel positions our eyes tend to integrate or average the fine detail into an overall intensity Bilevel monitors and printers in particular can take advantage of this visual effect to produce pictures that appear to be dis played with multiple intensity values  Continuous tone photographs are reproduced for publication in newspa pers magazines and books with a printing process called halftoning and the re produced pictures are called halftones For a black and white photograph each intensity area is reproduced as a series of black circles on a white background The diameter of each circle is proportional to the darkness required for that in tensity region Darker regions are printed with larger circles and lighter regions are printed with smaller circles more white area   shows an en larged section of a gray scale halftone reproduction Color halftones are printed using dots of various sizes and colors as shown in  Book and maga zine halftones are printed on high quality paper using approximately to cir cles of varying diameter per centimeter Newspapers use lower quality paper and lower resolution about to dots per centimeter  Halftone Approximations In computer graphics halftone reproductions are approximated using rectangu lar pixel regions called halftone patterns or pixel patterns The number of intensity   Color halftone dot patterns The top half of the clock in the color halftone a is enlarged by a factor of in b and by a factor of in c  Section Halftone Patterns and Dithering Techniques   1 O421 06 08 Os    A by pixel grid used to display five intensity evels on a bilevel system The intensity values that would be mapped to each grid are listed below each pixel pattern     Oc O42i O8sl   A 3by pixel grid can be used to display intensities on a bilevel system The intensity values that would be mapped to each grid are listed below each pixel pattern  levels that we can display with this method depends on how many pixels we in clude in the rectangular grids and how many levels a system can display With by  pixels for each grid ona bilevel system we can represent  intensity levels  shows one way to set up pixel patterns to represent five in tensity levels that could be used with a bilevel system In pattern all pixels are turned off in pattern one pixel is turned on and in pattern all tour pixels are turned on An intensity value I in a scene is mapped to a particular pattern ac cording to the range listed below each grid shown in the figure Pattern is used for   pattern for  I  and pattern is used for    With by pixel grids on a bilevel system we can display intensity lev els One way to set up the pixel patterns for these levels is shown in  Pixel positions are chosen at each level so that the patterns approximate the in creasing circle sizes used in halftone reproductions That is the on pixel posi tions are near the center of the grid far lower intensity levels and expand out ward as the intensity level increases  For any pixel grid size we can represent the pixel patterns for the various possible intensities with a mask of pixel position numbers As an example the following mask can be used to generate the nine by grid patterns for intensity levels above shown in  Chapter Illumination Models and Surface  Rendering Methods 5 30  To display a particular intensity with level number k we turn on each pixel whose position number is less than or equal to k  Although the use of by pixel patterns increases the number of intensities that can be displayed they reduce the resolution of the displayed picture by a factor of n along each of the x and y axes A by screen area for in stance is reduced to an area containing by intensity points with by grid patterns And with by patterns we would reduce the by area to intensity positions along each side  Another problem with pixel grids is that subgrid patterns become apparent as the grid size increases The grid size that can be used without distorting the in tensity variations depends on the size of a displayed pixel Therefore for systems with lower resolution fewer pixels per centimeter  we must be satisfied with fewer intensity levels On the other hand high quality displays require at least intensity levels This means that we need by pixel grids And to achieve a res olution equivalent to that of halftones in books and magazines we must display dots per centimeter Thus we need to be able to display x  dots per centimeter Some devices for example high quality film recorders are able to dis play this resolution  Pixel grid patterns for halftone approximations must also be constructed to minimize contouring and other visual effects not present in the original scene Contouring can be minimized by evolving each successive grid pattern from the previous pattern That is we form the pattern at level k by adding an on posi tion to the grid pattern at level k  Thus if a pixel position is on for one grid level it is on for all higher levels Figs 36 and 37  We can minimize the in troduction of other visual effects by avoiding symmetrical patterns With a by pixel grid for instance the third intensity level above zero would be better repre sented by the pattern in  a than by any of the symmetrical arrange ments in  b  The symmetrical patterns in this figure would produce vertical horizontal or diagonal streaks in any large area shaded with intensity level For hard copy output on devices such as film recorders and some print ers isolated pixels are not effectly reproduced Therefore a grid pattern with a single on pixel or one with isolated on pixels as in  should be avoided        ta    For a by pixel grid pattern a is to be preferred to the patterns in b for representing the third intensity level above          Halttone grid patterns with isolated pixels that cannot be effectively reproduced on some hard copy devices                 Intensity levels through obtained with halftone approximations using by pixel grids on a four level system  Halftone approximations also can be used to increase the number of inten sity options on systems that are capable of displaying more than two intensities per pixel For example on a system that can display four intensity levels per pixel we can use by pixel grids to extend the available intensity levels from to In  the four grid patterns above zero now represent several levels each since each pixel position can display three intensity values above zero Fig ure 40 shows one way to assign the pixel intensities to obtain the distinct levels Intensity levels for individual pixels are labeled through and the over all levels for the system are labeled through  Similarly we can use pixel grid patterns to increase the number of intensi ties that can be displayed on a color system As an example suppose we have a three bit per pixel RGB system This gives one bit per color gun in the monitor providing eight colors including black and white  Using by pixel grid pat terns we now have phosphor dots that can be used to represent a particular color value as shown in  Each of the three RGB colors has four phos phor dots in the pattern which allows five possible settings per color This gives us a total of different color combinations  Dithering Techniques The term dithering is used in various contexts Primarily it refers to techniques for approximating halftones without reducing resolution as pixel grid patterns do But the term is also applied to halftone approximation methods using pixel grids and sometimes it is used to refer to color halftone approximations only Random values added to pixel intensities to break up contours are often re ferred to as dither noise Various algorithms have been used to generate the ran   An RGB by pixel grid pattern  Chapter Iumination Models and Surface  Rendering Methods dom distributions The effect is to add noise over an entire picture which tends to soften intensity boundaries  Ordered dither methods generate intensity variations with a one to one map ping of points in a scene to the display pixels To obtain n intensity levels we set up ann by n dither matrix D  whose elements are distinct positive integers in the range to n   For example we can generate four intensity levels with Pr lo 31  and we can generate nine intensity levels with Dy  32 98 The matrix elements for D and D are in the same order as the pixel mask for set ting up by and by pixel grids respectively For a bilevel system we then determine display intensity values by comparing input intensities to the matrix elements Each input intensity is first scaled to the range   n  If the inten sity  is to be applied to screen position x y  we calculate row and column num bers for the dither matrix as i xmoda  j ymodn  33  If f  Dif  we turn on the pixel at position x y  Otherwise the pixel is not turned on  Elements of the dither matrix are assigned in accordance with the guide lines discussed for pixel grids That is we want to minimize added visual effect in a displayed scene Order dither produces constant intensity areas identical to those generated with pixel grid patterns when the values of the matrix elements correspond to the grid mask Variations from the pixel grid displays occur at boundaries of the intensity levels  Typically the number of intensity levels is taken to be a multiple of Higher order dither matrices are then obtained from lower order matrices with the recurrence relation  D  AD  DIA DU 4Dyyy  DX1 2U  _ nf2   34   4Dy  D2 4Day  DA2 U yr assuming n  Parameter U is the unity matrix all elements are  As an example if D is specified as in Eq 31 then recurrence relation 34 yields 5 p  Hol 4 35 10 Another method for mapping a picture with m by n points to a display area with m by pixels is error diffusion Here the error between an input intensity value and the displayed pixel intensity level at a given position is dispersed or diffused to pixel positions to the right and below the current pixel position Starting with a matrix M of intensity values obtained by scanning a photograph we want to construct an array  of pixel intensity values for an area of the screen We do this by first scanning across the rows of M from left to right top to bot tom and determining the nearest available pixel intensity level for each element of M Then the error between the value stored in matrix M and the displayed in tensity level at each pixel position is distributed to neighboring elements in M using the following simplified algorithm  for G i m i  for Y j n j     Determine the available intensity level I      that is closest to the value M    y ls err  M   Miyat M   a err Miny  Mia  Bert  ity i Mini TY ert Morijes  Mister   ert   Once the elements of matrix I have been assigned intensity level values we then map the matrix to some area of a display device such as a printer or video moni tor Of course we cannot disperse the error past the last matrix column j  or below the last matrix row i  m  For a bilevel system the available intensity levels are and Parameters for distributing the error can be chosen to satisfy the following relationship a Bty 8s1 36  One choice for the error diffusion parameters that produces fairly good re sults is a B y   16 16 16 16   illustrates the error distribution using these parameter values Error diffusion sometimes produces ghosts in a picture by repeating or echoing certain parts of the picture partic ularly with facial features such as hairlines and nose outlines Ghosting can be re   column j  ra row i qo Y  NN 7T tI  16 i6 row i      Fraction of intensity error that can be distributed to neighboring pixel positions using an error diffusion scheme  Section Haiftone Patterns and Dithering Techniques  Chapter SS Itlumination Models and Surface  Rendering Methods  48 32 15 31 58 21 50 61 13 38 54 26 44 2z 35 41     11     One possible distribution scheme w  63   for dividing the intensity array into dot diffusion classes numbered from through        duced by choosing values for the error diffusion parameters that sum to a value less than and by rescaling the matrix values after the dispersion of errors One way to rescale is to multiply all elements of M by and then add Another method for improving picture quality is to alternate the scanning of matrix rows from right to left and left to right  A variation on the error diffusion method is dot diffusion In this method the m by n array of intensity values is divided into classes numbered from to as shown in  The error between a matrix value and the displayed intensity is then distributed only to those neighboring matrix elements that have a larger class number Distribution of the class numbers is based on minimiz ing the number of elements that are completely surrounded by elements with a lower class number since this would tend to direct all errors of the surrounding elements to that one position  In this section we consider the application of an illumination model to the ren dering of standard graphics objects those formed with polygon surfaces The ob jects are usually polygon mesh approximations of curved surface objects but they may also be polyhedra that are not curved surface approximations Scan line algorithms typically apply a lighting model to obtain polygon surface ren dering in one of two ways Each polygon can be rendered witha single intensity or the intensity can be obtained at each point of the surface using an interpola tion scheme  Constant Intensity Shading A fast and simple method for rendering an object with polygon surfaces is con stant intensity shading also called flat shading In this method a single inten sity is calculated for each polygon All points over the surface of the polygon are then displayed with the same intensity value Constant shading can be useful for quickly displaying the general appearance of a curved surface as in   In general flat shading of polygon facets provides an accurate rendering for an object if all of the following assumptions are valid   The object is a polyhedron and 1s not an approximation of an object with a curved surface Section Polygon Rendering Methods   The normal vector at vertex V is calculated as the average of the surface normais for each polygon  sharing that vertex   All light sources illuminating the object are sufficiently far from the surface so that N  L and the attenuation function are constarit over the surface   The viewing position is sufficiently far from the surface so that V  R is con stant over the surface  Even if all of these conditions are not true we can still reasonably approximate surface lighting effects using small polygon facets with flat shading and calculate the intensity for each facet say at the center of the polygon  Gouraud Shading This intensity interpolation scheme developed by Gouraud and generally re ferred to as Gouraud shading renders a polygon surface by linearly interpolat ing intensity values across the surface Intensity values for each polygon are matched with the values of adjacent polygons along the common edges thus eliminating the intensity discontinuities that can occur in flat shading  Each polygon surface is rendered with Gouraud shading by performing the following calculations   Determine the average unit normal vector at each polygon vertex  Apply an illumination model to each vertex to calculate the vertex intensity  Linearly interpolate the vertex intensities over the surface of the polygon  At each polygon vertex we obtain a normal vector by averaging the surface normals of all polygons sharing that vertex as illustrated in  Thus for any vertex position V we obtain the unit vertex normal with the calculation Ny k l Ny  Ne k Once we have the vertex normals we can determine the intensity at the vertices from a lighting model   demonstrates the next step interpolating intensities along the polygon edges For each scan line the intensity at the intersection of the scan line with a polygon edge is linearly interpolated from the intensities at the edge end points For the example in  the polygon edge with endpoint vertices at positions and is intersected by the scan line at point A fast method for ob  taining the intensity at point is to interpolate between intensities I and I using only the vertical displacement of the scan line  37    For Gouraud shading the intensity at point is linearly interpolated from the intensities at vertices and The intensity at point is linearly interpolated from intensities at vertices and An interior point p is then assigned an intensity value that is linearly interpolated from x  intensities at positions and   scan line   B41  A  38 Yi Ye mH  Similarly intensity at the right intersection of this scan line point is interpo lated from intensity values at vertices and Once these bounding intensities are established for a scan line an interior point such as point p in  is interpolated from the bounding intensities at points and as _ ne   ey    39 xX Xe X5  X4q Incremental calculations are used to obtain successive edge intensity values between scan lines and to obtain successive intensities along a scan line As shown in  if the intensity at edge position x y is interpolated as po PO  BOY  40 yi Ya i  Y2 then we can obtain the intensity along this edge for the next scan line y  as  I r it honk 41   Ye scan lines  x ox4 t   Incremental interpolation of intensity values along a polygon edge for successive scan lines    A polygon mesh approximation of an object a is rendered with flat shading b and with Gouraud shading c  Similar calculations are used to obtain intensities at successive horizontal pixel positions along each scan line  When surfaces are to be rendered in color the intensity of each color com ponent is calculated at the vertices Gouraud shading can be combined with a hidden surface algorithm to fill in the visible polygons along each scan line An example of an object shaded with the Gouraud method appears in   Gouraud shading remeves the intensity discontinuities associated with the constant shading model but it has some other deficiencies Highlights on the surface are sometimes displayed with anomalous shapes and the linear intensity interpolation can cause bright or dark intensity streaks called Mach bands to ap pear on the surface These effects can be reduced by dividing the surface into a greater number of polygon faces or by using other methods such as Phong shad ing that require more calculations  Phong Shading A more accurate method for rendering a polygon surface is to interpolate normal vectors and then apply the illumination model to each surface point This method developed by Phong Bui Tuong is called Phong shading or normal vector interpolation shading It displays more realistic highlights on a surface and greatly reduces the Mach band effect  A polygon surface is rendered using Phong shading by carrying out the fol lowing steps  e Determine the average unit normal vector at each polygon vertex  Linearly imterpolate the vertex normals over the surface of the polygon   Apply an illumination model along each scan line to calculate projected pixel intensities for the surface points  Interpolation of surface normals along a polygon edge between two vertices is illustrated in  The normal vector N for the scan line intersection point along the edge between vertices and can be obtained by vertically inter polating between edge endpoint normals  Chapter Mumination Models and Surface  Rendering Methods N N scan line  Interpolation of surface normals N along a polygon edge n 4y BON  42  Mi  Y2 no Incremental methods are used to evaluate normals between scan lines and along each individual scan line At each pixel position along a scan line the illumina tion model is applied to determine the surface intensity at that point  Intensity calculations using an approximated normal vector at each point along the scan line produce more accurate results than the direct interpolation of intensities as in Gouraud shading The trade off however is that Phong shading requires considerably more calculations  Fast Phong Shading Surface rendering with Phong shading can be speeded up by using approxima tions in the illumination model calculations of normal vectors Fast Phong shad ing approximates the intensity calculations using a Taylor series expansion and triangular surface patches  Since Phong shading interpolates normal vectors from vertex normals we can express the surface normal N at any point x y over a triangle as N Ax By C 43 where vectors A B and C are determined frum the three vertex equations N  Ax  By  C k 44  with x y  denoting a vertex position Omitting the reflectivity and attenuation parameters we can write the cal culation for light source diffuse reflection from a surface point x y as L N Tau  TNT _ L Ax  By   IL  Ax  By Cl 45  _ Lt Aix  L By tL IL  Ay  By  C We can rewrite this expression in the form ax  by c  dx  exy  fy  gx thy  46  Laut y   where parameters such as a b c and d are used to represent the various dot products For example  L A a  47 I Finally we can express the denominator in Eq 46 as a Taylor series expansion and retain terms up to second degree in x and y This yields dag    Tyx  Taxy  T3y  Trav  Ty  To 48  where each is a function of parameters a b c and so forth  Using forward differences we can evaluate Eq 48 with only two addi tions for each pixel position x y once the initial forward difference parameters have been evaluated Although fast Phong shading reduces the Phong shading calculations it still takes approximately twice as long to render a surface with fast Phong shading as it does with Gouraud shading Normal Phong shading using forward differences takes about six to seven times longer than Gouraud shading  Fast Phong shading for diffuse reflection can be extended to include specu lar reflections Calculations similar to those for diffuse reflections are used to evaluate specular terms such as N  H  in the basic illumination model In ad dition we can generalize the algorithm to include polygons other than triangles and finite viewing positions  In Section 15 we introduced the notion of ray casting where a ray is sent out from each pixel position to locate surface intersections for object modeling using constructive solid geometry methods We also discussed the use of ray casting as a method for determining visible surfaces in a scene Section 10  Ray tracing is an extension of this basic idea Instead of merely looking for the visible surface for each pixel we continue to bounce the ray around the scene as illustrated in  collecting intensity contributions This provides a simple and power ful rendering technique for obtaining global reflection and transmission effects The basic ray tracing algorithm also provides for visible surface detection shadow effects transparency and multiple light source illumination Many ex tensions to the basic algorithm have been developed to produce photorealistic displays Ray traced displays can be highly realistic particularly for shiny ob jects but they require considerable computation time to generate An example of the global reflection and transmission effects possible with ray tracing is shown in   Section Ray Tracing Methods Chapter Illumination Models and Surface Rendering Methods   pixel positions on projection projection plane reference point   Tracing a ray from the projection reference point through a pixel position with multiple reflections and transmissions  Basic Ray Tracing Algorithm We first set up a coordinate system with the pixel positions designated in the xy plane The scene description is given in this reference frame   From the center of projection we then determine a ray path that passes through the center of each screen pixel position Illumination effects accumulated along this ray path are then assigned to the pixel This rendering approach is based on the prin ciples of geometric optics Light rays from the surfaces in a scene emanate in all directions and some will pass through the pixel positions in the projection plane Since there are an infinite number of ray paths we determine the contributions to a particular pixel by tracing a light path backward from the pixel to the scene We first consider the basic ray tracing algorithm with one ray per pixel which is equivalent to viewing the scene through a pinhole camera       gee sah _ A ray traced scene showing global  y  i  at reflection and transmission illumination effects from object Pe oN AG surfaces  Courtesy of Evans  ar   Sutheriand  _ pixel screen area ae centered an viewing coordinate origin  nn ee projection reference point  Ray tracing coordinate reference frame  For each pixel ray we test each surface in the scene to determine if it is in tersected by the ray If a surface is intersected we calculate the distance from the pixel tu the surtace intersection point The smallest calculated intersection dis tance identifies the visible surface for that pixel We then reflect the ray off the visible surface along a specular path angle of refiection equals angle of inci dence  If the surface ts transparent we also send a ray through the surface in the refraction direction Reflection and refraction rays are referred to as secondary rus  This procedure is repeated for each secondary ray Objects are tested for in tersection and the nearest surface along a secondary ray path is used to recur sively produce the next generation of reflection and refraction paths As the rays from a pixel ricochet through the scene each successively intersected surface is added toa binary ray tracing tree as shown in  We use left branches in the tree to represent reflection paths and right branches represent transmission paths Maximum depth of the ray tracing trees can be set as a user option or it can be determined by the amount of storage available A path in the tree is then terminated if it reaches the preset maximum or if the ray strikes a light source  The intensity assigned to a pixel is then determined by accumulating the in tensity contributions starting at the bottom terminal nodes of its ray tracing tree Surface intensity from each node in the tree is attenuated by the distance from the parent surface next node up the tree and added to the intensity of the parent surface Pixel intensity is then the sum of the attenuated intensities at the reat node of the ray tree If no surfaces are intersected by a pixel ray the ray tracing tree is emply and the pixel is assigned the intensity value of the back ground If a pixel ray intersects a nonreflecting light source the pixel can be as signed the intensity of the source although light sources are usuallv placed bevand the path of the initial rays  shows a surface intersected by a ray and the unit vectors needed for the reflected light intensity calculations Unit vector u is in the direc bon of the rav path N is the unit surface normal R is the unit reflection vector L is the unit vector pomting to the light source and H is the unit vector halfway be tween V opposite to u and L The path along L is referred to as the shadow ray It any object intersects the shadow ray between the surface and the point light Section Ray Tracing Methods INumination Models and Surface Rendering Methods  Projection reference point  a    a Reflection and refraction ray paths through a scene for a screen pixel  b Binary ray tracing tree for the paths shown in a  source the surface is in shadow with respect to that source Ambient light at the surface is calculated as k  diffuse reflection due to the source is proportional to KAN  L  and the specular reflection component is proportional to k H  N  As discussed in Section the specular reflection direction for the secondary ray path R depends on the surface normal and the incorning ray direction  R u  Qu N N 49  For a transparent surface we also need to obtain intensity contributions from light transmitted through the material We can locate the source of this con tribution by tracing a secondary ray along the transmission direction T as shown in  The unit transmission vector can be obtained from vectors u and N as T Bu  cos  Ecos N 50 n   r     reflected light ray source  incoming ray  Unit vectors at the surface of an object intersected by an incoming ray along direction u refracted tay path incoming ray   Refracted ray path T through a transparent material  Parameters and are the indices of refraction in the incident material and the refracting material respectively Angle of refraction can be calculated from Snell s law  _ cos  JI     cos   51  Ray Surface Intersection Calculations A ray can be described with an initial position Py and unit direction vector u as illustrated in  The coordinates of any point P along the ray at a distance s from Po is computed from the ray equation  P Pyt su 52  Initailly P can be set to the position of the pixel on the projection plane or it could be chosen to be the projection reference point Unit vector u is initially ob  Section Ray Tracing Methods illumination Models and Surface  Chaoter Rendering Methods   Describing a ray with an initial  x position vector Py and unit direction Z vector u  tained from the position of the pixel through which the ray passes and the projec tion reference point  P  P u ip Pl 53 Pou  Porp At each intersected surface vectors Py and u are updated for the secondary rays at the ray surface intersection point For the secondary rays reflection direction for u is R and the transmission direction is T To locate surface intersections we simultaneously solve the ray equation and the surface equation for the individ ual objects in the scene  The simplest objects lo ray trace are spheres If we have a sphere of radius r and center position P    then any point P on the surface must satisfy the sphere equation  Ip P  r2 54 Substituting the ray equation 52 we have  P su P  r  55   If we let AP  P  Py and expand the dot product we obtain the quadratic equa tion   A ray intersecting a sphere with radius r centered on position P  Section Ray Tracing Methads   A sphereflake rendered with ray tracing using spheres and light sources  Courtesy of Eric Haines SD EYE Inc  s  u AP s   AP   r2  56 whose solution is s u AP V u APY  AP   57  If the discriminant is negative the ray does not intersect the sphere Otherwise the surface intersection coordinates are obtained from the ray equation 52 using the smaller of the two values from Eq 57  For small spheres that are far from the initial ray position Eq 57 is sus ceptible to roundoff errors That is if ri  aP  we could lose the r term in the precision error of  AP  We can avoid this for most cases by rearranging the calculation for distance s as s u AP V7   AP  u AP ul  58   shows a snowflake pattern of shiny spheres rendered with ray trac ing to display global surface reflections  Polyhedra require more processing than spheres to locate surface intersec tions For that reason it is often better to do an initial intersection test on a bounding volume For example  shows a polyhedron bounded by a sphere If a ray does not intersect the sphere we do not need to do any further testing on the polyhedron But if the ray does intersect the sphere we first locate front faces with the test u N 59  where N is a surface normal For each face of the polyhedron that satisifies in equality 59 we solve the plane equation N P D 60  for surface position P that also satisfies the ray equation 52 Here N  A B C Chapter ilumination Models and Surface  Rendering Methods   Polyhedron enclosed by a bounding sphere  and D is the fourth plane parameter Position P is both on the plane and on the ray path if N  Py su  D 61 And the distance from the initial ray position to the plane is _D N Py N u   62  This gives us a position on the infinite plane that contains the polygon face but this position may not be inside the polygon boundaries   So we need to perform an inside outside test Chapter to determine whether the ray in tersected this face of the polyhedron We perform this test for each face satisfying inequality 59 The smallest distance s to an inside point identifies the inter sected face of the polyhedron If no intersection positions from Eq 62 are in side points the ray does not intersect the object  Similar procedures are used to calculate ray surface intersection positions for other objects such as quadric or spline surfaces We combine the ray equation with the surface definition and solve for parameter s In many cases numerical root finding methods and incremental calculations are used to locate intersection plane intersection point La  polygon Figure J4 Ray intersection with the plane of a polygon Section Ray Tracing Methods   A ray traced scene showing global reflection of surface texture patterns  Courtesy of Sun Microsystems  points over a surface  shows a ray traced scene containing multiple objects and texture patterns  Reducing Object Intersection Calculations Ray surface intersection calculations can account for as much as percent of the processing time in a ray tracer For a scene with many objects most of the pro cessing time for each ray is spent checking objects that are not visible along the ray path Therefore several methods have been developed for reducing the pro cessing time spent on these intersection calculations  One method for reducing the intersection calculations is to enclose groups of adjacent objects within a bounding volume such as a sphere or a box Fig   We can then test for ray intersections with the bounding volume If the ray does not intersect the bounding object we can eliminate the intersection tests with the enclosed surfaces This approach can be extended to include a hierarchy of bounding volumes That is we enclose several bounding volumes within a larger volume and carry out the intersection tests hierarchically First we test the outer bounding volume then if necessary we test the smaller inner bounding volumes and so on  Space Subdivision Methods Another way to reduce intersection calculations is to use space subdivision meth ods We can enclose a scene within a cube then we successively subdivide the cube until each subregion cell contains no more than a preset maximum num ber of surfaces For example we could require that each cell contain no more than one surface If parallel arid vector processing capabilities are available the maximum number of surfaces per cell can be determined by the size of the vector  _  bounding sphere  A group of objects enclosed within a bounding sphere Chapter Ilurnination Models and Surace  Rendering Methods   Ray intersection with a cube enclosing all objects in a scene  registers and the number of processors Space subdivision of the cube can be stored in an octree or in a binary partition tree In addition we can perform a uniform subdivision by dividing the cube into eight equal size octants at each step or we can perform an adaptive subdivision and subdivide only those regions of the cube containing objects  We then trace rays through the individual cells of the cube performing in tersection tests only within those cells containing surfaces The first object surface intersected by a ray is the visible surface for that ray There is a trade off between the cell size and the number of surfaces per cell If we set the maximum number of surfaces per cell too low cell size can become so small that much of the sav ings in reduced intersection tests goes into cell traversal processing   illustrates the intersection of a pixel ray with the front face of the cube enclosing a scene Once we calculate the intersection paint on the front face of the cube we determine the initial cell intersection by checking the inter section coordinates against the cell boundary positions We then need to process the ray through the cells by determining the entry and exit points   for each cell traversed by the ray until we intersect an object surface or exit the cube enclosing the scene  Given a ray direction u and a ray entry position P  for a cell the potential exit faces are those for which uN  63  If the normal vectors for the cell faces in  are aligned with the coordi nates axes then   N       Ray traversal through a subregion cell of a cube enclosing a scene  and we only need to check the sign of each component of u to determine the three candidate exit planes The exit position on each candidate plane is obtained from the ray equation  Pours  Pin  64  where is the distance along the ray from P  to Pau Substituting the ray equa tion into the plane equation for each cell face  Ni  Pots  D 65 we can solve for the ray distance to each candidate exit face as  D N P    Nou 66  and then select smallest s  This calculation can be simplified if the norma vec tors N are aligned with the coordinate axes For example if a candidate normal vector is   then for that plane we have XX X    67  where u  u  uy u  and x is the value of the right boundary face for the cell  Various modifications can be made to the cell traversal procedures to speed up the processing One possibility is to take a trial exit plane k as the one perpen dicular to the direction of the largest component of u The sector on the trial plane Tig 64 containing P   determines the true exit plane If the intersec tion point P u  is in sector the trial plane is the true exit plane and we are done If the intersection point is sector the true exit plane is the top plane and we simply need to calculate the exit point on the top boundary of the cell Simi larly sector identifies the bottom plane as the true exit plane and sectors and identify the true exit plane as the left and right cell planes respectively When the trial exit point falls in sector  or we need to carry out two additional intersection calculations to identify the true exit plane Implementation of these methods on parallel vector machines provides further improvements in perfor mance  The scene in  was ray traced using space subdivision methods Without space subdivision the ray tracing calculations took times longer Eliminating the polygons also speeded up the processing For a scene containing spheres and no polygbns the same algorithm executed times faster than the basic ray tracer   illustrates another ray traced scene using spatial subdivision and parallel processing methods This image of Rodin s Thinker was ray traced with over million rays in seconds  The scene shown in  was rendered with a light buffer technique a form of spatial partitioning Here a cube is centered on each point light source and each side of the cube is partitioned with a grid of squares A sorted list of ob jects that are visible to the light through each square is then maintained by the ray tracer to speed up processing of shadow rays To determine surface illumina tion effects the square for each shadow ray is computed and the shadow ray is then processed against the list of objects for that square  Section Ray Tracing Methods     Sectors of the trial exit plane  Chapter Rendering Methods Intersection testa in ray tracing programs can also be reduced with direc Nlumination Models and Surface tional subdivision procedures by considering sectors that contain a bundle of rays Within each sector we can sort surfaces in depth order as in   Each ray then only needs to test objects within the sector that contains that ray  Antialiased Ray Tracing Two basic techniques for antialiasing in ray tracing algorithms are supersampling and adaptive sampling Sampling in ray tracing is an extension of the sampling methods we discussed in Chapter In supersampling and adaptive sampling    A parallel ray traced scene containing spheres and polygon surfaces The ray tracing algorithm used rays per pixel and a tree depth of Spatial subdivision methods processed the scene times faster than the basic ray tracing algorithm on an Alliant FX  Courtesy of Lee Hian Quek information Technology Institute Republic of Singapore  _ aimed se SD ers Lineal ail Mil it aie Leni nan aca    This ray traced scene took seconds to render on a Kendall Square Research KSR1 parallel computer with processors Rodin s Thinker was modeled with primitives Two light sources and one primary ray per pixel were used to obtain the global illumination effects from the 675 rays processed  Courtesy of M   Keates and R J Hubbold Department of Computer Science University of Manchester   a tb     A room scene illuminated with light sources a was rendered using the ray tracing light buffer technique to process shadow rays A closeup b of part of the room shown in a illustrates the global illumination effects The room is modeled with polygons spheres cylinders and quadrics Rendering time was minutes on a VAX 780 compared to minutes without using light buffers  Courtesy of Eric Haines and Donald P Greenberg Program of Computer Graphics Cornell University  Sector fora Bundle of Rays   Directional subdivision of space All rays in this sector only need to test the surfaces within the sector in depth order  the pixel is treated as a finite square area instead of a single point Supersampling uses multiple evenly spaced rays samples over each pixel area Adaptive sam pling uses unevenly spaced rays in some regions of the pixel area For example more rays can be used near object edges to obtain a better estimate of the pixel in tensities Another method for sampling is to randomly distribute the rays over the pixel area We discuss this approach in the next section When multiple rays Rendering Methods       Subdividing a pixel into nine subpixels with one ray at each subpixel corner        Ray positions centered on subpixel areas   Pixel Pasitions on Projection Plane Projection Reference Point   Supersampling with four rays per pixel one at each pixel comer  per pixel are used the intensities of the pixel rays are averaged to produce the overall pixel intensity   illustrates a simple supersampling procedure Here one ray is generated through each corner of the pixel If the intensities for the four rays are not approximately equal or if some small object lies between the four rays we divide the pixel area into subpixels and repeat the process As an example the pixel in  is divided into nine subpixels using rays one at each sub pixel corner Adaptive sampling is then used to further subdivide those subpixels that do not have nearly equal intensity rays or that subtend some small object This subdivision process can be continued until each subpixel has approximately equal intensity rays or an upper bound say  has been reached for the num ber of rays per pixel  The cover picture for this book was rendered with adaptive subdivision ray tracing using Rayshade version on a Macintosh II An extended light source was used to provide realistic soft shadows Nearly million primary rays were generated with million shadow rays and million reflection rays Wood grain and marble surface patterns were generated using solid texturing methods with a noise function Total rendering time with the extended light source was hours Each image of the stereo pair shown in  was generated in hours using a point light source  Instead of passing rays through pixel corners we can generate rays through subpixel centers as in  With this approach we can weight the rays ac cording to one of the sampling schemes discussed in Chapter  Another method for antialiasing displayed scenes is to treat a pixel ray as a cone as shown in  Only one ray is generated per pixel but the ray now has a finite cross section To determine the percent of pixel area coverage with objects we calculate the intersection of the pixel cone with the object surface For a sphere this requires finding the intersection of two circles For a polyhedron we must find the intersection of a circle with a polygon  Distributed Ray Tracing This is a stochastic sampling method that randomly distributes rays according to the various parameters in an illumination model Mlumination parameters in   Projection  Reference Point A pixel ray cone  clude pixel area reflection and refraction directions camera lens area and time Aliasing effects are thus replaced with low level noise  which improves picture quality and allows more accurate modeling of surface gloss and translucency fi nite camera apertures finite light sources and motion blur displays of moving objects Distributed ray tracing also referred to as distribution ray tracing essen tially provides a Monte Carlo evaluation of the multiple integrals that occur in an accurate description of surface lighting  Pixel sampling is accomplished by randomly distributing a number of rays over the pixel surface Choosing ray positions completely at random however can result in the rays clustering together in a small region of the pixel area and leaving other parts of the pixel unsampled A better approximation of the light distribution over a pixel area is obtained by using a technique called jittering on a regular subpixel grid This is usually done by initially dividing the pixel area a unit square into the subareas shown in  and generating a random jitter position in each subarea The random ray positions are obtained by jittering the center coordinates of each subarea by small amounts  and  where both and are assigned values in the interval   We then choose the ray po sition in a cell with center coordinates x y as the jitter position x  y    Integer codes through are randomly assigned to each of the rays and a table lookup is used to obtain values for the other parameters reflection angle time etc  as explained in the following discussion Each subpixel ray is then processed through the scene to determine the intensity contribution for that ray The ray intensities are then averaged to produce the overall pixel inten sity If the subpixel intensities vary too much the pixel is further subdivided  To model camera lens effects we set a lens of assigned focal length f in front of the projection plane and distribute the subpixel rays over the lens area As suming we have rays per pixel we can subdivide the lens area into zones Each ray is then sent to the zone corresponding to its assigned code The ray po sition within the zone is set to a jittered position from the zone center Then the ray is projected into the scene from the jittered zone position through the focal point of the lens We locate the focal point for a ray at a distance f from the lens along the line from the center of the subpixel through the lens center as shown in  Objects near the focal plane are projected as sharp images Objects in front or in back of the focal plane are blurred To obtain better displays of out of focus objects we increase the number of subpixel rays  Ray reflections at surface intersection points are distributed about the spec ular reflection direction R according to the assigned ray codes   The Section Ray Tracing Methods       Pixel sampling using subpixel areas and a jittered ray position from the center coordinates for each subarea  incoming N ray   Distributing subpixel rays about the reflection direction R and the transmission direction T   Distributing subpixel rays over a Direction camera lens of focal length f  maximum spread about R is divided into angular zones and each ray is re flected in a jittered position from the zone center corresponding to its integer code We can use the Phong model cos  to determine the maximum reflection spread If the material is transparent refracted rays are distributed about the transmission direction T in a similar manner  Extended light sources are handled by distributing a number of shadow rays over the area of the light source as demonstrated in  The light source is divided into zones and shadow rays are assigned jitter directions to the various zones Additionally zones can be weighted according to the intensity of the light source within that zone and the size of the projected zone area onto the object surface More shadow rays are then sent to zones with higher weights If some shadow rays are blocked by opaque objects between the surface and the light source a penumbra is generated at that surface point  illus trates the regions for the umbra and penumbra on a surface partially shielded from a light source  We create motion blur by distributing rays over time A total frame time and the frame time subdivisions are determined according to the motion dynam ics required for the scene Time intervals are labeled with integer codes and each ray is assigned to a jittered time within the interval corresponding to the ray code Objects are then moved to their positions at that time and the ray is traced ub   Cee Extended N EOE Light L re  max Source   Distributing shadow rays over a finite sized light source  Earth  Penumbra   Umbra and penumbra regions created by a solar eclipse on the surface of the earth Section Ray Tracing Methods    A scene entitled rendered with distributed ray tracing illustrating motion blur and penumbra effects  Courtesy of Pixar  Pixar All rights reserved  through the scene Additional rays are used for highly blurred objects To reduce calculations we can use bounding boxes or spheres for initial ray intersection tests That is we move the bounding object according to the motion requirements and test for intersection If the ray does not intersect the bounding object we do not need to process the individual surfaces within the bourding volume Figure 78 shows a scene displayed with motion blur This image was rendered using distributed ray tracing with by pixels and rays per pixel In addition to the motion blurred reflections the shadows are displayed with penumbra areas resulting from the extended light sources around the room that are illumi nating the pool table  Additional examples of objects rendered with distributed ray tracing meth ods are given in Figs 79 and 80   illustrates focusing refrac tion and antialiasing effects with distributed ray tracing   A brushed aluminum wheel showing reflectance and shadow effects generated with distributed ray tracing techniques  Courtesy of Stephen H Westin Program of Computer Graphics Cornell University      A room scene rendered with distributed ray tracing methods  Courtesy of Jokn Snyder Jed Lengyel Devendra Kalra and Al Barr Computer Graphics Lab California Institute of Technology Copyright  Caltech    Ascene showing the focusing antialiasing and illumination effects possible with a combination of ray tracing and radiosity methods Realistic physical models of light illumination were used to generate the refraction effects including the caustic in the shadow of the glass  Courtesy of Peter Shirley Department of Computer Science Indiana University   We can accurately mode diffuse reflections from a surface by considering the ra diant energy transfers between surfaces subject to conservation of energy laws This method for describing diffuse reflections is generally referred to as the ra diosity model  Basic Radiosity Model In this method we need to consider the radiant energy interactions between all surfaces in a scene We do this by determining the differential amount of radiant energy dB leaving each surface point in the scene and summing the energy con tributions over all surfaces to obtain the amount of energy transfer between sur faces With reference to  dB is the visible radiant energy emanating from the surface point in the direction given by angles and  within differential solid angle dw per unit time per unit surface area Thus dB has units of joules sec ond  meter  or watts meter   Intensity or luminance of the diffuse radiation in direction   is the ra diant energy per unit time per unit projected area per unit solid angle with units watts meter  steradians  dB    _ dw cos  68    Visible radiant energy emitted from a surface point in direction   within solid angle dw   Direction of Energy Transfer   or a unit surface element the projected area perpendicular to the direction of energy transfer is equal tocos   Assuming the surface is an ideal diffuse reflector we can set intensity I to a con stant for all viewing directions Thus dB dw is proportional to the projected sur face area   To obtain the total rate of energy radiation from the surface point we need to sum the radiation for all directions That is we want the to  tal energy emanating from a hemisphere centered on the surface point as in   B 48 69  For a perfect diffuse reflector  is a constant so we can express radiant energy B as B if cos deo 70  Also the differential element of solid angle dw can be expressed as Appendix A  dw    sinpdd d    Total radiant energy from a surface point is the sum of the contributions in all directions over a hemisphere centered on the surface point  Section Radiosity Lighting Model Chapter illumination Models and Surface  Rendering Methods  Surface k   An enclosure of surfaces for the radiosity model  so that 2a WZ B  cosdsinddgde 71 In i A model for the light reflections from the various surfaces is formed by set ting up an enclosure of surfaces   Each surface in the enclosure is ei ther a reflector an emitter light source  or a combination reflector emitter We designate radiosity parameter B as the total rate of energy leaving surface k per unit area Incident energy parameter H is the sum of the energy contributions from all surfaces in the enclosure arriving at surface k per unit time per unit area That is  Hy   BF 72 i where parameter Fj is the form factor for surfaces j and k Form factor F is the fractional amount of radiant energy from surface j that reaches surface k  For a scene with n surfaces in the enclosure the radiant energy from surface kis described with the radiosity equation  B  Ey  ply n 73  E a BF jel If surface k is not a light source F   Otherwise E is the rate of energy emitted from surface k per unit area watts meter  Parameter p is the reflectivity factor for surface k percent of incident light that is reflected in all directions  This re flectivity factor is related to the diffuse reflection coefficient used in empirical il lumination models Plane and convex surfaces cannot see themselves so that no self incidence takes place and the form factor F  for these surfaces is To obtain the illumination effects over the various surfaces in the enclosure we need to solve the simultaneous radiosity equations for the n surfaces given the array values for F  p  and Fi That is we must solve   pF eBe  BF  Ee k 23   74 jtk or pFy prFi2  AF B E TPif pe mn PY Be PLD Be  95 Pin PF Fae  B E  We then convert to intensity values I by dividing the radiosity values B by For color scenes we can calculate the individual RGB components of the radios ity Bz Big Byp from the color components of p and E  Before we can solve Eq 74 we need to determine the values for form factors F  We do this by considering the energy transfer from surface j to surface k   The rate of radiant energy falling on a small surface element dA from area element dA is 4B dA  U cosd dwidA  76  But solid angle dw can be written in terms of the projection of area element dA perpendicular to the direction dB  _ 4A _ cosddA  dw r  77  Surface k Surface j   Rate of energy transfer dB from a surface element with area dA to surface element dA  Section Radiosity Lighting Model Chapter IHumination Models and Surface  Rendering Methods so we can express Eq 76 as dB dA  I cos cos dy dA dA mu r2 78  The form factor between the two surfaces is the percent of energy emanating from area dA that is incident on dA  _ energy incident on dA  E  Ar44  total energy leaving dA  79 I cos dj COs d aA dA B dA Also B  mi so that r cos  cos dy dA dA day  a 80  The fraction of emitted energy from area dA incident on the entire surface k is then cos cos dy Faa a  dA aay Ay Wc ar  81  where A is the area of surface k We now can define the form factor between the two surfaces as the area average of the previous expression  cos cos Fs    b COSHr A dA A  suet J sur 82  Integrals 82 are evaluated using numerical integration techniques and stipu lating the following conditions   Sp Fx  for all k conservation of energy  AF  Afi  uniform light reflection  F   for all j assuming only plane or convex surface patches  Each surface in the scene can be subdivided into many small polygons and the smaller the polygon areas the more realistic the display appears We can speed up the calculation of the form factors by using a hemicube to approximate the hemisphere This replaces the spherical surface with a set of linear plane surfaces Once the form factors are evaluated we can solve the simultaneous lin ear equations 74 using say Gaussian elimination or LU decomposition meth ods Appendix A  Alternatively we can start with approximate values for the B and solve the set of linear equations iteratively using the Gauss Seidel method At each iteration we calculate an estimate of the radiosity for surface patch k using the previously obtained radiosity values in the radiosity equation  n B Ey  p BiFn yl We can then display the scene at each step and an improved surface rendering is viewed at each iteration until there is little change in the calculated radiosity val ues Progressive Refinement Radiosity Method Although the radiosity method produces highly realistic surface rendings there are tremendous storage requirements and considerable processing time is needed to calculate the form factors Using progressive refinement we can restruc ture the iterative radiosity algorithm to speed up the calculations and reduce storage requirements at each iteration  From the radiosity equation the radiosity contribution between two surface patches is calculated as B due to B  pp BF  83 Reciprocally B due to By  pBiFy  for all j 84 which we can rewrite as B due to B  Bagh for all  85  This relationship is the basis for the progressive refinement approach to the ra diosity calculations Using a single surface patch k we can calculate all form fac tors F and shoot light from that patch to all other surfaces in the environment Thus we need only to compute and store one hemicube and the associated form factors at a time We then discard these values and choose another patch for the next iteration At each step we display the approximation to the rendering of the scene  Initially we set B  E for all surface patches We then select the patch with the highest radiosity value which will be the brightest light emitter and calcu late the next approximation to the radiosity for all other patches This process is repeated at each step so that light sources are chosen first in order of highest ra diant energy and then other patches are selected based on the amount of light re ceived from the light sources The steps in a simple progressive refinement ap froach are given in the following algorithm  Section Radiosity Lighting Mcdel Chapter Illumination Models and Surface  Rendering Methods   Nave of Chartres Cathedral rendered with a progressive refinement radiosity model by John Wallace and John Lin using the Hewlett Packard Starbase Radiosity and Ray Tracing software Radiosity form factors were computed with  Tay tracing methods  Courtesy of Eric Haines 3D EYE Inc   Hewlett Packard Co   for each patch k set up hemicube calculate form factors F   for each patch   drad  BF A Ac 4B   4B  Arad B   B  Srad  AB     At each step the surface patch with the highest value for AB A is selected as the shooting patch since radiosity is a measure of radiant energy per unit area And we choose the initial values as AB  B  E for all surface patches This progres sive refinement algorithm approximates the actual propagation of light through a scene  Displaying the rendered surfaces at each step produces a sequence of views that proceeds from a dark scene to a fully illuminated one After the first step the only surfaces illuminated are the light sources and those nonemitting patches that are visible to the chosen emitter To produce more useful initial views of the scene we can set an ambient light level so that all patches have some illumina tion At each stage of the iteration we then reduce the ambient light according to the amount of radiant energy shot into the scene   shows a scene rendered with the progressive refinement ra diosity model Radiosity renderings of scenes with various lighting conditions are illustrated in Figs 88 to 90 Ray tracing methods are often combined with the radiosity model to produce highly realistic diffuse and specular surface shadings as in  Section Radiosity Lighting Model   Image of a constructivist museum rendered with a progressive refinement radiosity method  Courtesy of Shenchang Eric Chen Stuart  Feldman and Julie Dorsey Program of Computer Grapitics Cornell University   Cornell University Program of Computer Graphics        Simulation of the stair tower of the Engineering Theory Center Building at Cormell University rendered with a progressive refinement radiosity method  Courtesy of Keith Howie and Ben Trumbore Program of Computer Graphics Cornell University   Cornell University Program of Computer Graphics   fa  b    Simulation of two lighting schemes for the Parisian garret from the Metropolitan Opera s production of La Boheme  a day view and b night view  Courtesy of Julie Dorsey and Mark Shepard Program of Computer Graphics Cornell University   Cornell University Program of Computer Graphics   Spherical Environment  Map  Oxyects meee co in Scene  A spherical enclosing universe ee containitg the environment map  ENVIRONMENT At PPING An alternate procedure tor modeling global reflections 1s to define an array of in tensity values that describes the environment around a single object or a set of objects Instead of intercbject rav tracing or radiosity calculations to pick up the global specular and dittuse illumination effects we simply map the environment array onto a2 obleect in relationship to the viewing direction This procedure is re ferred to as environment mapping also called reflection mapping although transparency effects cond also be modeled with the environment map Environ ment mapping is sometimes referred to as the pocr person s ray tracing method since it fast approximation of the more accurate global illumination rendering techniques we discussed in the previous two sections  The environment map is defined over the surface of an enclosing universe Information in the environment map includes intensity values for light sources the sky and other backy ound objects  shows the enclosing universe as a sphere but a cube or a cylinder is often used as the enclosing universe  To render the surtace of an object we project pixel areas onto the surface and then reflect the projected pixel area onto the environment map to pick up the surface shading attributes for each pixel If the object is cransparent we can also refract the projected pial area to the environment map The environment map ping process for reflection of a projected pixel area is illustrated in  Pixel intensity is determined by averaging the intensity values within the inter sected region of the environment map  oN p Surface Pixel Projection   onto Enviranment Map  _ Figure 92 Projecung a pixel area to a surface Picection then reflecting the area to the Refers ce Point environtient map  So far we have discussed rendering techniques for displaying smooth surfaces typically polygons or splines However most objects do not have smooth even surfaces We need surface texture to model accurately such objects as brick walls gravel roads and shag carpets In addition some surfaces contain patterns that must be taken into account in the rendering procedures The surface of a vase could contain a painted design a water glass might have the family crest en graved into the surface a tennis court contains markings for the alleys service areas and base line and a four lane highway has dividing lines and other mark ings such as oil spills and tire skids  illustrates objects displayed with various surface detail  Modeling Surface Detail with Polygons A simple method for adding surface detail is to model structure and patterns with polygon facets For large scale detail polygon modeling can give good re sults Some examples of such large scale detail are squares on a checkerboard di viding lines on a highway tile patterns on a linoleum floor floral designs in a smooth low pile rug panels in a door and lettering on the side of a panel truck Also we could model an irregular surface with small randomly oriented poly gon facets provided the facets were not too small   Scenes illustrating computer graphics generation of surface detail  a  Deborah R Fi  Premyslaw Prusinkiewicz and Johannes Battjes  b  Deborah R Fowler Hans Metnhardt and Przemysiaw Prusinkiewicz University of Calgary  c and d Courtesy of SOFTIMAGE Inc   Section Adding Surface Detail Chapter Niumination Models aud Surface  Rendering Methods       Texture Object Image Space Space Space  s t Array u v Surface ix y Pixel Coordinates Parameters Coordinates Texture Surface Viewing and Transformation Projection Transformation   Coordinate reference systems for texture space object space and image space  Surface pattern polygons are generally overlaid on a larger surface polygon and are processed with the parent surface Only the parent polygon is processed by the visible surface algorithms but the illumination parameters for the surface detail polygons take precedence over the parent polygon When intricate or fine surface detail is to be modeled polygan methods are noi practical For example it would be difficult to accurately model the surface structure of a raisin with polygon facets  Texture Mapping A common method for adding surface detail is to map texture patterns onto the surfaces of objects The texture pattern may either be defined in a rectangular array or as a procedure that modifies surface intensity values This approach is referred to as texture mapping or pattern mapping  Usually the texture pattern is defined with a rectangular grid of intensity values in a texture space referenced with s t coordinate values as shown in Fig 94 Surface positions in the scene are referenced with wv object space coordi nates and pixel positions on the projection plane are referenced in xy Cartesian coordinates Texture mapping can be accomplished in one of two ways Either we can map the texture pattern to object surfaces then to the projection plane or we can map pixel arcas onto object surfaces then to texture space Mapping a texture pattern to pixel coordinates is sometimes called texture scanning while the map ping from pixel coordinates to texture space is referred to as pixel order scanning Or inverse scanning or imuiye order scanning  To simplify calculations the mapping from texture space to object space is often specified with parametric linear functions w fist  as  bt e  48a v fGt  a s bf c  The object to image space mapping is accomplished with the concatenation of the viewing and projection transformations A disadvantage of mapping from texture space to pixel space is thal a selected texture patch usually does not match up with the pixel boundaries thus requiring calculation of the fractional area of pixel coverage Therefore mapping from pixel space to texture space Fig 95 is the most commonly used texture mapping method This avoids pixel subdivision calculations and allows antialiasing filtering procedures to be eas Projected     Pixel Area my ww Thee ae re  Phat oe me me Pixel Surface Area Rectangular Pattern Array   Texture mapping by projecting pixel areas to texture space  Extended  Pixel Area      Extended area for a pixel that includes centers of adjacent pixels     ily applied An effective antialiasing procedure is to project a slightly larger pixel area that includes the centers of neighboring pixels as shown in  and applying a pyramid function to weight the intensity values in the texture pattern But the mapping from image space to texture space does require calculation of the inverse viewing projection transformation Mj and the inverse texture map transformation M7  In the following example we illustrate this approach by mapping a defined pattern onto a cylindrical surface  Example Texture Mapping To illustrate the steps in texture mapping we consider the transfer of the pattern shown in  to a cylindrical surface The surface parameters are with Chapter  lucunation Madels and Surface  Render ng Methods   z NS os NO uv  ao  y s x e c Of O75 fa  b   Mapping a texture pattern defined on a unit square a to a cylindrical surface b  And the parametric representation for the surface in the Cartesian reference frame is   FrcOsh Y  rsini  te We can map the arrav pattern to the surface with the following linear transforma tion which maps the pattern origin to the lower left corner of the surface  u  s7  v t Next we select a viewing position and perform the inverse viewing transforma tion from pixel coordinates to the Cartesian reference for the cylindrical surface Cartesian coordinates are then mapped to the surface parameters with the trans formation w tan Gy x  v z and projected pixel positians are mapped to texture space with the inverse trans formation s 2u m t il Intensity values in the pattern array covered by each projected pixel area are then averaged to obtain the pixel intersity  Procedural Texturing Methods Another method for adding surface texture is to use procedural definitions of the color variations that are to be applied to the objects in a scene This approach avoids the transfarmation calculations invelved in transferring two dimensional texture patterns to object surfaces  When values are assigned throughout a region of three dimensional space the object color variatiors are referred to as salid textures Values from fexture  A scene with surface characteristics generated using solid texture methods  Courtesy of Peter Shirley  Computer Science Department Indiana University   space are transferred to object surfaces using procedural methods since it is usu ally impossible to store texture values for all points throughout a region of space Other procedural methods can be used to set up texture values over two dimen sional surfaces Solid texturing allows cross sectional views of three dimensional abjects such as bricks to be rendered with the same texturing as the outside sur faces  As examples of procedural texturing wood grains or marble patterns can be created using harmonic functions sine curves defined in three dimensional space Random variations in the wood or marble texturing can be attained by su perimposing a noise function on the harmonic variations  shows a scene displayed using solid textures to obtain wood grain and other surface pat terns The scene in  was rendered using procedural descriptions of ma terials such as stone masonry polished gold and banana leaves   A scene rendered with VG Shaders and modeled with RenderMan using polygonal facets for the gem faces quadric surfaces and bicubic patches In addition to surface texturing procedural methods were used to create the steamy jungle atmosphere and the forest canopy dappled lighting effect  Courtesy of the VALIS Group Reprinted from Graphics Gems Il edited by David Kirk Copyright  Academic Press Inc   Section Adding Surface Detail Chapter Nlumination Models and Surface  Rendering Methods Bump Mapping Although texture mapping can be used to add fine surface detail it is not a good method for modeling the surface roughness that appears on objects such as or anges strawberries and raisins The illumination detail in the texture pattern usually does not correspond to the illumination direction in the scene A better method for creating surface bumpiness is to apply a perturbation function to the surface normal and then use the perturbed normal in the illumination model cal culations This techniques is called bump mapping If P u v represents a position on a parametric surface we can obtain the surface normal at that point with the calculation N P x P  87 where P and P  are the partial derivatives of P with respect to parameters u and v To obtain a perturbed normal we modify the surface position vector by adding a small perturbation function called a bump function P u v  Plu v  b u v n 88  This adds bumps to the surface in the direction of the unit surface normal n  N Nj  The perturbed surface normal is then obtained as N PxXP 89  We calculate the partial derivative with respect to w of the perturbed position vector as P Le  bn  90  P  bn  bn  Assuming the bump function is small we can neglect the last term and write  P  P  bn 91 Similarly  P P  n 92 And the perturbed surface normal is N P xP  bP X n  b n x P   b n X n  But n X n  50 that N N P Xn  b n x PL  93  The final step is to normalize N for use in the iJlumination model calculations    Surface roughness characteristics rendered with bump mapping  Courtesy of a Peter Shirley Computer Science Department Indiana University and b SOFTIMAGE Inc     The stained glass knight from the motion picture Young Sherlock Holmes A combination of bump mapping environment mapping and texture mapping was used to render the armor surface  Courtesy of Industrial Light  Magic Copyright  Paramount Pictures Amblin   There are several ways in which we can specify the bump function b u v  We can actually define an analytic expression but bump values are usually ob tained with table lookups With a bump table values for b can be obtained quickly with linear interpolation and incremental calculations Partial derivatives b and b are approximated with finite differences The bump table can be set up with random patterns regular grid patterns or character shapes Random pat terns are useful for modeling irregular surfaces such as a raisin while a repeat ing pattern could be used to model the surface of an orange for example To an tialiase we subdivide pixel areas and average the computed subpixel intensities   shows examples of surfaces rendered with bump mapping An example of combined surface rendering methods is given in  The armor for the stained glass knight in the film Yourg Sherlock Holmes was rendered with a combination of bump mapping environment mapping and texture map ping An environment map of the surroundings was combined with a bump map to produce background illumination reflections and surface roughness Then ad ditional color and surface illumination bumps spots of dirt ard stains for the seams and rivets were added to produce the overall effect shown in   Frame Mapping This technique is an extension of bump mapping In frame mapping we perturb both the surface normal N and a local coordinate system   attached to Section Adding Surface Detail Chapter Nluminatian Models and Surtat e  Rendering Methods Figure H 1o2 A local coordinate system at a surface point  N The local coordinates are defined with a surface tangent vector T and a binor mal veclor B TN Frame mapping used to model anisotropic surfaces We onent T along the grain of the surtace and apply directional perturbations in addition to bump perturbations ir che direction of N In this way we can model wood grain patterns cross thread patterns in cloth and streaks in marble or similar materi als Both bump and directional perturbations can be obtained with table lookups  SUMMARY In general an object 1s illuminated with radiant energy from light emitting soutces and from the reflective surfaces of other objects in the scene Light sources can be modeled es point sources or as distributed extended sources Ob jects can be either opaque or transparent And lighting effects can be described in terms of diffuse and specular components for both reflections and refractions  An empirical puint light source iumination model can be used to de scribe diffuse reflections with Lambert s cosine law and to describe specular re flections with the Mhony model General background ambient lighting can be modeled with a tixed intensity level and a coefficient ot reflection for each sur face In this basic model we can approximate transparency effects by combining surface intensities using transparency coefficient Accurate geometric modeling of light paths through transparent materials is obtained by calculating refraction angles using Snell s law Color is incorporated into the mode by assigning a triple of RGB values te intensities and surface reflection coefficients We can also extend the basic model to incorporate distributed light sources studio lighting effects and intensity attenuation  Intensity values calculated with an illumination model must be mapped to the intensity levels available on the display system in use A loganthmic intensity scale is used to provide set of intensity Ievels with equal perceived brightness In addition gamma correction is applied to intensity values to correct for the nonlinearity of diaplay devices With bilevel monitors we can use halftone pat terns and dithering techniques to simulate a range of intensity values Halftone approximations can also be used to increase the number ef intensity options on systems that are capable of displaying more than two irtensities per pixel Or dered dither error diffision and dot diffusion methods are used to simulate a range of intensities when the number of points to be plotted in a scene is equal to the number of pixels on the display device  Surface rendering can be accomplished by applying a basic illumination model to the objects in a scence We apply an illumination model using either con stant intensity shading Gouraud shading or Phong shading Constant shading is accurate for polyhedrons or for curved surface polygon meshes when the References viewing and light source positions are far from the objects in a scene Gouraud shading approximates light reflections from curved surfaces by calculating inten  sity values at polygon vertices and interpolating these intensity values across the polygon facets A more accurate but slower surface rendering procedure is Phong shading which interpolates the average narmal vectors for polygon ver  tices over the polygon facets Then surface intensities are calculated using the in  terpolated normal vectors Fast Phong shading can be used to speed up the calcu  lations using Taylor series approximations  Ray tracing provides an accurate n ethod for obtaining global specular re flection and transmission effects Pixel rays are traced through a scene bouncing from object to object while accumulating intensity contributions A ray tracing tree is constructed for each pixel and intensity values are combined from the ter minal nodes of the tree back up to the root Object intersection calculations in ray tracing can be reduced with space subdivision methods that test for ray object in tersections only within subregions of the total space Distributed or distribution ray tracing traces multiple rays per pixel and distributes the rays randomly over the various ray parameters such as direction and time This provides an accurate method for modeling surface gloss and translucency finite camera apertures dis tributed light sources shadow effects and motion blur  Radiosity methods provide accurate modeling of diffuse reflection effects by calculating radiant energy transfer between the various surface patches in a scene Progressive refinement is used to speed up the radiosity calculations by considering energy transfer from one surface patch at a time Highly photorealis tic scenes are generated using a combination of ray tracing and radiosity  A fast method for approximating global illumination effects is environment mapping An environment array is used to store background intensity informa tion for a scene This array is then mapped to the objects in a scene based on the specified viewing direction  Surface detail can be added to objects using polygon facets texture map ping bump mapping or frame mapping Small polygon facets can be overlaid on larger surfaces to provide various kinds of designs Alternatively texture pat terns can be defined in a two dimensional array and mapped to object surfaces Bump mapping is a means for modeling surface irregularities by applying a bump function to perturb surface normals Frame mapping is an extension of bump mapping that allows for horizontal surface variations as well as vertical variations  REFERENCES A general discussion of energy propagation transfer equations rendering processes and our perception of light and color is given in Glassner  Algorithms for various surface rendering techniques are presented in Glassner  Arvo  and Kirk  For further discussion of ordered dither error diffusion and dot diffusion see Knuth  Additional information on ray tracing methods can be tound in Quek and Hearn  Glassner  Shirley  and Koh and Hearn  Radiosity methods are dis cussed in Goral et al   Cohen and Greenberg  Cohen et al   Wallace Elmquist and Haines  Chen et al   Dorsey Sillion and Greenberg  He et al   Sillion et al   Schoeneman et al  and Lischinski Tampieri and Greenberg   Chapter t4  Murination Models and Surface  Rendering Methods  EXERCISES      10  11 12  13  14 15 16 17 18 19  20 Write a routine to imelement Eq of the basic illumination mode using a single point light source and constant surface shading for the faces of a specified polyhe dron The object description is to be given as a set of polygon tables including sur face normals for each of the polygon faces Additional input parameters include the ambient intensity light source intensity and the sudace reflection coefficients All coordinate information can be specified directly in the viewrng reference frame  Modify the routine n Exercise to render a polygon surface mesh using Gouraud shading  Modify the routine in Exercise to render a polygon surface mesh using Phong shading Write a routine to implement Eq of the basic illumination model using a single point light source and Gouraud surface shading for the tales of a specified polygon mesh The object des ription is to be given as a set of polygon tables including sur face normals for each of the polygon faces Additional input includes values for the ambient intensity light source intensity surface reflection cvefficients and the spec ular reflection parameter All coordinate information can be specified directly in the viewing reference frame Modify the routine in  xercise to render the polygon surfaces using Phong shad ing  Modify the routine in Exercise to include a linear intensity attenuation func tion Modify the routine in Exercise  to render the pulygon surfaces using Phong shad ing and a linear intens ty attenuation function  Modify the routine in ixercise to impiement Eq 13 with any specified num ber of polyhedrans and light sources in the scene  Modify the routine in xercise to implement Eq 14 with any specified num ber of polyhedrons and light sources in the scene  Modify the routine in Exercise to implement Fq 15 with any specified num ber of polyhedrons and light sources iy the scene  Modify the routine t Exercise to implernent Eqs 15 aid 19 with any specified number of ight sources and polyhedrons e ther opaque or transparent in the scene Discuss the differenies you might expect lo see in the appearance of specula retlec tions modeled with N  H  compared to specular reflections modeled with V Rr  Verify that 2a  din Fip 18 when all vectors are coplanar but that in general 2a o  Discuss how the diterent visible surface detection me hous can he cambined with an intensity model for displaying a set of pelyhedrons with opaque sucaces Discuss how the var ous visinle suface detection Mmetyods can be mod tied ta process transparent ofyects Are there ary visible surtace detection methods tho cannut Fandle transparent surlaces  Set up algorithms based on one ot the wisibie surtace detection methods that wil identify shadow areas na scene illuminated by a distant port source How many intensity levels can be displayed with halftane approximations using by npixel grids where each pixel can be displayed with mm citterent intensities  How many different Color combinattons can be generated sing halttone approxi ttons on twa level RGB system with a  by pixel grd  Write a routine to disp av a given set of surrace intensily varatians using halftone ap proximations with  by pixel grids and two intensity levels and 1s per pixel Write a ouline to generale ordered dither matrices using the recurrence relation in Eq 34 21  22  23  24  25  26  27  28  29 30 31 32  33  Write a procedure to display a given array of intensity values using the ordered dither method  Write a procedure to implement the error diffusion algorithm for a given m by n array of intensity values  Write a program to implement the basic ray tracing algorithm for a scene containing a single sphere hovering over a checkerboard ground square The scene is to be illu minated with a single point light source at the viewing position  Write a program to implement the basic ray tracing algorithm for a scene containing any specified arrangement of spheres and polygon surfaces illuminated by a given set of point light sources  Write a program to implement the basic ray tracing algorithm using space subdivi sion methods for any specified arrangement of spheres and polygon surfaces iNumi nated by a given set of point light sources  Write a program to implement the following features of distributed ray tracing pixel sampling with jittered rays per pixel distributed reflection directions distributed tefraction directions and extended light sources  Set up an algorithm for modeling the motion blur of a moving object using distrib uted ray tracing  Implement the basic radiosity algorithm for rendering the inside surfaces of a cube when one inside face of the cube is a light source  Devise an algorithm for implementing the progressive refinement radiosity method Write a routine to transform an environment map to the surface of a sphere  Write a program to implement texture mapping tor a spherical surfaces and b polyhedrons  Given a spherical surface write a bump mapping procedure to generate the bumpy surface of an orange  Write a bump mapping routine to produce surface normal variations for any speci fied bump function  Exercises CHAPTER  Color Models and Color Applications    ob4  ur discussions of color up to this point have concentrated on the mecha nisms for generating color displays with combinations of red green and blue light This mode is helpful in understanding how color is represented on a video monitor but several other color models are useful as well in graphics ap plications Some models are used to describe color output on printers and plot ters and other models provide a more intuitive color parameter interface for the user  A color model is a method for explaining the properties or behavior of color within some particular context No single color model can explain all as pects of color so we make use of different models to help describe the different perceived characteristics of color  PROPERTIES OF LIGHT What we perceive as light  or different colors is a narrow frequency band within the electromagnetic spectrum A few of the other frequency bands within this spectrum are called radio waves microwaves infrared waves and X rays  shows the approximate frequency ranges for some of the electromag  netic bands  Each frequency value within the visible band corresponds to a distinct color At the low frequency end is a red color X hertz  and the highest frequency we can see is a violet color X hertz  Spectral colors range from the reds through orange and yellow at the low frequency end to greens blues and violet at the high end   z   29 s z Z g e  gs    e  i    x          Frequency 108  1081978 thertz   Electromagnetic spectrum  Chapter Colo Models and Color Applications a   Time variations for one electric frequency component of a plane polarized electromagnetic wave  Since light is an electromagnetic wave we can describe the various colors in terms of either the frequency f or the wavelength A of the wave In  we illustrate the oscillations present in a monochromatic electromagnetic wave po larized so that the electric oscillations are in one plane The wavelength and fre quency of the monochromatic wave are inversely proportional to each other with the proportionality constant as the speed of light c  c a  Frequency is constant for all materials but the speed of light and the wavelength are material dependent Ina vacuum c  X cm sec Light wavelengths are very small so length units for designating spectral colors are usually either angstroms 1A  cm or nanometers nm  cm  An equivalent term for nanometer is millimicron Light at the red end of the spectrum has a wave length of approximately nanometers nm  and the wavelength of the violet light at the other end of the spectrum is about nm Since wavelength units are somewhat more convenient to deal with than frequency units spectral colors are typically specified in terms of wavelength  A light source such as the sun or a light bulb emits all frequencies within the visible range to produce white light When white light is incident upon an ob ject some frequencies are reflected and some are absorbed by the object The combination of frequencies present in the reflected light determines what we per ceive as the color of the object If low frequencies are predominant in the reflected light the object is described as red In this case we say the perceived light has a dominant frequency or dominant wavelength at the red end of the spectrum The dominant frequency is also called the hue or simply the color of the light  Other properties besides frequency are needed to describe the various char acteristics of light When we view a source of light our eves respond to the color or dominant frequency and two other basic sensations One of these we call the brightness which is the perceived intensity of the light Intensity is the radiant energy emitted per unit time per unit solid angle and per unit projected area of the source Radiant energy is related to the Juminance of the source The second Energy  Frequency  Energy distribution of a white light source  Red Violet  perceived characteristic is the purity or saturation of the light Purity describes how washed out or how pure the color of the light appears Pastels and pale colors are described as less pure These three characteristics dominant frequency brightness and purity are commonly used to describe the different properties we perceive in a source of light The term chromaticity is used to refer collectively to the two properties describing color characteristics purity and dominant fre quency  Energy emitted by a white light source has a distribution over the visible frequencies as shown in  Each frequency component within the range from red to violet contributes more or less equally to the total energy and the color of the source is described as white When a dominant frequency is present the energy distribution for the source takes a form such as that in  We would now describe the light as having the color corresponding to the dominant frequency The energy density of the dominant light component is labeled as Ep in this figure and the contributions from the other frequencies produce white light of energy density Ey We can calculate the brightness of the source as the area under the curve which gives the total energy density emitted Purity de pends on the difference between Ep and Ey The larger the energy Ep of the dom inant frequency compared to the white light component Ey the more pure the light We have a purity of percent when Fy  and a purity of percent when Ey  Ep  When we view light that has been formed by a combination of two or more sources we see a resultant light with characteristics determined by the original sources Two different color light sources with suitably chosen intensities can be used to produce a range of other colors If the two color sources combine to pro  i Energy t E  o i Ew i Red Viol   Frequency ee roret  Dominant Energy distribution of a light source Frequency with a dominant frequency near the red end of the frequency range  Section Properties of Light  Chapter Color Models and Color Applications Color Matching RGB Amounts Ainm    Amounts of RGB primaries needed to display spectral colors  duce white light they are referred to as complementary colors Examples of complementary color pairs are red and cyan green and magenta and blue and yellow With a judicious choice of two or more starting colors we can form a wide range of other colors Typically color models that are used to describe com binations of light in terms of dominant frequency hue use three colors to obtain a reasonably wide range of colors called the color gamut for that model The two or three colors used to produce other colors in such a color model are referred to as primary colors  No finite set of real primary colors can be combined to produce all possible visible colors Nevertheless three primaries are sufficient for most purposes and colors not in the color gamut for a specified set of primaries can still be described by extended methods If a certain color cannot be produced by combining the three primaries we can mix one or two of the primaries with that color to obtain a match with the combination of remaining primaries In this extended sense a set of primary colors can be considered to describe all colors  shows the amounts of red green and blue needed to produce any spectral color The curves plotted in  called color matching functions were obtained by aver aging the judgments of a large number of observers Colors in the vicinity of nm can only be matched by subtracting an amount of red light from a combi nation of blue and green lights This means that a color around nm is de scribed only by combining that color with an amount of red light to produce the blue green combination specified in the diagram Thus an RGB color monitor cannot display colors in the neighborhood of S00 am  STANDARD PRIMARIES AND THE CHROMATIC TY DIAGRAM Since no finite set of color light sources can be combined to display all possible colors three standard primaries were defined in by the International Com mission on Illumination referred to as the CIE Commission Internationale de  Eclairage  The three standard primaries are imaginary colors They are defined mathematically with positive color matching functions   that specity the Color Matching CIE Amounts   an   Amounts of CIE primaries needed to display spectral colors  amount of each primary needed to describe any spectral color This provides an international standard definition for all colors and the CIE primaries eliminate negative value color matching and other problems associated with selecting a set of real primaries  XYZ Color Model The set of CIE primaries is generally referred to as the XYZ or X Y Z  color model where X Y and Z represent vectors in a three dimensional additive color space Any color C is then expressed as C  XX  YY  ZZ  where X Y and Z designate the amounts of the standard primaries needed to match C  In discussing color properties it is convenient to normalize the amounts in Eq against luminance X  Y  Z  Normalized amounts are thus calculated as x Y Z  yayen Yo xeyez xs ezZ with x  y  z  Thus any color can be represented with just the x and y amounts Since we have normalized against luminance parameters x and y are called the chromaticity values because they depend only on hue and purity Also if we specify colors only with x and y values we cannot obtain the amounts X Y and Z Therefore a complete description of a color is typically given with the three values x y and Y The remaining CIE amounts are then calculated as X mae z y  where z   x  y Using chromaticity coordinates x y  we can represent all colors on a two dimensional diagram  CIE Chromaticity Diagram When we plot the normalized amounts x and y for colors in the visible spectrum we obtain the tongue shaped curve shown in  This curve is called the CIE chromaticity diagram Points along the curve are the pure colors in the Section Standard Primaries and the Chromaticity Diagram  Chapter Color Models and Color Applications    Spectral   Colors  Yellow  Red   CIE chromaticity diagram Spectra 2 4 06 x color positions along the curve are  labeled in wavelength units nm  electromagnetic spectrum labeled according to wavelength in nanometers from the red end to the violet end of the spectrum The line joining the red and violet spectral points called the purple line is not part of the spectrum Interior points represent all possible visible color combinations Point C in the diagram corre sponds to the white light position Actually this point is plotted for a white light source known as illuminant C which is used as a standard approximation for average daylight  Luminance values are not available in the chromaticity diagram because of normalization Colors with different luminance but the same chromaticity map to the same point The chromaticity diagram is useful for the following   Comparing color gamuts for different sets of primaries  Identifying complementary colors  Determining dominant wavelength and purity of a given color  Color gamuts are represented on the chromaticity diagram as straight line segments or as polygons Al colors along the line joining points C and C in Fig can be obtained by mixing appropriate amounts of the colors C and C  If a greater proportion of C is used the resultant color is closer to C than to C  The color gamut for three points such as C3 C  and C in  is a triangle with vertices at the three color positions Three primaries can only generate colors in side or on the bounding edges of the triangle Thus the chromaticity diagram helps us understand why no set of three primaries can be additively combined to generate all colors since no triangle within the diagram can encompass all colors Color gamuts for video monitors and hard copy devices are conveniently com pared on the chromaticity diagram  Since the color gamut for two points is a straight line complementary col ors must be represented on the chromaticity diagram as two points situated on opposite sides of C and connected with a straight ine When we mix proper amounts of the two colors C and C in  we can obtain white light  We can also use the interpretation of color gamut for two primaries to de termine the dominant wavelength of a color For color point C in  we can draw a straight line trom C through C to intersect the spectral curve at point   x x    Color gamuts defined on the Representing complementary Determining dominant chromaticity diagram for a colors on the chromaticity wavelength and purity with two color and a three color diagram the chromaticity diagram  system of primaries  C  Color C can then be represented as a combination of white light C and the spectral color C  Thus the dominant wavelength of C is C  This method for de termining dominant wavelength will not work for color points that are between Cand the purple line Drawing a line from C through point C in  takes us to point C on the purple line which is not in the visible spectrum Point C is referred to as a nonspectral color and its dominant wavelength is taken as the compliment of C that lies on the spectral curve point C   Nonspectral colors are in the purple magenta range and have spectral distributions with subtractive dominant wavelengths They are generated by subtracting the spectral dominant wavelength such as C  from white light  For any color point such as C in  we determine the purity as the relative distance of C from C along the straight line joining C to C  If d  denotes the distance from C to C and d  is the distance from C to C  we can calculate pu rity as the ratio d  d  Color C in this figure is about percent pure since it is situated at about one fourth the total distance from C to C  At position C the color point would be percent pure   An artist creates a color painting by mixing color pigments with white and blacx pigments to form the various shades tints and tones in the scene Starting with the pigment for a pure color  or pure hue  the artist adds a black pigment to produce different shades of that color The more black pigment the darker the shade Similarly different tints of the color are obtained by adding a white pig ment to the original color making it lighter as more white is added Tones of the color are produced by adding both black and white pigments  To many these color concepts are more intuitive than describing a color as a set of three numbers that give the relative proportions of the primary colors It is generally much easier to think of making a coler lighter by adding white and making a color darker by adding black Therefore graphics packages providing Chapter Color Models and Color Applications color palettes to a user often employ two or more color models One model pro vides an intuitive color interface for the user and others describe the color com ponents for the output devices  Based on the tristimulus theory of vision our eyes perceive color through the stim ulation of three visual pigments in the cones of the retina These visual pigments have a peak sensitivity at wavelengths of about nm red  nm green  and nm blue  By comparing intensities in a light source we perceive the color of the light This theory of vision is the basis for displaying color output on a video monitor using the three color primaries red green and blue referred to as the RGB color model  We can represent this model with the unit cube defined on R G and B axes as shown in  The origin represents black and the vertex with coordi nates   is white Vertices of the cube on the axes represent the primary col ors and the remaining vertices represent the complementary color for each of the primary colors  As with the XYZ color system the RGB color scheme is an additive model Intensities of the primary colors are added to produce other colors Each color point within the bounds of the cube can be represented as the triple R G B  where values for R G and B are assigned in the range from to Thus a color C is expressed in RGB components as C  RR GG  BB  The magenta vertex is obtained by adding red and blue to produce the triple   and white at   is the sum of the red green and blue vertices Shades of gray are represented along the main diagonal of the cube from the origin black to the white vertex Each point along this diagonal has an equal contribu tion from each primary color so that a gray shade halfway between black and Grayscale Green   Yeltow ens    Cyan  a    white Black i a4   J  Red R oO ee    Blus Magenta The RGB color model defining    gs colors with an additive process within the unit cube  ial ib   Two views of the RGB color cube  a along the grayscale diagonal from white to black and b along the grayscale diagonal from black to white  TABLE RGB X Y CHROMACITY COORDINATES    NTSC Standard CIE Model Approx Color Monitor Values R 670 330  735 265  628 346 G 210 710  274 747  268 588 B 140 080  167 009  150 070 O 02 0 06 x  RGB color gamut   white is represented as   The color graduations along the front and top planes of the RGB cube are illustrated in   Chromaticity coordinates for an NTSC standard RGB phosphor are listed in Table Also listed are the RGB chromaticity coordinates for the CIE RGB color model and the approximate values used for phosphors in color monitors  shows the color gamut for the NTSC standard RGB rrimaries  Section RGB Color Model  Chapter Color Models and Color Applications  Whereas an RGB monitor requires separate signals for the red green and blue components of an image a television monitor uses a single composite signal The National Television System Committee NTSC color model for forming the com posite video signal is the YIQ model which is based on concepts in the CIE XYZ model  In the YIQ color model parameter Y is the same as in the XYZ model Lu minance brightness information is contained in the Y parameter while chro maticity information hue and purity is incorporated into the J and Q parame ters A combination of red green and blue intensities are chosen for the Y parameter to yield the standard luminosity curve Since Y contains the luminance information black and white television monitors use only the Y signal The largest bandwidth in the NTSC video signal about MHz is assigned to the Y information Parameter  contains orange cyan hue information that provides the flesh tone shading and occupies a bandwidth of approximately MHz Para meter Q carries green magenta hue information in a bandwidth of about MHz  An RGB signal can be converted to a television signal using an NTSC en coder which converts RGB values to YIQ values then modulates and superim poses the J and Q information on the Y signal The conversion from RGB values to YIQ values is accomplished with the transformation Y 299 587 144 R   275  G  Q 212 528 311 _B This transformation is based on the NTSC standard RGB phosphor whose chro tMaticity coordinates were given in the preceding section The larger proportions of red and green assigned to parameter Y indicate the relative importance of these hues in determining brightness compared to blue  An NTSC video signal can be converted to an RGB signal using an NTSC decoder which separates the video signal into the YIQ components then con verts to RGB values We convert from YIQ space to RGB space with the inverse matrix transformation from Eq  R 000 956 620  y G      B 000 108 705 Q  A color model defined with the primary colors cyan magenta and yellow CMY is useful for describing color output to hard copy devices Unlike video monitors which produce a color pattern by combining light from the screen phosphors hard copy devices such as plotters produce a color picture by coating a paper with color pigments We see the colors by reflected light a subtractive process  As we have noted cyan can be formed by adding green and blue light Therefore when white light is reflected from cyan colored ink the reflected light must have no red component That is red light is absorbed or subtracted by the ink Similarly magenta ink subtracts the green component from incident light and yellow subtracts the blue component A unit cube representation for the CMY model is illustrated in   In the CMY model point   represents black because all components of the incident light are subtracted The origin represents white light Equal amounts of each of the primary colors produce grays along the main diagonal of the cube A combination of cyan and magenta ink produces blue light because the red and green components of the incident light are absorbed Other color combinations are obtained by a similar subtractive process The printing process often used with the CMY mode generates a color point with a collection of four ink dots somewhat as an RGB monitor uses a col lection of three phosphor dots One dot is used for each of the primary colors cyan magenta and yellow  and one dot is black A black dot is included be cause the combination of cyan magenta and yellow inks typically produce dark gray instead of black Some plotters produce different color combinations by spraying the ink for the three primary colors over each other and allowing them to mix before they dry  We can express the conversion from an RGB representation to a CMY repre sentation with the matrix transformation Cc K MI  Ll   Y B where the white is represented in the RGB system as the unit column vector Sim ilarly we convert from a CMY color represeniation to an RGB representation with the matrix transformation c 1M ia y where black is represented in the CMY system as the unit column vector  Instead of a set of color primaries the HSV model uses color descriptions that have a more intuitive appeal to a user To give a color specification a user selects a spectral color and the amounts of white and black that are to be added to ob tain different shades tints and tones Color parameters in this model are Aue H  saturation S  and value V  Section HSV Color Model M G t rayseate Magenta Blue Red  Nie Black Cyan White c    Yellow Green   The CMY color model defining colors with a subtractive process inside a unit cube  Chapter Green Color Models and Color  Applications Cyan Yellow Blue Red R Magenta RGB Color Cube Color Hexagon a  b   When the RGB color cube a is viewed along the diagonal from white to black the color cube outline is a hexagon b  The three dimensional representation of the HSV model is derived from the RGB cube If we imagine viewing the cube along the diagonal from the white vertex to the origin black  we see an outline of the cube that has the hexagon shape shown in  The boundary of the hexagon represents the various hues and it is used as the top of the HSV jhexcone   In the hexcone saturation is measured along a horizontal axis and value is along a vertical axis through the center of the hexcone  Hue is represented as an angle about the vertical axis ranging from at red through  Vertices of the hexagon are separated by intervals Yellow is at  green at  and cyan opposite red at H   Complementary colors are apart  V Value    Yellow  Wal Cyan White  V  H Hue Angle  Vad  Biack SiSaturation  The HSV hexcone   v Tints Pure Hue White   V  Shades  Black Cross section of the HSV hexcone  showing regions for shades tints  and tones  Saturation S varies from to It is represented in this madel as the ratio of the purity of a selected hue to its maximum purity at S  A selected hue is said to be one quarter pure at the value S  25 At S  we have the gray scale  Value V varies from at the apex of the hexcone to at the top The apex represents black At the top of the hexcone colors have their maximum intensity When V  and S  we have the pure hues White is the point at V  and S  This is a more intuitive model for most users Starting with a selection fora pure hue which specifies the hue angle H and sets V   we describe the color we want in terms of adding either white or black to the pure hue Adding black decreases the setting for V while S is held constant To get a dark blue V could be set to with   and H   Similarly when white is to be added to the hue selected parameter S is decreased while keeping V constant A light blue could be designated with S  while V  and H   By adding some black and some white we decrease both V and S An interface for this model typically presents the HSV parameter choices in a color palette  Color concepts associated with the terms shades tints and tones are repre sented in a cross sectional plane of the HSV hexcone   Adding black to a pure hue decreases V down the side of the hexcone Thus various shades are represented with values   and V Adding white to a pure tone pro duces different tints across the top plane of the hexcone where parameter values are V and S Various tones are specified by adding both black and white producing color points within the triangular cross sectional area of the hexcone  The human eye can distinguish about different hues and about dif ferent tints saturation levels  For each of these a number of shades value set tings can be detected depending on the hue selected About shades are dis cernible with yellow colors and about different shades can be seen at the blue end of the spectrum This means that we can distinguish about x x  720 different colors For most graphics applications hues saturation lev els and value settings are sufficient With this range of parameters in the HSV color model 384 colors would be available to a user and the system would need bits of color storage per pixel Color lookup tables could be used to re  duce the storage requirements per pixel and to increase the number of available colors  Section HSV Color Mode  Chapter Color Models and Color Applications  If HSV color parameters are made available to a user of a graphics package these parameters are transformed to the RGB settings needed for the color monitor To determine the operations needed in this transformation we first consider how the HSV hexcone can be derived from the RGB cube The diagonal of this cube trom black the origin to white corresponds to the V axis of the hexcone Also each subcube of the RGB cube corresponds to a hexagonal cross sectional area of the hexcone At any cross section all sides of the hexagon and all radial lines from the V axis to any vertex have the value V For any set of RGB values V is equal to the maximum value in this set The HSV point corresponding to the set of RGB values lies on the hexagonal cross section at value V Parameter S is then determined as the relative distance of this point from the V axis Parameter H is determined by calculating the relative position of the point within each sextant of the hexagon An algorithm for mapping any set of RGB values into the corre sponding HSV values is given in the following procedure   include math h   Input h s v in range   Outputs r g bk ain range    void hsviloRgb float h float s float v float  r   int i  float aa bb cc ft float  yg float  b  if s    Grayscale  tr se tg   bey else  if th   h  h    ios floor nh  i f h a  aa v   5s  bb sv    s   co v   Ss      switch i  case  r v  g  cc  b  aa break case tr  bb  g  vi  b  ad break case  r  aa  g  v th  cc break case  r  aa  g  bb  h  v break case  r  cc  g  aa  b  v break case  r v  g  aa  b  bb break     We obtain the transformation from HSV parameters to RGB parameters by determining the inverse of the equations in rgbToHsv procedure These in verse operations are carried out for each sextant of the hexcone The resulting transformation equations are summarized in the following algorithm  Kinclude math h   define MIN a b  a a a b  define MAX a b  a  a b   define NO_HUE i   Input r g b in range  Outputs h s v in range  ioe void rgbToHsyv float r float g float b floac  h float  s float  v  float max  MAX r MAX g b  min  MIN r MIN g b  float delta  max  min  yvo  max if max   s  delta  max else s  if  s    h  NO_HUE else  if r  max   h  g  b  delta i else if g  max I th   b  x  delta else if b  max  h   x  g  delta   h t   if  h  th    h     i  a   HLS COLOR MODEL Another model based on intuitive color parameters is the HLS system used by Tektronix This model has the double cone representation shown in  The three color parameters in this model are called hue H  lightness L  and saturation S  Hue has the same meaning as in the HSV model It specifies an angle about the vertical axis that locates a chosen hue In this model H  corresponds to blue The remaining colors are specified around the perimeter of the cone in the same order as in the HSV model Magenta is at  red is at  and cyan is lo cated at H   Again complementary colors are apart on the double cone  The vertical axis in this model is called lightness L At L  we have black and white is at L  Gray scale is along the L axis and the pure hues lie on the L  plane  Saturation parameter S again specifies relative purity of a color This para meter varies from to and pure hues are those for which S  and L  As S decreases the hues are said to be less pure At  we have the gray scale  As in the HSV model the HLS system allows a user to think in terms of making a selected hue darker or lighter A hue is selected with hue angle H and the desired shade tint or tone is obtained by adjusting L and S Colors are made lighter by increasing L and made darker by decreasing L When S is decreased the colors move toward gray  Section HLS Color Model  Chapter Color Models and Color Applications  L Lightness d ded Yeliow Blue  H Hue Angle L  Black S Saturation      The HLS double cone   A graphics package can provide color capabilities in a way that aids us in making color selections Various combinations of colors can be selected using sliders and color wheels and the system can also be designed to aid in the selection of har monizing colors In addition the designer of a package can follow some basic color rules when designing the color displays that are to be presented to a user One method for obtaining a set of coordinating colors is to generate the set from some subspace of a color model If colors are selected at regular intervals along any straight line within the RGB or CMY cube for example we can expect to obtain a set of well matched colors Randomly selected hues can be expected to produce harsh and clashing color combinations Another consideration in the selection of color combinations is that different colors are perceived at different depths This occurs because our eyes focus on colors according to their frequency Blues in particular tend to recede Displaying a blue pattern next to a red pattern can cause eye fatigue because we continually need to refocus when our attention is switched from one area to the other This problem can be reduced by separat ing these colors or by using colors from one half or tess of the color hexagon in the HSV model With this technique a display contains either blues and greens or reds and yellows  As a gencral rule the use of a smaller number of colors produces a more pleasing display than a large number of colors and tints and shades blend better than pure hues For a background gray or the complement of one of the fore ground colors is usually best  SUMMARY In this chapter we have discussed the basic properties of light and the concept of a color model Visible light can be characterized as a narrow frequency distribu tion within the electromagnetic spectrum Light sources are described in terms of their dominant frequency or hue  luminance or brightness  and purity or sai uration  Complementary color sources are those that combine to produce white light  One method for defining a color model is to specify a set of two or more primary colors that are combined to produce various other colors Common color models defined with three primary colors are the RGB and CMY madels Video monitor displays use the RGB model while hardcopy devices produce color out put using the CMY model Other color models based on specification of lumi nance and purity values include the Y Q HSV and HLS color models Intuitive color models such as the HSV and HLS models allow colors to be specified by selecting a value for hue and the amounts of white and black to be added to the selected hue  Since no model specified with a finite set of color parameters is capable of describing all possible colors a set of three hypothetical colors called the CIE primaries has been adopted as the standard for defining all color combinations The set of CIE primaries is commonly referred to as the XYZ color model Plot ting normalized values for the X and Y standards produces the CIE chromaticity diagram which gives a representation for any color in terms of hue and purity We can use this diagram to compare color gamuts for different color models to identify complementary colors and to determine dominant frequency and purity for a given color  An important consideration in the generation of a color display is the selec tion of harmonious color combinations We can do this by following a few simple rules Coordinating colors usually can be selected from within a small subspace of a color model Also we should avoid displaying adjacent colors that differ widely ir dominant frequency And we should limit displays to a small number of color combinations formed with tints and shades rather than with pure hues  REFERENCES A comprehensive discussion of the science of color is given in Wyszecki and Stiles  Color models and color display techniques are discussed in Durrett  Hall  and Travis  Algorithms for various color applications are presented n Glassner  Arvo  and Kirk  For additional infoymation on the human visual sys tem and our perception of light and color see Glassnet  References  Chapter Color Models and Color Applications  EXERCISES       10  11  12  Derive expressions for converting RGB color parameters to HSV values  Derive expressions for converting HSV color values to RGB values  Write an interactive procedure that allows selection of HSV color parameters irom a displayed menu then the HSV values are to be converted to RGB values for storage in a frame buffer  Derive expressions for converting RGB color values to HLS color parameters  Derive expressions for converting HLS color values to RGB values  Write a program that allows interactive selection of HLS values from a color menu then converts these values ta corresponding RGB values Write a program that will produce a set of colors that are linearly interpolated be tween any two specified positions in RGB space  Write an interactive routine for selecting color values from within a specified sub space of RGB space Write a program that will produce a set of cofors that are linearly interpolated be tween any two specified positions in HSV space  Write a program that will produce a set of colors that are linearly interpolated be tween any two specified positions in HLS space  Display two RGB color grids side by side on a video monitor Fill one grid with a set of randomly selected RGB colors and fill the other grid with a set of colors that are selected from a small RGB subspace Experiment with different random selections and different RGB subspaces and compare the two color grids  Display the two color grids in Exercise 11 using color selections from either the HSV or the HLS calor space  CHAPTER Computer Animation      S ome typical applications of computer generated animation are entertain ment motion pictures and cartoons  advertising scientific and engineering studies and training and education Although we tend to think of animation as implying object motions the term computer animation generally refers to any time sequence of visual changes in a scene In addition to changing object posi tion with translations or rotations a computer generated animation could dis play time variations in object size color transparency or surface texture Adver tising animations often transition one object shape into another for example transforming a can of motor oi into an automobile engine Computer animations can also be generated by changing camera parameters such as position orienta tion and focal length And we can produce computer animations by changing lighting effects or other parameters and procedures associated with illumination and rendering  Many applications of computer animation require realistic displays An ac curate representation of the shape of a thunderstorm or other natural phenomena described with a numerical model is important for evaluating the reliability of the model Also simulators for training aircraft pilots and heavy equipment oper ators must produce reasonably accurate representations of the environment En tertainment and advertising applications on the other hand are sometimes more interested in visual effects Thus scenes may be displayed with exaggerated shapes and unrealistic motions and transformations There are many entertain ment and advertising applications that do require accurate representations for computer generated scenes And in some scientific and engineering studies real ism is not a goal For example physical quantities are often displayed with pseudo colors or abstract shapes that change over tume to help the researcher un derstand the nature of the physical process  In general an animation sequence is designed with the following steps   Storyboard layout  Object definitions  Key frame specifications  Generation of in between frames This standard approach for animated cartoons is applied to other animation ap plications as well although there are many special applications that do not fol low this sequence Real time computer animations produced by flight simulators for instance display motion sequences in response to settings on the aircraft con trols And visualization applications are generated by the solutions of the numer ical models For frame by frame animation each frame of the scene is separately generated and stored Later the frames can be recorded on film or they can be consecutively displayed in real time playback mode  The storyboard is an outline of the action It defines the motion sequence as a set of basic events that are to take place Depending on the type of animation to be produced the storyboard could consist of a set of rough sketches or it could be a list of the basic ideas for the motion  An object definition is given for each participant in the action Objects can be defined in terms of basic shapes such as polygons or splines In addition the as sociated movements for each object are specified along with the shape  A key frame is a detailed drawing of the scene at a certain time in the anima tion sequence Within each key frame each object is positioned according to the time for that frame Some key frames are chosen at extreme positions in the ac tion others are spaced so that the time interval between key frames is not too great More key frames are specified for intricate motions than for simple slowly varing motions  In betweens are the intermediate frames between the key frames The num ber of in betweens needed is determined by the media to be used to display the animation Film requires frames per second and graphics terminals are re freshed at the rate of to frames per second Typically time intervals for the motion are set up so that there are from three to five in betweens for each pair of key frames Depending on the speed specified for the motion some key frames can be duplicated For a minute film sequence with no duplication we would need frames With five in betweens for each pair of key frames we would need key frames If the motion is not too complicated we could space the key frames a little farther apart  There are several other tasks that may be required depending on the appli cation They include motion verification editing and production and synchro nization of a soundtrack Many of the functions needed to produce general ani mations are now computer generated Figures and show examples of computer generated frames for animation sequences   One frame from the award winning computer animated short film Luxe Jr The film was designed using a key frame animation system and cartoon animation techniques to provide lifelike actions of the lamps Final images were rendered with multiple light sources and procedural texturing techniques  Courtesy of Pixar  Pixar   Section Design of Animation Sequences   One frame from the short film Tin Toy the first computer animated film to win an Oscar Designed using a key frame animation system the film also required extensive facial expression modeling Final images were rendered using procedural shading self shadowing techniques motion blur and texture mapping  Courtesy of Pixar  Pixar    GENERAL COMPUTER ANIMATION FUNCTIONS Some steps in the development of an animation sequence are well suited to com puter solution These include object manipulations and rendering camera mo tions and the generation of in betweens Animation packages such as Wave front for example provide special functions for designing the animation and processing individual objects  One function available in animation packages is provided to store and man age the object database Object shapes and associated parameters are stored and updated in the database Other object functions include those for motion genera tion and those for object rendering Motions can be generated according to speci fied constraints using two dimensional or three dimensional transformations Standard functions can then be applied to identify visible surfaces and apply the rendering algorithms  Another typical function simulates camera movements Standard motions are zooming panning and tilting Finally given the specification for the key frames the in betweens can be automatically generated  On raster systems we can generate real time animation in limited applications using raster operations As we have seen in Section a simple method for trans lation in the xy plane is to transfer a rectangluar block of pixel values from one location to another Two dimensional rotations in multiples of are also simple to perform although we can rotate rectangular blocks of pixels through arbitrary angles using antialiasing procedures To rotate a block of pixels we need to de termine the percent of area coverage for those pixels that overlap the rotated block Sequences of raster operations can be executed to produce real time ani mation of either two dimensional or three dimensional objects as long as we re Strict the animation to motions in the projection plane Then no viewing or visi ble surface algorithms need be invoked  We can also animate objects along two dimensional motion paths using the color table transformations Here we predefine the object at successive positions along the motion path and set the successive blocks of pixel values to color table  Real time raster color table animation  entries We set the pixels at the first position of the object to on values and we set the pixels at the other object positions to the background color The animation is then accomplished by changing the color table values so that the object is on at successively positions along the animation path as the preceding position is set to the background intensity     Design and control of animation sequences are handled with a set of animation routines A general purpose language such as C Lisp Pascal or FORTRAN is often used to program the animation functions but several specialized animation languages have been developed Animation functions include a graphics editor a key frame generator an in between generator and standard graphics routines The graphics editor allows us to design and modify object shapes using spline surfaces constructive solid geometry methods or other representation schemes  A typical task in an animation specification is scene description This includes the positioning of objects and light sources defining the photometric parameters light source intensities and surface illumination properties  and setting the camera parameters position orientation and lens characteristics  Another stan dard function is action specification This involves the layout of motion paths for the objects and camera And we need the usual graphics routines viewing and perspective transformations geometric transformations to generate cbject move ments as a function of accelerations or kinematic path specifications visible sur face identification and the surface rendering operations  Key frame systems are specialized animation languages designed simply to generate the in betweens from the user specified key frames Usually each ob ject in the scene is defined as a set of rigid bodies connected at the joints and with a limited number of degrees of freedom As an example the single arm robot in  has six degrees of freedom which are called arm sweep shoulder swivel elbow extension pitch yaw and roll We can extend the number of de grees of freedom for this robot arm to nine by allowing three dimensional trans lations for the base   If we also allow base rotations the robot arm can have a total of degrees of freedom The human body in comparison has over degrees of freedom  Parameterized systems allow object motion characteristics to be specified as part of the object definitions The adjustable parameters control such object characteristics as degrees of freedom motion limitations and allowable shape changes  Section Computer Animation Languages Chapter Computer Animation LA   Translational and rotational degrees of freedom for the base of the robot arm  Elbow Kon renin Shoutder A  Swivel   o Yaw rn  Roll Pitch   Degrees of freedom for a stationary single arm robot Scripting systems allow object specifications and animation sequences to be defined with a user input serip  From the script a library of various objects and motions can be constructed   We generate each set of in betweens from the specification of two or more key frames Motion paths can be given with a kinematic description as a set of spline curves or the motions can be physically tased by specifying the forces acting on the objects to be animated  For complex scenes we can separate the frames into individual components or objects called cefs celluloid transparencies  an acronym from cartoon anima tion Given the animation paths we can interpolate the positions of individual objects between any two times  With complex object transformations the shapes of objects may change over time Examples are clothes facial features magnified detail evolving shapes exploding or disintegrating objects and transforming one object into an other object If all surfaces are described with polygon meshes then the number of edges per polygon can change from one frame to the next Thus the total num ber of line segments can be different in different frames  Morphing Transformation of object shapes from one form to another is called morphing which is a shortened torm of metamorphosis Morphing methods can be applied to any motion or transition involving a change in shape  Given two key frames for an object transformation we first adjust the object specification in one of the frames so that the number of polygon edges or the number of vertices is the same for the two frames This preprocessing step is il lustrated in  A straight line segment in key frame k is transformed into two line segments in key frame k  Since key frame k   has an extra vertex we add a vertex between vertices and in key frame k to balance the number of vertices and edges in the two key frames Using linear interpolation to generate the in betweens we transition the added vertex in key frame k into vertex along the straight line path shown in  An example of a triangle linearly expanding into a quadrilateral is given in  Figures and 10 show examples of morphing in television advertising  v   Key Key Frame k Frame k   An edge with vertex positions and in key frame k evolves into two connected edges in key frame k     Linear interpolation for transforming a line segment in key frame  into two connected line segments in key frame k   Key Frame k   Linear interpolation for transforming a triangle into a quadrilateral  We can state general preprocessing rules for equalizing key frames in terms of either the number of edges or the number of vertices to be added to a key frame Suppose we equalize the edge count and parameters L and L  denote the number of line segments in two consecutive frames We then define Lina  Max Ly Lea  Linn  Min Ly Ly   and N  Las mod Ll  N  ine   mun   Transformation of an STP oil can into an engine block  Courtesy of Salicon Graphres inc   b  eed tot   td   Transtormation of a moving automobile into a running tiger  Courtesy of Exxon Company USA arid Pactfic Data Images  Then the preprocessing is accomplished by  dividing N edges of keyframe   into N  sections dividing the remaining lines of keyframe   into N sections As an example if L  and L    we would divide lines of keyframe  into sections each The remaining lines of keyframe   are left intact  If we equalize the vertex count we can use parameters V and V  to de note the number of vertices in the two consecutive frames In this case we define Vinax  MAX VyE Ver  Vinin  min V  V   and Nis  Vinax  mod Vann  Venax  N  int T   min  Preprocessing using vertex count is performed by  adding N points to N  line sections of keyframerun adding N  points to the remaining edges of keyframegin For the triangle to quadrilateral example V  and V    Both N  and N  are so we would add one point to one edge of keyframe  No points would be added to the remaining lines of keyframe    Simulating Accelerations Curve fitting techniques are often used to specify the animation paths between key frames Given the vertex positions at the key frames we can fit the positions with linear or nonlinear paths  illustrates a nonlinear fit of key frame positions This determines the trajectories for the in betweens To simulate accel erations we can adjust the time spacing for the in betweens  For constant speed zero acceleration  we use equal interval time spacing for the in betweens Suppose we want n in betweens for key frames at times and     The time interval between key frames is then divided into n  subintervals yielding an in between spacing of ho   Al nel  We can calculate the time for any in between as fB f  jdt jf  4H  and determine the values for coordinate positions color and other physical para meters  Nonzero accelerations are used to produce realistic displays of speed changes particularly at the beginning and end of a motion sequence We can model the start up and slow down portions of an animation path with spline or Section Key Frame Systems    Fitting key frame vertex positions with nonlinear splines  trignometric functions Parabolic and cubic time functions have been applied to acceleration modeling but trignometric functions are more commonly used in animation packages  To model increasing speed positive acceleration  we want the time spacing between frames to increase so that greater changes in position occur as the object moves faster We can obtain an increasing interval size with the function  cos6 O0 w For in betweens the time for the jth in between would then be calculated as   cos ft j  tBy  ty  asf Ss i    where Af is the time difference between the two key frames  gives a plot of the trigonometric acceleration function and the in between spacing for  We can model decreasing speed deceleration with sin in the range    The time position of an in between is now defined as   jw tBe t  in j    ty  Atsin  j n  I J J t J T a  y t  t at ty  In between positions for motion at constant speed  LE   A trigonometric acceleration function and the corresponding in between spacing for n  and  ja in Eq  producing increased coordinate changes as the object moves through each time interval  A plot of this function and the decreasing size of the time intervals is shown in  for five in betweens  Often motions contain both speed ups and slow downs We can model a combination of increasing decreasing speed by first increasing the in between time spacing then we decrease this spacing A function to accomplish these time changes is   sind  t8 tB a         OF 2345 i y t v z   A trigonometric deceleration function and the corresponding in between spacing for n  and  jr in Eq  producing decreased coordinate changes as the object moves through each time interval   t  Bs  A trigonometric accelerate decelerate function and the corresponding in between spacing for n  in Eq   cos   The time for the jth in between is now calculated as 1Bp  arf t coetintn  NY j    with Af denoting the time difference for the two key frames Time intervals for the moving object first increase then the time intervals decrease as shown in Fig 15  Processing the in betweens is simplified by initially modeling skeleton  wireframe objects This allows interactive adjustment of motion sequences After the animation sequence is completely defined objects can be fully ren dered  There are several ways in which the motions of objects can be specified in an ani mation system We can define motions in very explicit terms or we can use more abstract or more general approaches  Direct Motion Specification The most straightforward method for defining a motion sequence is direct specifi cation of the motion parameters Here we explicitly give the rotation angles and translation vectors Then the geometric transformation matrices are applied to transform coordinate positions Alternatively we could use an approximating  y          a  mf  Po      i      s a og ro og t   i  t     rOe oy re     ay ra    be oi  ow ay it  if    s vi e  or rene  u i ve rel eros t U  Ys  g y vi ve   4  a yy x   Approximating the motion of a bouncing ball with a damped sine function Eq 10  equation to specify certain kinds of motions We can approximate the path of a bouncing ball for instance with a damped rectified sine curve    yaX A Isin ox   le  10 where A is the initial amplitude w is the angular frequence  is the phase angle  and k is the damping constant These methods can be used for simple user pro grammed animation sequences  CGoal Directed Systems At the opposite extreme we can specify the motions that are to take place in gen eral terms that abstractly describe the actions These systems are referred to as goal directed because they determine specific motion parameters given the goals of the animation For example we could specify that we want an object to walk or to run to a particular destination Or we could state that we want an object to pick up some other specified object The input directives are then inter preted in terms of component motions that will accomplish the selected task  Human motions for instance can be defined as a hierarchical structure of sub motions for the torso limbs and so forth  Kinematics and Dynamics We can also construct animation sequences using kinematic or dynamic descrip tions With a kinematic description we specify the animation by giving motion parameters position velocity and acceleration without reference to the forces that cause the motion For constant velocity zero acceleration  we designate the motions of rigid bodies in a scene by giv ng an initial position and velocity vector  Chapter Computer Animation for each object As an example if a velocity is specified as   km sec then this vector gives the direction for the straight line motion path and the speed magnitude of velocity is km sec If we also specify accelerations rate of change of velocity  we can generate speed ups slow downs and curved motion paths Kinematic specification of a motion can also be given by simply describing the motion path This is often done using spline curves  An alternate approach is to use inverse kinematics Here we specify the ini tial and final positions of objects at specified times and the motion parameters are computed by the system For example assuming zero accelerations we can de termine the constant velocity that will accomplish the movement of an object fom the initial position to the final position This method is often used with com plex objects by giving the positions and orientations of an end node of an object such as a hand or a foot The system then determines the motion parameters of other nodes to accomplish the desired motion  Dynamic descriptions on the other hand require the specification of the forces that produce the velocities and accelerations Descriptions of object behav ior under the irfluence of forces are generally referred to as a physically based modeling Chapter  Examples of forces affecting object motion include electro magnetic gravitational friction and other mechanical forces  Object motions are obtained from the force equations describing physical laws such as Newton s laws of motion for gravitational and friction processes Euler or Navier Stokes equations describing fluid flow and Maxwell s equations for electromagnetic forces For example the general form of Newton s second law for a particle of mass m1 is d  F au 11 with F as the force vector and v as the velocity vector If mass is constant we solve the equation F  ma where a is the acceleration vector Otherwise mass is a function of time as in relativistic motions or the motions of space vehicles that consume measurable amounts of fuel per unit time We can also use inverse dy namics to obtain the forces given the initial and final positions of objects and the type of motion  Applications of physically based modeling include complex rigid body sys tems and such nonrigid systems as cloth and plastic materials Typically numeri cal methods are used to obtain the motion parameters incrementally from the dy namical equations using initial conditions or boundary values  SUMMARY A computer animation sequence can be set up by specifying the storyboard the object definitions and the key frames The storyboard is an outline of the action and the key frames define the details of the object motions for selected positions in the animation Once the key frames have been established a sequence of in be tweens can be generated to construct a smooth motion from one key frame to the next A computer animation can involve motion specifications for the objects in a scene as well as motion paths for a camera that moves through the scene Com puter animation systems include key frame systems parameterized systems and Scripting systems For motion in two dimensions we can use the raster anima tion techniques discussed in Chapter For some applications key frames are used to detine the steps in a morph ing sequence that changes one object shape into another Other in between meth ods include generation of variable time intervals to simulate accelerations and decelerations in the motion  Motion specifications can be given in terms of translation and rotation para meters or motions can be described with equations or with kinematic or dy namic parameters Kinematic motion descriptions specify positions velocities and accelerations Dynamic motion descriptions are given in terms of the forces acting on the objects in a scene  REFERENCES For additional information on computer animation systems and techniques see Magnenat Thalmann and Thalmann  Barzel  and Watt and Wait  Algorithms for animation applications are presented in Glassner  Arvo  Kirk  Gas cuel  Ngo and Marks  van de Panne and Fiume  and in Snyder et al  Morphing techniques are discussed in Beier and Neely  Hughes  Kent Carlson and Parent  and in Sederberg and Greenwood  A discussion of animation techniques in PHIGS is given in Gaskins   EXERCISES  Design a storyboard layout and accompanving key trames for an animation of a sin gle polyhedron  Write a program to generate the in betweens for the key frames specified in Exercise using linear interpolation  Expand the animation sequence in Exercise to include two or more moving ob jects  Write a program to generate the in betweens jor the key frames in Exercise using linear in erpolation  Write a morphing program to transform a sphere into a specilied polyhedron  Sel up an animation specification invalving accelerations and implement Eq   Sel up an animation specification involving both accelerations and decelerations and implement the in between spacing calculations given in Eqs and Set up an animation specification implementing the acceleration deceleration calcu latians of Eq   Write a program to simulate the linear two dimensional motion of a filled circle inside a given rectangular area The circle is to be given an initial velocity and the circle is to rebound from the walls with the angle of reflection equal to the angle of incidence  10 Convert the program of Exercise into a ball and paddle game by replacing one side of the rectangle with a short line segment that can be moved back and forth to intercept the circle path The game is over when the circle escapes from the interior of the rectangle Initial input parameters include circle position direction and speed The game score can include the number of times the circle is intercepted by the pad dle  11 Expand the program of Exercise to simulate the three dimensional motion of a sphere moving inside a parallelepiped Interactive viewing parameters can be set to view the motion from different directions  12 Write a program to implement the simulation of a bouncing ball using Eq 10  13 Write a program to implement the motion of a bouncing bail using a downward  Exercises Chapter Computer Animation 14  15  Zravitational force and a ground plane friction force Initially the ball is to be pro jected into space with a given velocity vector  Write a program to implement the two player pillbox game The game can be imple mented on a flat plane with fixed pillbox positions or random terrain features and pillbox placements can be generated at the start of the game  Write a program to implement dynamic motion specifications Specify a scene with two or more objects initial motion parameters and specified forces Then generate the animation from the solution of the force equations  For example the objects could be the earth moon and sun with attractive gravitational forces that are propor tional mass and inversely proportional to distance squared  APPENDIX Mathematics for Computer Graphics        omputer graphics algorithms make use of many mathematica concepts and techniques Here we provide a brief reference for the topics from ana lytic geometry linear algebra vector analysis tensor analysis complex numbers numerical analysis and other areas that are referred to in the graphics algorithms discussed throughout this book  A COORDINATE REFERENCE FRAMES Graphics packages typically require that coordinate parameters be specified with respect to Cartesian reference frames But in many applications non Cartesian coordinate systems are useful Spherical cylindrical or other symmetries often can be exploited to simplify expressions involving object descriptions or manipu lations Unless a specialized graphics system is available however we must first convert any non Cartesian descriptions to Cartesian coordinates In this section we first review standard Cartesian coordinate systems then we consider a few common non Cartesian systems  Two Dimensional Cartesian Reference Frames Figure A shows two possible orientations for a Cartesian screen reference sys tem The standard coordinate orientation shown in Fig A a  with the coordi nate origin in the lower left corner of the screen is a commonly used reference Figure A Screen Cartesian reference systems  a coordinate origin at the lower left screen corner and b coordinate origin in the upper left corner Figure A A polar coardinate reference frame formed with concentric circles and radial lines  Figure A Relationship between polar and Cartesian coordinates  frame Some systems particularly personal computers orient the Cartesian refer ence frame as in Fig A b  with the origin at the upper left corner In addition it is possible in some graphics packages to select a position such as the center of the screen for the coordinate origin  Polar Coordinates in the xy Plane A frequently used non Cartesian system is a polar coordinate reference frame Fig A  where a coordinate position is specified with a radial distance r from the coordinate origin and an angular displacement from the horizontal Posi tive angular displacements are counterclockwise and negative angular displace ments are clockwise Angle can be measured in degrees with one complete counterclockwise revolution about the origin as  The relation between Carte sian and polar coordinates is shown in Fig A Considering the right triangle in Fig A and using the definition of the trigonometric functions we transform from polar coordinates to Cartesian coordinates with the expressions x rcos  y rsind A  The inverse transfarmation from Cartesian to polar coordinates is r Very tan   A x Other conics besides circles can be used to specify coordinate positions For example using concentric ellipses instead of circles we can give coordinate positions in elliptical coordinates Similarly other types of symmetries can be ex ploited with hyperbolic or parabolic plane coordinates  Section A Coordinate Reference Frames o Figure A Right triangle with hypotenuse r and sides x and y  Appendix A  Figure A An angle  subtended by a circular arc of length s and radius r  Angular values can be specified in degrees or they can be given in dimen sionless units radians  Figure A shows two intersecting lines in a plane and a circle centered on the intersection point P The value cf angle in radians is then given by Ia  A  where s is the length of the circular arc subtending and r is the radius of the cir cle Total angular distance around point P is the length of the circle perimeter 2a divided by r or radians  Three Dimensianal Cartesian Reference Frames Figure A a shows the conventional orientation for the coordinate axes in a three dimensional Cartesian reference system This is called a right handed sys tem because the right hand thumb points in the positive z direction when we imagine grasping the z axis with the fingers curling from the positive x axis to the positive y axis through  as illustrated in Fig A b  Most computer graph ics packages require object descriptions and manipulations to be specified in right handed Cartesian coordinates For discussions throughout this book in cluding the appendix  we assume that all Cartesian reference frames are right handed  Another possible arrangement of Cartesian axes is the left handed system shown in Fig A For this system the left hand thumb points in the positive z direction when we imagine grasping the z axis so that the fingers of the left hand curl from the positive x axis to the positive y axis through  This orientation of axes is sometimes convenient for describing depth of objects relative to a display screen If screen locations are described in the xy plane of a left handed system with the coordinate origin in the lower left screen comer positive z values indi cate positions behind the screen as in Fig A a  Larger values along the posi tive z axis are then interpreted as being farther from the viewer  Three Dimensional Curvilinear Coordinate Systems Any non Cartesian reference frame is referred to as a curvilinear coordinate sys tem The choice of coordinate system for a particular graphics application de pends on a number of factors such as symmetry ease of computation and visu  y axis y axis a ate a oN ed i     see Zaxis  x axis axis x axis  Figure A b Coordinate representation of a point P at position x y z in a right handed Cartesian reference system  a  B  Figure A Left handed Cartesian coordinate system superimposed on the surface af a video monitor  Figure A A general curvilinear coordinate reference frame  alization advantages Figure A shows a general curvilinear coordinate reference frame formed with three coordinate surfaces where each surface has one coordi nate held constant For instance the x x surface is defined with x held constant Coordinate axes in any reference frame are the intersection curves of the coordi nate surfaces If the coordinate surfaces intersect at right angles we have an or thogonal curvilinear coordinate system Nonorthogonal reference frames are useful for specialized spaces such as visualizations of motions governed by the laws of general relativity but in general they are used less frequently in graphics applications than orthogonal systems  A cylindrical coordinate specification of a spatial position is shown in Fig A in relation to a Cartesian reference frame The surface of constant pis a vertical zZ axis  Ply   z  i    y axis Figure A  x axis Cylindrical coordinates p  z  Section A Coordinate Reference Frames  Appendix A Figure A Spherical coordinates r    cylinder the surface of constant is a vertical plane containing the z axis and the surface of constant z is a horizontal plane parallel to the Cartesian xy plane We transform from a cylindrical coordinate specification to a Cartesian reference frame with the calculations x  pcos  y psin  z z A  Figure A shows a spherical coordinate specification of a spatial position in reference to a Cartesian reference frame Spherical coordinates are sometimes re ferred to as polar coordinates in space The surface of constant r is a sphere the sur face of constant  is a vertical plane containing the z axis same  surface as in cylindrical coordinates  and the surface of constant  is a cone with apex at the coordinate origin If    the cone is above the xy plane If    the cone is below the xy plane We transfrom from a spherical coordinate specification to a Cartesian reference frame with the calculations x  rcos sing y rsin sing z rcosd A  Solid Angle We define a solid angle in analogy with that for a two dimensional angle  be tween two intersecting lines Eq A  Instead of a circle we consider any sphere with center position P The solid angle w within a cone shaped region with apex at P is defined as w  A  where A is the area of the spherical surface intersected by the cone Fig A  and r is the radius of the sphere  Also in analogy with two dimensional polar coordinates the dimension less unit for solid angles is called the steradian The total solid angle about a point is the total area of the spherical surface  divided by r or steradians Figure A A solid angle w subtended by a spherical surface patch of area A with radius r  A POINTS AND VECTORS There is a fundamental difference between the concept of a point and that of a vector A point is a position specified with coordinate values in some reference frame so that the distance from the origin depends on the choice of refer ence frame Figure A illustrates coordinate specification in two reference frames In frame A point coordinates are given by the values of the ordered pair x y  In frame B the same point has coordinates  and the distance to the ori gin of frame B is  A vector on the other hand is defined as the difference between two point positions Thus for a two dimensional vector Fig A  we have V P P   x      A   V  V  where the Cartesian components or Cartesian elements V and V are the projec tions of V onto the x and y axes Given two point positions we can obtain vector components in the same way for any coordinate reference frame  We can describe a vector as a directed line segment that has two fundamental properties magnitude and direction For the two dimensional vector in Fig A  we calculate vector magnitude using the Pythagorean theorem  o frame B Oo x frame A  Figure A Position of point P with respect to two different Cartesian reference frames  Section A Points and Vectors Appendix A  Figure A Direction angles a  and y  Figure A A gravitational force vector F and a velocity vector v  Ys  Figure A Vector V in the ay plane of a Cartesian reference frame  lvl  V V2 A  The direction for this two dimensional vector can be given in terms of the angu lar displacement from the x axis as a tan   A  x A vector has the same properties magnitude and direction no matter where we position the vector within a single coordinate system And the vector magnitude is independent of the coordinate representation Of course if we change the coor dinate representation the values for the vector components change  For a three dimensiunal Cartesian space we calculate the vector magnitude as lvl VVi i V2 A  Vector direction is given with the direction angles a B and y that the vector makes with each of the coordinate axes Fig A  Direction angles are the posi tive angles that the vector makes with each of the positive coordinate axes We calculate these angles as Vv lv  V l cosB  TI cosy  vl A 1D  cosa   The values cosa cos8 and cosy are called the direction cosines of the vector Actu ally we only need to specify two of the direction cosines to give the direction of V since cos a  cos B cos y  A Vectors are used to represent any quantities that have the properties of magnitude and direction Two common examples are force and velocity Fig A  A force can be thought of as a push or a pull of a certain amount ina par   Figure A Two vectors a can be added geometrically by positioning the two vectors end to end b and drawing the resultant vector from the start of the first vector to the tip of the second vector  ticular direction A velocity vector specifies how fast speed an object is moving in a certain direction  Vector Addition and Scalar Multiplication By definition the sum of two vectors is obtained by adding corresponding com ponents  Vit Va  Way  Van Vi  Vag Vis  Vin  13  Vector addition is illustrated geometrically in Fig A We obtain the vector sum by placing the start position of one vector at the tip of the other vector and draw ing the summation vector as in Fig A  Addition of vectors and scalars is undefined since a scalar always has only one numerical value while a vector has mt numerical components in an n dimen sional space Scalar multiplication of a three dimensional vector is defined as aV  aV aV  aV  A  For example if the scalar parameter a has the value each component of V is doubled  We can also multiply two vectors but there are two possible ways to do this The multiplication can be carried out so that either we obtain another vector or we obtain a scalar quantity  Scalar Product of Two Vectors Vector multiplication for producing a scalar is defined as V Vi Vil Valcos OS 15  where  is the angle between the two vectors Fig A  This product is called the scalar product or dot product of two vectors It is also referred to as the inner product particularly in discussing scalar products in tensor analysis Equa tion A is valid in any coordinate representation and can be interpreted as the product of parallel components of the two vectors  Section A Points and Vectors  wane  Figure A The dot product of two vectors is obtained by multiplying parallel components  Appendix A  In addition to the coordinate independent form of the scalar product we can express this product in specific coordinate representations For a Cartesian reference frame the scalar product is calculated as Vi V2  VisVag  ViyVoy  ViVa A  The dot product of a vector with itself is simply another statement of the Pythagorean theorem Also the scalar product of two vectors is zero if and only if the two vectors are perpendicular orthogonal  Dot products are commutative V V2 VV  A  because this operation produces a scalar and dot products are distributive with respect to vector addition V  V2  V3  Vy V2  Vy V3 A  Vector Product of Two Vectors Multiplication of two vectors to produce another vector is defined as V xV ulVv  V sing O50 sa 19  where u is a unit vector magnitude that is perpendicular to both V and V  Fig A  The direction for u is determined by the right hand rule We grasp an axis that is perpendicular to the plane of V and V so that the fingers of the right hand curl from V to V  Our right thumb then points in the direction of u This product is called the vector product or cross product of two vectors and Equa tion A is valid in any coordinate representation The cross product of two vec tors is a vector that is perpendicular to the plane of the two vectors and with magnitude equal to the area of the parallelogram formed by the two vectors  We can also express the cross product in terms of vector components in a specific reference frame In a Cartesian coordinate system we calculate the com ponents of the cross product as Vy X Va  ViyVaz  VirVoye VieVax  VisVoe VirVay  ViyW2   20  If we let u  u  and u represent unit vectors magnitude along the x y and z axes we can write the cross product in terms of Cartesian components using de terminant notation  Vix Me Figure A The cross product of two vectors is a vector in a direction perpendicular to the two orginal vectors and with a magnitude equal to the area of the shaded parallelogram  u uu VixV  Vi Vy Ve A Vx Vy Va The cross product of any two parallel vectors is zero Therefore the cross product of a vector with itself is zero Also the cross product is not commutative it is anticommutative  V x V   V x V  A  And the cross product is not associative V Xx Vy X Va  V XV X Vy A But the cross product is distributive with respect to vector addition that is  Vy X V2  Va   CV X V2   V1 V3  A  A BASIS VECTORS AND THE METRIC TENSOR We can specify the coordinate axes in any reference frame with a set of vectors one for each axis Fig A  Each coordinate axis vector gives the direction of that axis at any point along the axis These vectors form a linearly independent set of vectors That is the axis vectors cannot be written as linear combinations of each other Also any other vector in that space can be written as a linear combi nation of the axis vectors and the set of axis vectors is called a basis or a set of base vectors for the space In general the space is referred to as a vector space and the basis contains the minimum number of vectors to represent any other vector in the space as a linear combination of the base vectors  Orthonormal Basis Often vectors in a basis are normalized so that each vector has a magnitude of In this case the set of unit vectors is called a normal basis Also for Cartesian reference frames and other commonly used coordinate systems the coordinate axes are mutually perpendicular and the set of base vectors is referred to as an orthogonal basis If in addition the base vectors are all unit vectors we have an orthonormal basis that satisfies the following conditions  uu  for allk u u   for all  k A Most commonly used reference frames are orthogonal but nonorthogonal coor dinate reference frames are useful in some applications including relativity the  ory and visualization of certain data sets For a two dimensional Cartesian system the orthonormal basis is Section A Basis Vectors and the Metric Tensor Figure A My Curvilinear coordinate axis vectors  Appendix A u    uy    A And the orthonormal basis for a three dimensional Cartesian reference frame is u  uy    a    A  Metric Tensor Tensors are generalizations of the notion of a vector Specifically a tensor is a quantity having a number of components depending on the tensor rank and the dimension of the space that satisfy certain transformation properties when con verted from one coordinate representation to another For orthogonal systems the transformation properties are straightforward Formally a vector is a tensor of rank one and a scalar is a tensor of rank zero Another way to view this classi fication is to note that the components of a vector are specified with one sub script while a scalar always has a single value and hence no subscripts A ten sor of rank two thus has two subscripts and in three dimensional space a tensor of rank two has nine components three values for each subscript  For any general curvilinear coordinate system the elements or coeffi cients of the metric tensor for that space are defined as Sa  Uys A Thus the metric tensor is of rank two and it is symmetric    Metric tensors have several useful properties The elements of a metric tensor can be used to de termine  distance between two points in that space  transformation equa tions for conversion to another space and  components of various differential vector operators such as gradient divergence and curl within that space In an orthogonal space  x  for j k A And in a Cartesian coordinate system assuming unit base vectors   ifj k      otherwise 50  The unit base vectors in polar coordinates can be expressed in terms of Cartesian base vectors as u  u cos  uysing up  u rsin  u rcos  A  Substituting these expressions into Eq A we obtain the elements of the metric tensor which can be written in the matrix form  flo Blo  A  For a cylindrical coordinate reference frame the base vectors are u  u cos  u sind Uy  U psin  u pcos  u  A And the matrix representation for the metric tensor in cylindrical coordinates is g  p A  We can write the base vectors in spherical coordinates as u  u cos sing  u sin sing  u cosd u   u rsinOsing  uyrcos sind ug  urcos cosd  uyrsin cos  u rsind A  Then the matrix representation for the metric tensor in spherical coordinates is  B Fsin d A Oo Ff A MATRICES A matrix is a rectangular array of quantities numbers functions or numerical expressions  called the elements of the matrix Some examples of matrices are x 60 01 00 ee x _  00 eh le 3h a a2 ay  y  A  We identify matrices according to the number of rows and number of columns For these examples the matrices in left to right order are by by by and by When the number of rows is the same as the number of columns as in the second example the matrix is called a square matrix  In general we can write an m by n matrix as ay ay sae ay My AQ Ay  A     A am Amr ann where the a represent the elements of matrix A The first subscript of any ele ment gives the row number and the second subscript gives the column number  A matrix with a single row or a single column represents a vector Thus the last two matrix examples in A are respectively a row vector and a column vec tor In general a matrix can be viewed as a collection of row vectors or as a col lection of column vectors  When various operations are expressed in matrix form the standard mathe matical convention is to represent a vector with a column matrix Following this convention we write the matrix representation for a three dimensional vector in Section A Matrices Appendix A  Cartesian coordinates as Vy ve v A Vv  We will use this matrix representation for both points and vectors but we must keep in mind the distinction between them It is often convenient to consider a point as a vector with start position at the coordinate origin within a single coor dinate reference frame but points do not have the properties of vectors that re main invariant when switching from one coordinate system to another Also in general we cannot apply vector operations such as vector addition dot product and cross product to points  Sealar Multiplication and Matrix Addition To multiply a matrix A by a scalar value s we multiply each element a by the scalar As an example if a  then 3a   is Matrix addition is defined only for matrices that have the same number of rows m and the same number of columns For any two m by n matrices the sum is obtained by adding corresponding elements For example   15 val  35   61 Matrix Multiplication The product of two matrices is defined as a generalization of the vector dot prod uct We can multiply an m by n matrix A by a p by q matrix B to form the matrix product AB providing that the number of columns in A is equal to the number of rows in B ie   p  We then obtain the product matrix by forming sums of the products of the elements in the row vectors of A with the corresponding ele ments in the column vectors of B Thus for the following product C AB A we obtain an m by q matrix C whose elements are calculated as mn Cy   abe  A kel In the following example a by matrix is postmultiplied by a by ma trix to produce a by product matrix Qo i1     alk l 3 4  38  14  22 Vector multiplication in matrix notation produces the same result as the dot product providing the first vector is expressed as a row vector and the second vector is expressed as a column vector      This vector product results in a matrix with a single element a by matrix  If we multiply the vectors in reverse order we obtain a by matrix  12 J223  10 12 As the previous two vector products illustrate matrix multiplication in general is not commutative That is  ABBA A But matrix multiplication is distributive with respect to matrix addition A B  C  AB  AC A  Matrix Transpose The transpose A of a matrix is obtained by interchanging rows and columns For example  a T i    abcJFF  A  c For a matrix product the transpose is  AB   BTAT A  Determinant of a Matrix For a square matrix we can combine the matrix elements to produce a single number called the determinant Determinants are defined recursively For a by matrix the second order determinant is defined to be fu  999  Ay707  A   ay an Section A Matrices  Appendix A We then calculate higher order determinants in terms of lower order determi nants To calculate the determinants of order or greater we can select any col umn k of ann by n matrix and compute the determinant as detA     ay detAy A  pl where detA is the n by n  determinant of the submatrix obtained from A by deleting the jth row and the kth column Alternatively we can select any row j and calculate the determinant as n deta   ay detAy A kel Calculating determinants for large matrices n  say can be done more efficiently using numerical methods One way to compute a determinant is to de compose the matrix into two factors A  LU where all elements of matrix L that are above the diagonal are zero and all elements of matrix U that are below the diagonal are zero We then compute the product of the diagonals for both L and U and we obtain detA by multiplying these two products together This method is based on the following property of determinants  det AB   detA det B  A  Another method for calculating determinants is based on Gaussian elimination procedures Section A  Matrix Inverse With square matrices we can obtain an inverse matrix if and only if the determi nant of the matrix is nonzero lf an inverse exists the matrix is said to be a non singular matrix Otherwise the matrix is called a singular matrix For most prac tical applications where a matrix represents a physical operation we can expect the inverse to exist  The inverse of ann by n square matrix A is denoted as A  and AA A A I1 A  where is the identiy matrix All diagonal elements of I have the value and all other off diagonal elements are zero  Elements for the inverse matrix A  can be calculated from the elements of Aas     det A   a4   a5q A where aq is the element in the jth row and kth column of A  and A is the n  by n  submatrix obtained by deleting the kth row and jth column of matrix A Again numerical methods can be used to evaluate the determinant and the elements of the inverse matrix for large values of n A COMPLEX NUMBERS By definition a complex number z is an ordered pair of real numbers z  x y  A  where x is called the real part of z and y is called the imaginary part of z Real and imaginary parts of a complex number are designated as x Re z  y Im  A Geometrically a complex number is represented in the complex plane as in Fig te complex numbers arise from solutions of equations such as e  r 2r which have no real number solutions Thus complex numbers and complex arithmetic are set up as extensions of real numbers that provide solutions to such equations  Addition subtraction and scalar multiplication of complex numbers are carried out using the same rules as for two dimensional vectors Multiplication of complex numbers is defined as  Xp Ye Yo  Oyt2  Wie HY2  XY  A  This definition for complex numbers gives the same result as for real number multiplication when the imaginary parts are zero   O  rg   xyx2  Thus we can write a real number in complex form as x  x  Similarly a pure imaginary number has a real part equal to  y  The complex number  is called the imaginary unit and it is denoted by t   A  imaginary axis Figure A Position of a point z in the complex reataxis plane  Section A Complex Numbers  Appendix A Electrical engineers often use the symbol j for the imaginary unit because the symbol i is used to represent electrical current From the rule for complex multi plication we have        Therefore  is the real number  and i V A  Using the mule for complex multiplication we can write any pure imaginary number in the form iy    y    y Also by the addition nile we can write any complex number as the sum z   Oy Therefore another representation for a complex number is zextiy A  which is the usual form used in practical applications Another concept associated with a complex number is the complex conjugate  zox y A Modulus or absolute value of a complex number is defined to be jzl Ve  A  which gives the length of the vector representing the complex number i e  the distance from the origin of the complex plane to point z  Real and imaginary parts for the division of two complex numbers is obtained as _ He  x yi   Yo xb  yf  A     YY2 X2y1  ae  2ay2  y WD B  A particularly useful representation for complex numbers is to express the tea and imaginary parts in terms of polar coordinates Fig A  z r cos  isin  A imaginary y axis  Figure A Polar coordinate position of a reslxaxis complex number z  We can also write the polar form of z as  res 62 where  is the base of the natural logarithms e  718281828   and  cos  isin  A  which is Euler s formula Complex multiplications and divisions are easily ob tained as Byhq  reat    And the nth reots of a complex number are calculated as   Vi Vilcos  Zar  isin  aryl k   n A64  n  The roots lie on a circle of radius with center at the origin of the complex plane and form the vertices of a regular polygon with n sides  A QUATERNIONS Complex number concepts are extended to higher dimensions with quaternions which are numbers with one real part and three imaginary parts written as g st iatjb ke A where the coefficients a b and c in the imaginary terms are rea numbers and pa rameter s is a real number called the scalar part Parameters i j k are defined with the properties P pP R  ij ji k A  From these properties it follows that jka kj i  ki ik j 67  Section A Quaternions  Appendix A Scalar multiplication is defined in analogy with the corresponding opera tions for vectors and complex numbers That is each of the four components of the quaternion is multiplied by the scalar value Similarly quaternion addition is defined as   Gz  Ss  Sp  fa  ay  f b    key  cy  A Multiplication of two quaternions is carried out using the operations in Eqs A and A  An ordered pair notation for a quaternion is also formed in analogy with complex number notation  q  s v  A  where v is the vector  b c  In this notauon quaternion addition is expressed as  G2    V  vy  A  Quaternion multiplication can then be expressed in terms of vector dot and cross products as   S15  Vy  V2  1V_  Sv  VX Vo  A  As an extension of complex operations the magnitude squared of a quater nion is defined using the vector dot product as Iqit sttvey A And the inverse of a quaternion is qi Tare y  A  so that qq  q q   A NONPARAMETRIC REPRESEN TATIONS When we write object descriptions directly in terms of the coordinates of the ref  erence frame in use the respresentation is called nonparametric For example we can represent a surface with either of the following Cartesian functions  fix y z   or z fix y  A  The first form in A gives an implicil expression for the surface and the second form gives an explicit representation with x and y as the independent variables and with z as the dependent variable Similarly we can represent a three dimensional curved line in nonparamet ric form as the intersection of two surface functions or we could represent the curve with the pair of functions y flx z g x  A  where coordinate x is selected as the independent variable Values for the depen dent variables y and z are then determined from Eqs A as we step through values for x from one line endpoint to the other endpoint  Nonparametric representations are useful in describing objects within a given reference frame but they have some disadvantages when used in graphics algorithms If we want a smooth plot we must change the independent variable whenever the first derivative slope of either f x or g x becomes greater than This means that we must continually check values of the derivatives which may become infinite at some points Also Eqs A provide an awkward format for representing multiple valued functions For instance the implicit equation of a circle centered on the origin in the xy plane is yor  P and the explicit expression for y is the multivalued function y sVAa x In general a more convenient representation for object descriptions in graphics algorithms is in terms of parametric equations  A PARAMETRIC REPRESENTATIONS Euclidean curves are one dimensional objects and positions along the path of a three dimensional curve can be described with a single parameter u That is we can express each of the three Cartesian coordinates in terms of parameter yu and any point on the curve can then be represented with the following vector point function relative to a particular Cartesian reference frame  PQ   x u  y u  u  A Often the coordinate equations can be set up so that parameter u is defined over the unit interval from to For example a circle in the xy plane with center at the coordinate origin could be defined in parametric form as x u  rcos  y u  rsin  zlu   O usl 77  Other parametric forms are also possible for describing circles and circular arcs Curved or plane Euclidean surfaces are two dimensional objects and po sitions on a surface can be described with two parameters u and v A coordinate position on the surface is then represented with the parametric vector function P x   x u v  yu v  z u v  A  Section A Parametric Representations Appendix A   Figure A Section of a spherical surface described by lines of constant u and lines of constant v in Eqs A  where the Cartesian coordinate values for x y and z are expressed as functions of parameters u and v As with curves it is often possible to arrange the parametric descriptions so that parameters u and v are defined over the range from0 to A spherical surface with center at the coordinate origin for example can be de scribed with the equations x u v  rsin au cos 10  y u v  r sin ans sin  u  rcos mu  A where r is the radius of the sphere Parameter u describes lines of constant lati tude over the surface and parameter v describes lines of constant longitude By keeping one of these parameters fixed while varying the other over a subinterval of the range from to we could plot latitude and longitude lines for any spher ical section Fig A  A NUMERICAL METHODS In computer graphics algorithms it is often necessary to solve sets of linear equa tions nonlinear equations integral equations and other functional forms Also to visualize a discrete set of data points it may be useful to display a continuous curve or surface function that approximates the points of the data set In this sec tion we briefly summarize some common algorithms for solving various numer ical problems  Solving Sets of Linear Equations For variables x k     we can write a system of n linear equations as AX  App  F AyyXy  by Ay Xy  Aygk     Ay X  by yy Xy  AygXz    Aan Xy  B  A  where the values for parameters a and b are known This set of equations can be expressed in the matrix form  AX B A with A as an n by m square matrix whose elements are the coefficients aj X as the column matrix of x values and B as the column matrix of b values The solution for the set of simultaneous linear equation can be expressed in matrix form as X A B A  which depends on the inverse of the coefficient matrix A Thus the system of equations can be solved if and only if A is a nonsingular matrix that is its deter minant is nonzero One method for solving the set of equations is Cramer s Rule   det A k deta  A  where A is the matrix A with the kth column replaced with the elements of B This method 1s adequate for problems with a few variables For more than three or four variables the method is extremely inetficient due to the large number of multiplications needed to evaluate each determinant Evaluation of a single n by n determinant requires more that n multiplications  We can solve the system of equations more efficiently using variations of Gaussian elimination The basic ideas in Gaussian elimination can be illustrated with the following set of two simultaneous equations x  2x   A 3x  4x  To solve this set of equations we can multiply the first equation by  then we add the two equations to elimunate the x term yielding the equation 2x   which has the solution x   This value can then be substituted into either of the original equations to obtain the solution for x  which is Efficient algo tithms have been devised to carry out the elimination and back substitution steps  Gaussian elimination is sometimes susceptable to high roundoff errors and it may not be possible to obtain an accurate solution In those cases we may be able to obtain a solution using the Gauss Seidel method We start with an initial guess for the values of variables x  then we repeatedly calculate successive ap proximations until the difference between successive values is small  At each iteration we calculate the approximate values for the variables as _ by  yax  ay3    Minkn X ay by  Oy ky  BX  Oak  nx   Xp EL A  My If we can rearrange matrix A so that each diagonal element has a magnitude greater than the sum of the magnitudes of the other elements acrass that row than the Gauss Seidel method is guaranteed to converge to a solution  Finding Roots of Nonlinear Equations A root of a function f x is a value for x that satisfies the equation f x   One of the most popular methods for finding roots of nonlinear equations is the New ton Raphson algorithm This algorithm is an iterative procedure that approximates a function f x with a straight line at each step of the iteration as shown in Fig A We start with an initial guess x for the value of the root then we calcu  Section A Numerical Methods  tangent line Figure A Approximating a curve at an initial value xq with a straight line that is tangent to the curve at that point  late the next approximation to the root as x by determining where the tangent line from xp crosses the x axis At xg the slope first derivative of the curve is  d  af _ feo A dx  xXq Xy Thus the next approximation to the root is x  Xp  xo  A Oo fo f  We repeat this procedure at each calculated approximation until the difference between successive approximations is small enough  If the Newton Raphson algorithm converges to a root it will converge faster than any other root finding method But it may not always converge For example the method fails if the derivative f x is at some point in the iteration Also depending on the oscillations of the curve successive approximation may diverge from the position of a root The Newton Raphson algorithm can be ap plied to a function of a complex variable f z  and to sets of simultaneous nonlin ear functions real or complex  Another method slower but guaranteed to converge is the bisection method Here we need to first determine an x interval that contains a root then we apply a binary search procedure to close in on the root We first look at the midpoint of the interval to determine whether the root is in the lower or upper half of the in terval This procedure is repeated for each successive subinterval until the differ ence between successive midpoint positions is smaller than some preset value A speedup can be attained by interpolating successive x positions instead of halv ing each subinterval false position method  Evaluating Integrals  Integration is a summation process For a function of a single variable x the inte gral of f x is the area under the curve as illustrated in Fig A An integral of fix can be numerically approximated with the following summation a  food  feooda  A b k where f x is an approximation to f x over the interval Ax  For example we can approximate the curve with a constant value in each subinterval and add the areas of the resulting rectangles Fig A  The smaller the subdivisions for the interval fram a to b the better the approximation up to a point  Actually if Section A Numerical Methods Figure A The integral of f x is equal ta the amount of area between the function and the x axis over the interval froma to b      f x  J xg a xy vas x  Figure A Approximating an integral as the sum of the areas of small rectangles  the intervals get too small the values of successive rectangular areas can get lost in the roundoff error  Polynomial approximations for the function in each subinterval generally give better results than the rectangle approach Using a linear approximation we obtain subareas that are trapezoids and the approximation methad is then re ferred to as the trapezoid rule lf we use a quadratic polynomial parabola to ap proximate the function in each subinterval the method is called Simpson s rule and the integral approximation is  b Ax n n  faddx   Aa  fb   fly   fx  A  a oddk evenk where the interval from a to bis divided into n equal width intervals  b a Ax  A n where n is a multiple of and with Xo a Xy  Xo  Ax k   n For functions with high frequency oscillations Fig A  the approxima tion methods previously discussed may not give accurate results Also multiple integrals involving several integration variables are difficult to solve with Simp  fix  Figure A _ _ A function with high frequency a b x  oscillations  son s rule or the other approximation methods In these cases we can apply Monte Carlo integration techniques The term Monte Carlo is applied to any method that uses random numbers to solve deterministic problems  We apply a Monte Carlo method to evaluate the integral of a function such as the one shown in Fig A by generating n random positions in a rectangular area that contains f x over the interval from a to b Fig A  An approximation for the integral is then calculated as b  fade  ho  a  sou A  where parameter Noun the count of the number of random points that are be tween f x and the x axis A random position x y in the rectangular region is computed by first generating two random numbers r and and then carrying out the calculations  Vinx  Yin x a r b a  Y  Ymin  72h A  Similar methods can be applied to multiple integrals  Random numbers r and r are uniformly distributed over the interval   We can obtain random numbers from a randam number function in a high level language or from a statistical package or we can use the following algorithm called the linear congruential generator  i  aiy   e modm  k   ik A  m  EF where parameters a c m and ig are integers and ig is a starting value called the seed Parameter m is chosen to be as large as possible on a particular machine with values for a and c chosen to make the string of random numbers as long as possible before a value is repeated For example on a machine with bit integer representations we can set m    and    toy    Yonex a olxyl b Figure A A rectangular area enclosing a Yeni function f x over the interval   b  Fitting Curves to Data Sets Section A waa    Numerical Methods A standard method for fitting a function linear or nonlinear to a set of data points is the least squares algorithm For a two dimensional set of data points yy yd K    we first select a functional form f x  which could be a straight line function a polynomial function or some other curve shape We then determine the differences deviations between f x and the y values at each x and compute the sum of deviations squared  E Sly  fool A k Parameters in the function f x are determined by minimizing the expression for E For example for the linear function fx  ag  yx parameters ap and a are assigned values that minimize E We determine the val ues for a and a by solving the two simultaneous linear equations that result from the minimization requirements That is E will be minimum if the partial de rivative with respect to a is and the partial derivative with respect to a is  0E dE     Ay 0a  Similar calculations are carried out for other functions For the polynomial fi  a9  a x t agx     a x we need to solve a set of n linear equations to determine valves for parameters  And we can also apply least squares fitting to functions of several variables f xy Xz   X  that can be linear or nonlinear in each of the variables   Bibliography  AKELEY K AND T JERMOLUK   High Performance Polygon Rendering  in proceedings of SIGGRAPH  Computer Graphics  pp 246  AKELEY K   RealityEngine Graphics  in proceed ings of SIGGRAPH  Computer Graphics Proceedings pp 116  AMANATIDES J   Ray Tracing with Cones  in pro ceedings of SIGGRAPH  Computer Graphics  pp 135  AMBURN P  E GRANT AND T WHITTED   Managing Geometric Complexity with Enhanced Procedural Mod els  in proceedings of SIGGRAPH  Computer Graph ics  pp 196  AN YO K  Y USAMI AND T KURIHARA   A Simple Method for Extracting the Natural Beauty of Hair  in proceedings of SIGGRAPH  Computer Graphics  pp 120  APPLE COMPUTER INC   inside Macintosh Volume Addison Wesley Reading MA  APPLE Computer INC   Human Interface Guidelines The Apple Desktop Interface Addison Wesley Reading MA  Arvo J AND D Kirk   Fast Ray Tracing by Ray Clas sification  in proceedings of SIGGRAPH  Computer Graphics  pp 64 Arvo J AND D Kikk   Particle Transport and Image Synthesis  in proceedings of SIGGRAPH  Computer Graphics  pp 66 ARvo J  ED   Graphics Gems H Academic Press Inc  San Diego CA  ATHERTON P R   A Scan Line Hidden Surface Re moval Procedure for Constructive Solid Geometry  in proceedings of SIGGRAPH  Computer Graphics  pp 82  Bararr D   Analytical Methods for Dynamic Simu lation of Non Penetrating Rigid Bodies  in proceedings of SIGGRAPH  Computer Graphics  pp 232  Bararr D AND A WITKIN   Dynamic Simulation of Non Penetrating Flexible Bodies  in proceedings of SIGGRAPH  Computer Graphics  pp 308  BARKANS A C   High Speed High Quality An tialised Vector Generation  in proceedings of SIG GRAPH  Computer Graphics  pp 326  BARNSLEY M F  A JACQUIN F MALASSENT ET AL   Harnessing Chaos for Image Synthesis  in proceed ings of SIGGRAPH  Computer Graphics  pp 1490  Barnsey M   Fractals Everywhere Second Edition Academic Press Inc  San Diego CA  Barr A H   Superquadrics and Angle Preserving Transformations  EEE Computer Graphics and Applica tions  pp 23  Barr A H   Ray Tracing Deformed Surfaces  in proceedings of SIGGRAPH  Computer Graphics  pp 296  Barsky B A AND J C BEATTY   Local Control of Bias and Tension in Beta Splines  ACM Transactions on Graphics  pp 134  Barsky B A   A Discription and Evaluation of Vari ous D Models  EEE Computer Graphics and Applica tions  pp 52  Barzel R aND A H Barr   A Modeling System Based on Dynamic Constraints  in proceedings of SIG GRAPH  Computer Graphics  pp  Barze  R   Physically Based Modeling for Computer Graphics Academic Press Inc  San Diego CA  Baum D R  MANN K P SmrtH FT AL   Making Radiosity Usable Autamatic Preprocessing and Mesh ing Techniques for the Generation of Accurate Radiosity Solutions  in proceedings of SIGGRAPH  Computer Graphics  pp 61  Becker S C  W A BARRETT AND D R OLSEN Jr   Interactive Measurement of Three Dimensional Ob jects Using a Depth Buffer and Linear Probe  ACM Transactions on Graphics  pp 207  Becker B G and N L Max   Smooth Transitions between Bump Rendering Algorithms  in proceedings of SIGGRAPH Computer Graphics Proceedings pp 190  Brier T AND S NEELY   Feature Based Image Meta morphosis  in proceedings of SIGGRAPH  Com puter Graphics  pp 42 BERGMAN L  H Fucus E GRANT ET AL   Image Rendering by Adaptive Refinement  in proceedings of SIGGRAPH  Computer Graphics  pp 38  BERGMAN L D  J S RICHARDSON D C RICHARDSON ET AL   VIEW an Eploratory Molecular Visualization System with User Definable Interaction Sequences  in proceedings of SIGGRAPH  Computer Graphics Pro ceedings pp 126  Bezier P   Numerical Control Mathematics and Appli cations translated by A R Forrest and A F Pankhurst John Wiley  Sons London  Bier E A  S A MacKay D A STEWART ET AL   Snap Dragging  in proceedings of SIGGRAPH  Computer Graphics  pp 248  Bier E A  M C STONE K PIER ET AL   Toolglass and Magic Lenses The See Through Interface  in pro ceedings of SIGGRAPH  Computer Graphics Proceed ings pp 80  Bishop G AND D M WIEMER   Fast Phong Shading  in proceedings of SIGGRAPH  Computer Graphics  pp 106  BLAKE J W   PHIGS and PHIGS Plus Academic Press London  BLESER T   TAE Plus Styleguide User Interface De scription  NASA Goddard Space Flight Center Green belt MD  Bunn J F anp M FE NeweLt   Texture and Reflec tion in Computer Generated Images  CACM 10  pp 547  BLINN J F   Models of Light Reflection for Com puter Synthesized Pictures  Computer Graphics  pp 198  BLINN J F AND M E NEWELL   Clipping Using Ho mogeneous Coordinates  Computer Graphics  pp 251 BLINN J F   Simulation of Wrinkled Surfaces  Computer Graphics  pp 292  BLINN J F   A Generalization of Algebraic Surface Drawing  ACM Transactions on Graphics  pp 256  BLINN J F   Light Reflection Functions for Simula tion of Clouds and Dusty Surfaces  in proceedings of SIGGRAPH  Computer Graphics  pp 29  BLINN J F   A Trip Down the Graphics Pipeline The Homogeneous Perspective Transform  EEE Com puter Graphics and Applications  pp 80  BLOOMENTHAL J   Modeling the Mighty Maple  in proceedings of SIGGRAPH  Computer Graphics  pp 312  Bono P R  J L ENCARNACAO E R A Hopcoop ET AL   GKS The First Graphics Standard  EEE Com puter Graphics and Applications  pp 23  Booth K S  M P BryDEn W B Cowan ET AL   On the Parameters of Human Visual Performance An In vestigation of the Benefits of Antialiasing  IEEE Com puter Graphics and Applications  pp 41  BRESENHAM J E   Algorithm for Computer Control of A Digital Plotter  IBM Systems Journal  pp 30  BRESENHAM J E   A Linear Algorithm for Incremen tal Digital Display of Circular Arcs  CACM  pp 106  Brooks F P  Jr   Walkthrough A Dynamic Graph ics System for Simulating Virtual Buildings  Interactive 3D  Brooks F P  Jr   Grasping Reality Through Tlu sion Interactive Graphics Serving Science  CHI  pp 11  Brooks J  P FREDERICK M QUH YOUNG J J BATTER ET AL   Project GROPE  Haptic Display for Scientific Visualization  in proceedings of SIGGRAPH  Com puter Graphics   pp 185  Brown M H AND R SEDGEWICK   A System for Al gorithm Animation  in proceedings of SIGGRAPH  Computer Graphics  pp 186  Brown J R AND S CUNNINGHAM  Programming the User Interface John Wiley  Sons New York  BRUDERLIN A AND T W CALVERT   Goal Directed Dynamic Animation of Human Walking  in proceed ings of SIGGRAPH  Computer Graphics  pp 242  BRUNET P AND I NavaZo   Solid Representation and Operation Using Extended Octrees  ACM Transactions on Graphics  pp 197  BRYSON S AND C Levit   The Virtual Wind Tunnel  IEEE Computer Graphics and Applications  pp 34  Burt P J AND E H ADELSON   A Multiresolution Spline with Application to Image Mosaics  ACM Transactions on Graphics  pp 236  BuxTon W  M R LaMB D SHERMAN ET AL   To wards a Comprehensive User Interface Management System  in proceedings of SIGGRAPH  Computer Graphics  pp 42  BuxTon W  R HILL aND P ROWLEY   Issues and Techniques in Touch Sensitive Tablet Input  in pro ceedings of SIGGRAPH  Computer Graphics  pp 224  CaLvert T  A BRUDERLIN   DILL ET AL   Desktop Animation of Multiple Human Figures  IEEE Computer Graphics and Applications  pp  26  CAMBELL G  F A DEFANTI   FREDERIKSEN ET AL   Two Bit Pixel Full Color Encoding  in proceedings of SIGGRAPH  Computer Graphics  pp 224  Camps Lt IIL A T AND D  FUsseLL   Adaptive Mesh Generation for Global Diffuse Illumination  in proceedings of SIGGRAPH  Computer Graphics  pp 164  Carp S K  J D MACKINLAY AND G G ROBERTSON   The Information Visualizer an Information Work space  CHI  pp 188 CARIGNAN M  Y YANG N M THALMANN  T AL   Dressing Animated Synthetic Actors with Complex Deformable Clothes  in proceedings of SIGGRAPH  Computer Graphics  pp 104  CARLBOM I  I CHAKRAVARTY AND D VANDERSCHEL  A Hierarchical Data Structure for Representing the Spatial Decomposition of D Objects  IEEE Compuler Graphics and Applications  pp 31  CARPENTER L   The A Buffer An Antialiased Hid den Surface Method  in proceedings of SIGGRAPH  Computer Graphics  pp 108  CARROLL J M AND C CARRITHERS  Training Wheels in a User Interface  CACM  pp 806 CASALE M S AND E L STANTON   An Overview of Analytic Solid Modeling  IEEE Computer Graphics and Applications  pp 56  CATMULL FE   Computer Display of Curved Sur faces  in proceedings of the IEEE Conference on Com puter Graphics Pattern Recognition and Data Structures Also in Freeman  pp 315 CaTMULL E   An Analytic Visible Surface Algo rithm for Independent Pixe Processing  in proceed ings of SIGGRAPH  Computer Graphics  pp 115  CHAZELLE B AND J INCERPI   Triangulation atl Shape Complexity  ACM Transactions on Graphics  pp 152  Cuen M  S J MOUNTFURD AND A SELLEN   A Study in Interactive 3D Rotation Using 2D Control Devices  in proceedings of SIGGRAPH  Computer Graphics  pp 130  Cuen S E  H E RUSHMEIER G MILLER ET AL   A Progressive Multi Pass Method for Global Ilumina tion  in proceedings of SIGGRAPH  Computer Graphics  pp 174  CHIN N AND S FEINER   Near Real Time Shadow Generation Using BSP Trees  in proceedings of SIG GRAPH  Computer Graphics  pp 106  CHUANG R AND G ENTIS   D Shaded Computer Animation Step by Step  EEE Computer Graphics and Applications  pp 25  CHUNG J C ET AL   Exploring Virtual Worlds with Head Mounted Visual Displays  Proceedings of SPiE Meeting on Non Holographic True Dimensional Display Technologies  January pp 20 CLARK J H   The Geometry Engine A VLSI Georm etry System for Graphics  in proceedings of SIG GRAPH  Computer Graphics  pp 133  Couen M F AND D P GREENBERG   The Hemi Cube A Radiosity Solution for Complex Environ ments  in proceedings of SIGGRAPH  Computer Graphics  pp 40  COHEN M F S E CHEN J R WALLACE ET AL   A Progressive Refinement Approach to Fast Radiosity Image Generation  in proceedings of SIGGRAPH  Computer Graphics  pp 84  COHEN M F AND J R WALLAC   Radtosity and Realis inc Image Synthesis Academic Press Boston MA Cook R L AND K E TorraNcr   A Reflectance Model for Computer Graphics  ACM Transactions on Graphics  pp 24  Cook R L  T PorTER AND L CARPENTER   Distrib uted Ray Tracing  in proceedings of SIGGRAPH  Computer Graphics  pp 145 Cook R L   Shade Trees  in proceedings af SIG GRAPH  Computer Graphics  pp 231 Cook R L   Stochastic Sampling in Computer Graphics  ACM Transactions on Graphics  pp 72  Coon R L  L Carpenter aso EB Carmutt   The Reyes Image Rendering Architecture  in proceedings of SIGGRAPH  Computer Graphics   pp 102  CoouUILLART S AND P JANCENE   Animated Free Form Deformation An Interactive Animation Tech nique  in proceedings of SIGGRAPH  Computer Graphics  pp 26  Crow F C   The Ahasing Problem in Computer Synthesizec Shaded Images  CACM  pp 805 Crow F C   Shadow Algorithms for Computer Graphics  in proceedings of SIGGRAPH  Computer Graphics  pp 248 Crow F C   The Lise of Grayscale for Improved Raster Display of Vectors and Characters  in proceed ings of SIGGRAPH  Computer Graphics  pp   Crow FC   A Comparison of Antialiasing Tech niques  EEE Computer Graphics and Applications  pp 49  Crow F C   A More Flexible Image Generation En vironment  in proceedings of SIGGRAPH Com puter Graphics  pp 18  Cruz Neira C  D J SANDIN AND T A DEFANTI   Surround Screen Projection Based Virtual Reality The Design and Implementatior of the CAVE  in proceed ings of SIGGRAPH Computer Graphics Proceedings pp 142 CUNNINGHAM S  N K CRAIGHILL M W FONG ET AL  ED   Computer Grapincs Using Object Oriented Pro gramming John Wiley  Sons New York  CuTLer E  D GILLY AND T O ReIuty ED  Tue X Win dow System in a Nutshell Second Edition O Reilly  Assoc  Inc  Sebastopol CA Cyrus M AND J Beck   Generalized Two and Three Dimensional Clipping  Computers and Graph ics  pp 28  Day A M   The Implementation of an Algorithm to Find the Convex Hull of a Set of Three Dimensional Points  ACM Transactions on Graphics  pp 132  De Reerve P  C EDELIN J FRANCON ET aL   Plant Models Faithful to Botanical Structure and Develop ment  in proceedings of SIGGRAPH  Computer Graphics  pp 158 DEERING M   High Resolution Virtual Reality  in proceedings of SIGGRAPEL  Computer Graphics  pp 202  DEERING M F AND S R NELSON   Leo A System for Cost Effective 3D Shaded Graphics  in proceedings ot SIGGRAPH  Computer Graphics Proceedings pp 108  Demko S  L HODGES AND B NAYLOR   Construction of Fractal Objects with Iterated Function Systems  in proceedings of SIGGRAPH  Computer Graphics  pp 278  Derr S W aND W E Howarp   Flat Panel Dis plays  Scientific American  pp 97  DeRose T D   Geometric Continuity Shape Para meters and Geometric Constructions for Catmull Rom Splines  ACM Transactions on Graphics  pp 41  DicivaL EQUIPMENT Corp   Digital Equipment Cor poration XUI Style Guide  Maynard MA  Dirre M AND J SWENSEN   An Adaptive Subdivi sion Algorithm and Parallel Architecture for Realistic Image Synthesis  in proceedings of SIGGRAPH  Computer Graphics  pp 158  Doskin D  L Gurpas J HERSHBERGER ET AL   An Efficiert Algorithm for Finding the CSG Representation of a Simple Polygon  in proceedings of SIGGRAPH  Computer Graphics  pp 40  Doctor L J AND J G TORBERG   Display Tech niques for Octree Encoded Objects  IEEE Computer Graphics and Applications  pp 38  Dorsey J O  FX SILLION AND D P GREENBERG  Design and Simulation of Opera Lighting and Projec  tion Effects  in proceedings of SIGGRAPH  Cont puter Graphics  pp 50  Dresin R A L CARPENTER AND P HANRAHAN  Volume Rendering  in proceedings of SIGGRAPH  Computer Graphics  pp 74  Durr T   Compositing 3D Rendered Images  in proceedings of SIGGRAPH  Computer Graphics  pp 44  DurerETT H J  ED   Color and the Computer Academic Press Boston  DUVANENKO V   Improved Line Segment Clipping  Dr Dobb s Journal July  DYER S AND S WHITMAN   A Vectorized Scan Line Z Buffer Rendering Algorithm  IEEE Computer Graph ics and Applications  pp 45  Dyer S   A Dataflow Toolkit for Visualization  IEEE Computer Graphics and Applications  pp 69  EarNsHaw R A  ED   Fundamental Algorithms for Computer Graphics Springer Verlag Berlin  EDFISBRUNNER H   Algorithms in Computational Geometry Springer Verlag Berlin  EDELSBRUNNER H AND E P MUCKE   Simulation of Simplicity A Technique to Cope with Degenerate Cases in Geometric Algorithms  ACM Transactions on Graph ics  pp 104  ELBER G AND E COHEN   Hidden Curve Removal for Free Form Surfaces  in proceedings of SIGGRAPH  Computer Graphics  pp 104  ENDERLE G  K KANSY AND G PFAFF  Computer Graphics Programming GKS The Graphics Standard Springer Verlag Berlin  Farin G   Curves and Surfaces for Computer Aided Geo metric Design Academic Press Boston MA  Farouk R T AND J K Hinps   A Hierarchy of Geo metric Forms  IEEE Computer Graphics and Applications  pp 78  FEDER J   Fractals Plenum Press New York  FEINER S  S Nacy AND A VAN Dam   An Experi mental System for Creating and Presenting Interactive Graphical Documents  ACM Transactions on Graphics  pp 77  FeRWERDA J A AND D P GREENBERG   A Psy chophysical Approach to Assessing the Quality of An tialiased Images  IEEE Computer Graphics and Applica tons  pp 95  FisHKIN K P AND B A Barsky   A Family of New Algorithms for Soft Filling  in proceedings of SIG GRAPH  Computer Graphics   pp 244  Fume E L   The Mathematical Structure of Raster Graphics Academic Press Boston  Fotey J D  V L WALLACE AND P CHAN   The Human Factors of Computer Graphics Interaction Tech niques  IEEE Computer Graphics and Applications 11  pp 48  Fotey J D   Interfaces for Advanced Computing  Scientific American  pp 135  Fotey J D  A VAN Dam  K FEINER ET AL   Com puter Graphics Principles and Practice Addison Wesley Reading MA  Fournier A  D FUSSEL AND L CARPENTER   Com puter Rendering of Stochastic Models  CACM  pp 384  Fournier A AND D Y MONTUNO   Triangulating Simple Polygons and Equivalent Problems  ACM Transactions on Graphics  pp 174   FOURNIER A AND W T REEVES   A Simple Model of Ocean Waves  in proceedings of SIGGRAPH  Com puter Graphics  pp 84  FOURNIER A AND D FUSSELL   On the Power of the Frame Buffer  ACM Transactions on Graphics  pp 128  FOURNIER A AND E Fiume   Constant Time Filtering with Space Variant Kernels  in proceedings of SIG GRAPH  Computer Graphics  pp 238  Fow er D R  H MEINHARDT AND P PRUSINKIEWICZ   Modeling Seashells  in proceedings of SIGGRAPH  Computer Graphics  pp 387 Fox D AND M WalITE  Computer Animation Primer McGraw Hill New York  Francis G K   A Topological Picturebook Springer Verlag New York  FRANKLIN W R AND M S KANKANHALLI   Parallel Object Space Hidden Surface Removal  in proceedings of SIGGRAPH  Computer Graphics  pp 94  FREEMAN H eb   Tutorial and Selected readings in In teractive Computer Graphics IEEE Computer Society Press Silver Springs MD  FRENKEL K A   Volume Rendering  CACM  pp 435  FRieDER G  D GORDON AND R A REYNOLD   Back to Front Display of Voxel Based Objects  EEE Com puter Graphics and Applications  pp 60  FRIEDHOFF R M AND W BENZON  The Second Com puter Revolution Visualization Harry N Abrams Inc  New York Fucus H  S M PIZer E R HEINZ S H BLoOMBER L Tsai AND D C STRICKLAND   Design of and Lmage Editing with a Space Filling Three Dimensional Display Based on a Standard Raster Graphics System  Proceed ings of SPIE  August pp 127  Fucus H   Poutton J EYLes ET at   Pixel Planes A Heterogeneous Multiprocessor Graphics System Using Processor Enhanced Memories  in proceedings of SIGGRAPH  Computer Graphics  pp 88  Fujimoto A ano K Iwata   Jag Free Images on Raster Displays  EEE Computer Graphics and Applica  _ Hons  pp 34  FUNKHOUSER T A AND C H SEQUIN   Adaptive Dis play Algorithms for Interactive Frame Rates During Vi sualizatior Complex Virtual Environments  in pro ceedings of SIGGRAPH  Computer Graphics Proceedings pp 254  GALYEAN T A AND J F HucHes   Sculpting An In teractive Volumetric Modeling Technique  in proceed ings of SIGGRAPH Computer Graphics  pp 274  GaRDNER G Y   Visual Simulation of Clouds  in proceedings of SIGGRAPH  Computer Graphics  pp 304 GascueL M P   An Implicit Formulation for Pre cise Contact Modeling between Flexible Solids  in pro ceedings of SIGGRAPH  Computer Graphics pp 320  GaskINS T   PHIGS Programming Manual O Reilly  Associates Sebastopo  CA  GHARACHORLOO N  S Gupta R F SPROULL ET AL   A Characterization of Ten Rasterization Algorithms  in proceedings of SIGGRAPH  Computer Graphics  pp 368  GirarD M   Interactive Design of 3D Computer Animated Legged Animal Motion  EEE Computer Graphics and Applications  pp 51  GLASSNER A S   Space Subdivision for Fast Ray Tracing  EFF Compuier Graphics and Applications  pp 22  GLASsNER A S   Adaptive Precision in Texture Mapping  in proceedings of SIGGRAPH  Computer Grapltics  pp 306  GLASSNER A S   Spacetime Ray Tracing for Anima tion  IEEE Computer Graphics and Applications  pp 70  GLASSNER A S  ED   An Introduction to Ray Tracing Academic Press San Diego CA  GLASSNER A  ED   Graphics Gems Academic Press San Diego CA  GLASSNER A S   Geometric Substitution A Tutor jal  IEEE Computer Graphics and Applications  pp 36  GLASSNER A S   Principles of Digital Image Synthesis Morgan Kaufmann Inc  New York  GLEICHER M AND A WITKIN   Through the Lens Camera Control  in proceedings of SIGGRAPH  Computer Graphics  pp 349  GOLDSMITH J AND J SALMON   Automatic Creation of Object Hierarchies for Ray Tracing  IEEE Computer Graphics and Applications  pp 20  GONZALEZ R C AND P WINT2  Digital Image Process ing Addison Wesley Reading MA Goop D M  J A WHITESICE D R WIxXoN AND S J Janes   Building A User Derived Interface  CACM 10  pp 1042  GoopMan T AND R SPENCE   The Effect of System Response Time on Interactive Computer Aided Prob lem Solving  in proceedings of SIGGRAPH  Com puter Graphics  pp 104  Gora C M  K E ToRRANCE D P GREENBERG ET AL   Modeling the Interaction of Light Between Dif fuse Surfaces  in proceedings of SIGGRAPH  Com puter Graphics  pp 222 Goxvon D AND S CHEN   Fron to Back Display of BSP Trees  JEEE Computer Graphics and Applications  pp 85 Gorter  J  P SCHRODER MF COHEN ET AL   Wavelet Radiosity  in proceedings of SIGGRAPH  Camputer Graphics Proceedings pp 230  GREEN M   The University of Alberta User Interface Management System  in proceedings of SIGGRAPH  Camputer Graphics  pp 214 GREENE N  M Kass AND G MILLER   Hierarchical Z Buffer Visibility  in proceedings of SIGGRAPH Computer Graphics Proceedings pp 238  Hareeru P AND K AKELLY   The Accumulation Buffer Hardware Support for High Quality Render ing  in proceedings of SIGGRAPH  Computer Graph ics  pp 318 HAHN J K   Realistic Animation of Rigid Bodies  in proceedings of SIGGRAPH  Computer Graphics  pp 308  HALL R A AND D P GREENBERG  A Testbed for Re alistic Image Synthesis  1EFE Computer Graphics and Applications  pp 20  Hatt R   Hiemmation ami Color in Computer Gener ated Imagery Springer Verlag New York HANRAHAN P   Creating Volume Models from Edge Vertex Graphs  in proceedings of SIGGRAPH  Computer Graphics  pp 84  HANRAHAN P AND J LAWSON   A Language for Shading and Lighting Calculations  in proceedings of SIGGRAPH  Computer Graphics  pp 298  Hart J C  D J SANDIN AND L H KAUFFMAN   Ray Tracing Deterministic 3D Fractals  in proceedings of SIGGRAPH  Computer Graphics  pp 296  Hart J C and T A DEFANTI   Efficient Antialiased Rendering of D Linear Fractals  in proceedings of SIGGRAPH  Computer Graphics  pp 100  HE X D  P O HEYNEN R L PHILLIPS ET AL   A Fast and Accurate Light Reflection Model  in proceedings of SIGGRAPH  Computer Graphics  pp 254  HEARN D AND P Baker   Scientific Visualization An Introduction  Eurographics Technical Report Se ries Tutorial Lecture  HECKBERT P   Color Image Quantization for Frame Buffer Display  in proceedings of SIGGRAPH  Com puter Graphics  pp 307  HECKBERT P AND P HANRAHAN   Beam Tracing Polygonal Objects  in proceedings of SIGGRAPH  Computer Graphics  pp 127  Hopcoop FR A  D A Duce J R GALLOP ET AL  Introduction to the Graphical Kernel System GKS  Acadc mic Press London  Hopcoon FR A AND D A Duce  A Primer for PHIGS John Wiley  Sons Chichester England  Hope H  T DEROSE T MCDONALD ET AL   Mesh Optimization  in proceedings of SIGGRAPH  Com puter Graphics Proceedings pp 26  Howarp T L J  W T Hewrrt R J HUBBOLD ET ac   A Practical Introduction to PHIGS and PHIGS Plus Addi son Wesley Wokingham England  HuGues J F   Scheduled Fourier Volume Morph ing  in proceedings of SIGGRAPH  Computer Graph ics  pp 46  Hurrric H AND M Nanas   B Spline Surfaces A Tool for Computer Painting  IEEE Computer Graphics and Applications  pp 47  Tkebo T   High Speed Techniques for a D Color Graphics Terminal  FEE Computer Graphics and Appli cations  pp 58  IMMEL D S  M FE COHEN AND D P GREENBERG   A Radiosity Method for Non Diffuse Environments  in proceedings of SIGGRAPH  Computer Graphics  pp 142  Isaacs P M AND M F COHEN   Controlling Dv namic Simulation with Kinematic Constraints Behavior Functions and Inverse Dynamics  in proceedings of SIGGRAPH  Computer Graphics  pp 224 Jarvis J FC N Jupice aND W H NINKE   A Sur vey of Techniques for the Image Display of Continuous Tone Pictures on Bilevel Displays  Computer Graphic and Image Processing  pp 40  JOHNSON S A   Clinical Varifocal Mirror Display System at the University uf Utah  Proceedings of SPIE  August pp 148  Kanrya J T   New Techniques for Ray Tracing Pro cedurally Defined Objects  ACM Transactions on Graph ies  pp 181 Kaliya J T   The Rendering Equation  in proceed ings of SIGGRAPH  Computer Graphics  pp 150 Kayrva J T AND T L Kay   Rendering Fur with Three Dimensional Textures  in proceedings of SIG GRAPH  Computer Graphics  pp 280 Kappet M R   An Ellipse Drawing Algorithm for Faster Displays  in Fundamental Algorithms for Com puter Graphics Springer Verlag Berlin pp 280  Karasick M  D LIEBER AND L R NACKMAN   Effi cient Delaunay Triangulation Using Rational Arith metic  ACM Transactions on Graphics  pp 91  Kass M   CONDOR Constraint Based Dataflow  in proceedings of SIGGRAPH  Computer Graphics  pp 330  Kasson J M AND W PLourFE   An Analysis of Se jected Computer Interchange Color Spaces  ACM Transactions on Graphics   pp 405  KaurMan A   Efficient Algorithms for 3D Scan Conversion of Parametric Curves Surfaces and Vol umes  in proceedings of SIGGRAPH  Computer Graphics  pp 179  KawaGucHl Y   A Morpholog cal Study of the Form of Nature  in proceedings of SIGGRAPH  Computer Graphics  pp 232  Kay T L anp J T Kana   Ray Tracing Complex Scenes  in proceedings of SIGGRAPH  Computer Graphics  pp 278  Kay D C AND J R Levine  Grapltics File Formats Windcrest McGraw Hill New York Kasey A D  M C Matin ann G M NIELSON   Terrain Simulation using a Model af Stream Erosion  in proceedings of SIGGRAPH  Computer Graphics  pp 264  Ken J R  WOE CARLSON AND R E PARENT   Shape Transformation for Polyhedral Objects  in proceedings of SIGGRAPH  Computer Graphics  pp 54  Kirk D aND J Arvo   Unbiased Sampling Tech niques for Image Synthesis  in proceedings of SIG GRAPH  Computer Graphics  pp 156 Kirk D  ED   Graphics Gems Hl Academic Press San Diego CA  Knutu D E   Digital Halftones by Dot Diffusion  ACM Transactions on Graphics  pp 273  Kocuanek D H U ano R H Bartets   Interpolat ing Splines with Local Tension Continuity and Bias Control  in proceedings of SIGGRAPH  Contputer Graphics  pp 41 Kon E K anD D HEARN   Fast Generation and Sur face Structuring Methods for Terrain and Other Natural Phenomena  in proceedings of Eurographs Com puter Graphics Forum  pp C 180  Korien J U AND N  BADLER   Techniques for Gen erating the Goal Directed Motion of Articulated Struc tures  IEEE Computer Graphics and Applications  pp 81  KOREN J U AND N I BADLER   Temporal antialias ing in Computer Generated Animation  in proceed ings of SIGGRAPH  Computer Graphics  pp 388  Lasseter J   Principles of Traditional Animation Applied to 3D Computer Animation  in proceedings of SIGGRAPH  Computer Graphics  pp 44  Laur D AND P HANRAHAN   Hierarchical Splatting A Progressive Refinement Algorithm for Volume Ren dering  in proceedings of SIGGRAPH  Computer Graphics  pp 288  LaurEL B   The Art of Human Computer Interface De sign Addision Wesley Reading MA  Lee M E  R A REDNER AND P USELTON   Statisi cally Optimized Sampling for Distributed Ray Tracing  in proceedings of SIGGRAPH  Computer Graphics  pp 68  LEVINTHAL A AND T PORTER   CHAP  A SIMD Graphics Processor  in proceedings of SIGGRAPH  Computer Graphics  pp 82  Lzvoy M   Display of Surfaces from Volume Data IEEE Computer Graphics and Applications  pp 37 Levoy M   A Hybrid Ray Tracer for Rendering Polygon and Volume Data  IEEE Computer Craphics and Applications  pp 40  Lewss J P   Algorithms for Solid Noise Synthesis  in proceedings of SIGGRAPH  Computer Graphics  pp 270  Lian Y D 4NnD B A BARSKy   An Analysis and Al gorithm for Polygon Clipping  CACM 11  pp 877  LIANG Y D AND B A Barsky   A New Concept anc Method for Line Clipping  ACM Transactions on Graph ics  pp 22  Lien S L  M SHANTZ AND V PRaTT   Adaptive For ward Differencing for Rendering Curves and Surfaces  in proceedings of SIGGRAPH  Computer Graphics  pp 118  LINDLEY C A   Practical Ray Tracing in C John Wiley  Sons New York  LISCHINSKI D  F TAMPIERI AND D P GREENBERG  Combining Hierarchical Radiosity and Discontinuity Meshing  in proceedings of SIGGRAPH  Computer Graphics pp 208  Latwinowicz P C   Inkwell A D Animation System  in proceedings of SIGGRAPH  Computer Graphics  pp 122  LODDING K N   Iconic Interfacing  IEEE Computer Graphics and Applications  pp 20  Loxe T S  D Tan H S SEAH ET AL   Rendering Fireworks Displays  EEE Computer Graphics and Appli cations  pp 43  Loomis J  H Poizner U BELLUGI ET AL   Computer Graphic Modeling of American Sign Language  in pro ceedings of SIGGRAPH  Computer Graphics  pp 114  LORENSON W E AND H CLINE   Marching Cubes A High Resolution 3D Surface Construction Algorithm  in proceedings of SIGGRAPH  Camputer Graphics  pp 169  MACcKINLAy J D  S K CARD AND G G ROBERTSON   Rapid Controlled Movement Through a Virtual 3D Workspace  SIGGRAPH pp 176  MACKINLAY   D  G G ROBERTSON AND S K Carp   The Perspective Wall Detail and Context Smoothly In tegrated  CHI  pp 179  MAGNENAT THALMANN N AND D THALMANN  Com puter Animation Theory and Practice Springer Verlag Tokyo  MAGNENAT THALMANN N AND D THALMANN  Image Synthesis Springer Verlag Tokyo  MAGNENAT THALMANN N AND D THALMANN   Complex Models for Animating Synthetic Actors  IEEE Computer Graphics and Applications  pp 45  MANDELBROT B B   Fractals Form Chance and Di mension Freeman Press San Francisco  MANDELBROT B B   The Fractal Geometry of Nature Freeman Press New York  Mantyta M   An Introduction to Solid Modeling Computer Science Press Rockville MD  Max N L AND D M LERNER   A Two and a Half D Motion Blur Algorithm  in proceedings of SIGGRAPH  Computer Graphics  pp 94  Max N L   Atmospheric Illumination and Shad ows  in proceedings of SIGGRAPH  Computer Graphics  pp 124  Max N L   Cone Spheres  in proceedings of SIG GRAPH  Computer Graphics  pp 62  Metaxas D AND D TERZOPOULOS   Dynamic Defor mation of Solid Primitives with Constraints  in pro ceedings of SIGGRAPH  Computer Graphics  pp 312  Mever G W  H E RUSHMEIER M F COHEN ET AL   An Experimental Evaluation of Computer Graphics Imagery  ACM Transactions on Graphics  pp 50  Mever G W AND D P GREENBERG   Color Defective Vision and Computer Graphics Displays  IEEE Com puter Graphics and Applications  pp 40  Meyers D  S SKINNER AND K SLOAN   Surfaces from Contours  ACM Transactions on Graphics  pp 258  MILLER G  P   The Motion Dynamics of Snakes and Worms  in proceedings of SIGGRAPH  Com puter Graphics  pp 178 MILLER J V  D E BREEN W E LORENSON ET AL   Geometrically Deformed Models A Method for Ex tracting Closed Geometric Models from Volume Data  in proceedings of SIGGRAPH  Computer Graphics  pp 226  MITCHELL D P   Spectrally Optimal Sampling for Distribution Ray Tracing  in proceedings of SIC GRAPH  Computer Graphics  pp 165  MrTcHELL D P AND P HANRAHAN   Mlumination from Curved Reflectors  in proceedings of SIGGRAPH  Computer Graphics  pp 291  Mrvata K   A Method of Generating Stone Wall Patterns  in proceedings of SIGGRAPH Computer Graphics  pp 394  MoLnakr S  J EYLES AND J POULTON   PixelFlow High Speed Rendering Using Image Composition  in proceedings of SIGGRAPH  Computer Graphics  pp 240   Moon F C   Chaotic and Fractal Dynamics John Wiley  Sons New York  Moore M AND J WILHELMS   Collision Detection ad Response for Computer Animation  in proceed ings of SIGGRAPH  Computer Graphics  pp 298  MORTENSON M E   Geometric Modeling John Wiley  Sons New York  Murai     Volumetric Shape Description of Range Data Using the Blobby Model   in proceedings of SIG GRAPH  Computer Graphics  pp 235  Muscrave F K  C E KOLB AND R S MAcE   The Synthesis and Rendering of Eroded Fractal Terrains  in proceedings of SIGGRAPH  Computer Graphics  pp 50  Myers B A AND W BUXTON   Creating High Inter active and Graphical User Interfaces by Demonstra tion  in proceedings of SIGGRAPH  Computer Graphics  pp 258  NayLor B  J AMANATIDES AND W THIBAULT   Merg ing BSP Trees Yields Polyhedral Set Operations  in pro ceedings of SIGGRAPH  Computer Graphics  pp 124  Newman W H   A System for Interactive Graphi cal Programming  SJCC Thompson Books Washington D C  pp 54  Newman W H AND R F SPROULL  Principles of Inter active Computer Graphics McGraw Hill New York  No J T AND J Marks   Spacetime Constraints Re visited  in proceedings of SIGGRAPH  Computer Graphics pp 350  NICHOLL T M  D T LEE AND R A NICHOLL   An Efficient New Algorithm for 2D Line Clipping Its De velopment and Analysis  in proceedings of SIG GRAPH  Computer Graphics  pp 262  NIELSON G M  B SHRIVER AND L ROSENBLUM ED   Visualization in Scientific Computing  EEE Computer So ciety Press Los Alamitos CA  Nietson G M   Scattered Data Modeling  IEEE Computer Graphics and Applications  pp 70  Nishimura H   Object Modeling by Distribution Function and a Method of Image Generation  Journal Electronics Comm Conf  J68  pp 725  Nisa T AND E NaAKAMAE   Continuous Tone Representation of Three Dimensional Objects Ilumi nated by Sky Light  in proceedings of SIGGRAPH  Computer Graphics  pp 132  Niseaa T  T SIRAL K TADAMURA ET AL   Display of the Earth Taking into Account Atmospheric Scattering  in proceedings of SIGGRAPH  Computer Graphics Proceedings pp 182  Norton A   Generation and Display of Geometric Fractals in D  in proceedings of SIGGRAPH  Computer Graphics  pp 67  NSF INVITATIONAL WORKSHOP   Research Directions in Virtual Environments  Computer Graphics  pp 177  Oxase H  H IMAoKa T TOMIHA ET AL   Three Dimensional Apparel CAD System  in proceedings of SIGGRAPH  Computer Graphics  pp 110  OPENGL ARCHITECTURE REVIEW BOARD  OpenGL Pro gramming Guide Addision Wesley Reading MA  OPPENHEIMER P E   Real Time Design and Anima tion of Fractal Plants and Trees  in proceedings of SIG GRAPH  Computer Graphics  pp 64  OSF Motir  OSF Motif Style Guide Open Software Foundation Prentice Hall Englewood Cliffs NJ  PAINTER J AND K SLOAN   Antialiased Ray Tracing by Adaptive Progressive Refinement  in proceedings of SIGGRAPH  Computer Graphics  pp 288  Pano A T   Line Drawing Algorithms for Parallel Machines  IEEE Computer Graphics and Applications  pp 59  Pavirois T   Algorithms For Graphics and Image Pro cessing Computer Science Press Rockville MD  Pavurpis T   Curve Fitting with Conic Splines  ACM Transactions on Graphics  pp 31  Pacey D R   Modeling Waves and Surf  in pro ceedings of SIGGRAPH  Computer Graphics  pp 74  PerrGen H O AND P H RICHTER  The Beauty of Frac tals Springer Verlag Berlin  Perrrcen H   AND D SAuPe ED   The Science of Frac tal Images Springer Verlag Berlin  PENTLAND A AND J WILLIAMS   Good Vibrations Modal Dynamics for Graphics and Animation  in pro ceedings of SIGGRAPH  Computer Graphics  pp 222  Peruin K ann E M Horrerr   Hypertexture  in proceedings of SIGGRAPH  Computer Graphics  pp 262  Puiturs R L   A Query Language for a Network Data Base with Graphical Entities  in proceedings of SIGGRAPH  Computer Graphics  pp 185  PHONG B T   Iltumination for Computer Generated Images  CACM  pp 317  Pinepa J   A Parallel Algorithm for Polygon Ras terization  in proceedings of SIGGRAPH  Computer Graphics  pp 20 Prreway M L V AND D J WATKINSON   Bresen ham s Algorithm with Gray Scale  CACM 11  pp 626  Piatt J C AND A H Barr   Constraint Methods for Flexible Models  in proceedings of SIGGRAPH  Computer Graphics  pp 288  Porter T and T DuFF   Compositing Digital Im ages  in proceedings of SIGGRAPH  Computer Graphics  pp 259  Potmesil M AND CHAKRAVARTY   Synthetic Image Generation with a Lens and Aperture Camera Model ACM Transactions on Graphics  pp 108  PorMEsiL M AND IL CHAKRAVARTY   Modeling Mo tion Blur in Computer Generated Images  in proceed ings of SIGGRAPH  Computer Graphics  pp 399  PoTmesiL M AND E M HorFerT   FRAMES Soft ware Tools for Modeling Rendering and Animation of 3D Scenes  in proceedings of SIGGRAPH  Computer Graphics  pp 93  PoTMESIL M aND E M HOFFerT   The Pixel Ma chine A Parallel Image Computer  in proceedings of SIGGRAPH  Computer Graphics  pp 78  Pratt W K   Digital Image Processing John Wiley  Sons New York  PREPARATA F P AND M_ I SHAMOS  Computational Geometry Springer Verlag New York  Press W H  A TEUKOLSKY W T VETTERLING ET Al   Numerical Recipes in C Cambridge University Press Cambridge England  PRUSINKIEWICZ P  M S HAMMEL AND E MJOLSNESS   Animation of Plant Development  in proceedings of SIGGRAPH  Computer Graphics Proceedings pp 360 Pruyn P W AND D P GREENBERG   Exploring 3D Computer Graphics in Cockpit Avionics  IEEE Cam puter Graphics and Applications  pp 35  Quek L H anp D HEARN   Efficient Space Subdi vision Methods in Ray Tracing Algorithms  Univer sity of lllinois Department of Computer Science Report UIUCDCS R  RalBERT M H AND J K HODGINS   Animation of Dy namic Legged Locomotion  in proceedings of SIG GRAPH  Computer Graphics  pp 358  Reeves W T   Particle Systems A Technique for Modeling a Class of Fuzzy Objects  ACM Transactions on Graphics  pp 108  Reeves W T   Particle Systems A Technique for Modeling a Class of Fuzzy Objects  in proceedings of SIGGRAPH  Computer Graphics  pp 376  Reeves W T AND R BLau   Approximate and Prob abilistic Algorithms for Shading and Rendering Struc tured Particle Systems  in proceedings of SIGGRAPH  Computer Graphics  pp 321  Reeves W T  D H SALESIN AND R L  CooK   Ren dering Antialiased Shadows with Depth Maps  in pro ceedings of SIGGRAPH  Compuier Graphics  pp 291  REQUICHA A A G AND J R ROSSIGNAC   Solid Mod eling and Beyond  IEEE Computer Graphics and Applica tions  pp 44  Reynowps C W   Computer Animation with Scripts and Actors  in proceedings of SIGGRAPH  Com puter Graphics  pp 296  REYNOLDS C W   Flocks Herds and Schools A Distributed Behavioral Model  in proceedings of SIG GRAPH  Computer Graphics  pp 34  RIESENFELD R F   Homogeneous Coordinates and Projective Planes in Computer Graphics  IEEE Com puter Graphics and Applications  pp 55  ROBERTSON P K   Visualizing Color Gamuts A User Interface for the Effective Use of Perceptual Color Spaces in Data Displays  EEE Computer Graphics and Applications  pp 64  ROBERTSON G G  J D MACKINLAY AND S K Carb   Cone Trees Animated 3D Visualizations of Hierarchi cal Information  CHI  pp 194  Rocerrs D F AND R A EARNSHAW ED   Techniques for Computer Graphics Springer Verlag New York  Rocers D F AND J A ADAMS  Mathematical Elements for Computer Graphics McGraw Hill New York  ROSENTHAL D S H  ET AL   The Detailed Semantics of Graphics Input Devices  in proceedings of SIG GRAPH  Computer Graphics  pp 38  Rusine D   Specifying Gestures by Example  in proceedings of SIGGRAPH  Computer Graphics  pp 337 RUSHMEIER H AND K Torrance   The Zonal Method for Calculating Light Intensities in the Presence of a Participating Medium  in proceedings of SIG GRAPH  Computer Graphics  pp 302  RUSHMEIER H E AND K E TORRANCE   Extending the Radiosity Method to Include Specularly Reflecting and Translucent Materials  ACM Transactions on Graph ics  pp 27  SABELLA P   A Rendering Algorithm far Visualizing 3D Scalar Fields  in proceedings of SIGGRAPH  Computer Graphics  pp 58  Sasin M A   Contouring The State of the Art  in Fundamental Algorithms for Computer Graphics R A Earnshaw ed Springer Verlag Berlin pp 482  Sa rsin D AND R BARZEL   Adjustable Tools An Object Oriented Interaction Metaphor  ACM Transac tions on Graphics  pp 107  Samet H AND R E WesBeR   Sorting a Collection of Polygons using Quadtrees  ACM Transactions on Graph fes  pp 222 SAMET H AND M TAMMINEN   Bintrees CSG Trees and Time  in proceedings of SIGGRAPH  Computer Graphics  pp 130  SaMET H AND R E WEBBER   Hierarchical Data Structures and Algorithms for Computer Graphics Part  IEEE Computer Graphics and Applications  pp 75  Samer H AND R E WesBER   Hierarchical Data Structures and Algorithms for Computer Graphics Part  IEEE Computer Graphics and Applications  pp 68  SCHEIFLER R W AND J GETTYS   The X Window Sys tem  ACM Transactions on Graphics  pp 109 SCHOENEMAN C  J Dorsey B Smits ET AL   Global Wumination  in proceedings of SIGGRAPH  Com puter Graphics Proceedings pp 146  SCHRODER P AND P HANRAHAN   On the Form Fac tor Between Two Polygons  in proceedings of SIG GRAPH  Computer Graphics Proceedings pp 164  ScHwaRrtTz M W  W B Cowan AND J C Beatty   An Experimental Comparison of RGB YIQ LAB HSV and Opponent Color Models  ACM Transactions on Graphics  pp 158  SEDERBERG T W AND E GREENWOOD   A Physically Based Approached to D Shape Bending  in proceed ings of SIGGRAPH  Computer Graphics  pp 34  SEDERBERG T W  P GAO G WANG ET AL   2D Shape Blending An Intrinsic Solution to the Vertex Path Prob lem  in proceedings of SIGGRAPH  Computer Graphics Proceedings pp 18  SEGAL M   Using Tolerances to Guarantee Valid Polyhedral Modeling Results  in proceedings of SIG GRAPH  Computer Graphics  pp 114  SEGAL M  C Koropiin R VAN WIDENFELT ET AL   Fast Shadows and Lighting Effects Using Texture Map ping  in proceedings of SIGGRAPH  Computer Graphics  pp 252  SEQUIN C H AND E K Smyei   Parameterized Ray Tracing  in proceedings of SIGGRAPH  Computer Graphics  pp 314  SHERR S   Electronic Displays John Wiley  Sons New York  SHILLING A AND W STRASSER   EXACT Algorithm and Hardware Architecture for an Improved A Buffer  in proceedings of SIGGRAPH  Computer Graphics Proceedings pp 92  SHIRLEY P   A Ray Tracing Method for Illumination Calculation in Diffuse Specular Scenes  Graphics Inter face  pp 212  SHNEIDERMAN B   Designing the User interface Addi son Wesley Reading MA  SHOEMAKE K   Animating Rotation with Quater nion Curves  in proceedings of SIGGRAPH  Com puter Graphics  pp 254 SIBERT J L  W D HURLEY AND T W BLESER   An Ob ject Oriented User Interface Management System  in proceedings of SIGGRAPH  Computer Graphics  pp 268  SILLION F X AND C PuecH   A General Two Pass Method Integrating Specular and Diffuse Reflection  in proceedings of SIGGRAPH  Computer Graphics  pp 344  SILLION F X  J R ARVO S H WESTIN ET AL   A Global Ilumination Solution for General Reflectance Distributions  in proceedings of SIGGRAPH  Com puter Graphics  pp 196 Sms K   Particle Anmation and Rendering Using Data Parallel Computation  in proceedings of SIG GRAPH  Computer Graphics  pp 413  Sims K   Artificial Evolution for Computer Graph ics  in proceedings of SIGGRAPH  Computer Graph ics  pp 328  SINGH B  J C BEATTY K S BOOTH ET AL   A Graph ics Editor for Benesh Movement Notation  in proceed ings of SIGGRAPH  Computer Graphics  pp 62  Situ A R   Color Gamut Transform Pairs  Com puter Graphics  pp 19 Smirn A R   Tint Fill  Computer Graphics  pp 283  SmiTH A R   Plants Fractals and Formal Lan guages  in proceedings of SIGGRAPH  Computer Graphics  pp 10  SMITH R B   Experiences with the Altemate Reality Kat An Example of the Tension Between Literalism and Magic  IEEE Computer Graphics and Applications  pp 50  Smitn A R   Planar Pass Texture Mapping and Warping  in proceedings of SIGGRAPH  Computer Graphics  pp 272  Sits B E  J R ARVO AND D H SALESIN   An Im portance Driven Radiosity Algorithm  in proceedings of SIGGRAPH  Computer Graphics  pp 282  SNYDER J M anD J T Kajiva   Generative Model ing A Symbolic System for Geometric Modeling  in proceedings of SIGGRAPH  Computer Graphics  pp 378  SNYDER J M  A R Woopwury K FLEISCHER ET AL   Interval Method for Multi Point Collisions between Time Dependent Curved Surfaces  in proceedings of SIGGRAPH  Computer Graphics pp 334  SPROULL R F aND I E SUTHERLAND   A Clipping Di vider  AFIPS Falt Joint Computer Conference Stam J ANNE Fume   Turbulent Wind Fields for Gaseous Phenomena  in proceedings of SIGGRAPH  Computer Graphics Proceedings pp 376  STETINER A AND D P GREENBERG   Computer Graphics Visualization for Acoustic Simulation  in pro ceedings of SIGGRAPH  Computer Graphics  pp 206 STRASSMANN S   Hairy Brushes  in proceedings of SIGGRAPH  Computer Graphics  pp 232  Strauss P S AND R Carey   An Object Oriented 3D Graphics Toolkit  in proceedings of SIGGRAPH Computer Graphics  pp 349  Sune H C K  G RoGers aND W J Kusitz   A Crit ical Evaluation of PEX  IEEE Computer Graphics and Ap plications  pp 75  SUTHERLAND I E   Sketchpad A Man Machine Graphical Communication System  AFTPS Spring Joint Computer Conference pp 346  SUTHERLAND   E  R F SPROULL AND R SCHUMACKER   A Characterization of Ten Hidden Surface Al gorithms  ACM Computing Surveys  pp 55  SUTHERLAND I E AND G W HopoGMaN   Reentrant Polygon Clipping  CACM  pp 42 Swezey R W AND E G Davis   A Case Study of Human Factors Guidelines in Computer Graphics IEEE Computer Graphics and Applications  pp 30 TAKALA T AND J HAHN   Sound Rendering  in pro ceedings of SIGGRAPH  Computer Graphics  pp 220  TANNAS J  LAWRENCE E  ED   Flat Panel Displays ana CRTs Van Nostrand Reinhold Company New York  TELLER S AND P HANRAHAN   Global Visibility Al gorithms for umination Computations  in proceed ings of SIGGRAPH Computer Graphics Proceedings pp 246  TERZOPOULOS D  J PLatt A H BARR ET AL   Elasti cally Deformable Models  in proceedings of SIG GRAPH  Computer Graphics  pp 214  THALMANN D  Ep   Scientific Visualization and Graph ics Simulation John Wiley  Sons Chichester England  TuIBauLt W C AND B F NAYLOR   Set Operations on Polyhedra using Binary Space Partitioning Trees  in proceedings of SIGGRAPH  Computer Graphics  pp 162  TORBERG J G   A Parallel Processor Architecture for Graphics Arithmetic Operations  in proceedings of SIGGRAPH  Computer Graphics  pp 204  TORRANCE K E ann E M SPARROW   Theory for Off Specular Reflection from Roughened Surfaces   Optical Society of America  pp 1114  Travis D   Effective Color Displays Academic Press London  Ture E R   The Visual Display of Quantitative Infor mation Graphics Press Cheshire CN  Turre E R   Envisioning Information Graphics Press Cheshire CN  Turkowskl K   Antialiasing Through the Use ot Coordinate Transformations  ACM Transactions on Graphics  pp 234  Upson C AND M KEELER   VBUFFER Visible Vol ume Rendering  in proceedings of SIGGRAPH  Computer Graphics  pp 64  Upson C  T FAULHABER JR  D KAMINS ET AL   The Application Visualization System A Computational En vironment for Scientific Visualization  IEEE Computer Graphics and Applications  pp 42  Upstitt S   The RenderMan Companion Addison Wesley Reading MA  VAN DE PANNE M AND E FiuMe   Sensor Actuator Networks  in proceedings of SIGGRAPH  Computer Graphics Proceedings pp 342  VAN WIk J J   Spot Noise Texture Synthesis for Data Visualization  in proceedings of SIGGRAPH  Computer Graphics  pp 318  VEENSTRA J AND N AHUJA   Line Drawings of Octree Represented Objects  ACM Transactions on Graphics  pp 75  VELHO L AND J D M Gomes   Digital Halftoning with Space Filling Curves  in proceedings of SIG GRAPH  Computer Graphics  pp 90  Von HERZEN B  A H BARR AND H R Zatz   Geo metric Collisions for Time Dependent Parametric Sur faces  in proceedings of SIGGRAPH  Computer Graphics  pp 48  WaLLace V L   The Semantics of Graphic Input Devices  in proceedings of SIGGRAPH  Computer Graphics  pp 65  WALLACE J R  K A ELMQUIST AND E A HAINES   A Ray Tracing Algorithm for Progressive Radiosity  in proceedings of SIGGRAPH  Computer Graphics  pp 324  WaANGER L R  J A FERWERDA AND D P GREENBERG  Perceiving Spatial Relationships i Computer Gener ated Images  IEEE Compuler Graphics and Applications  pp 58  Wart C   Color Sequences for Univariate Maps Theory Experiments and Principles  IEEE Computer Graphics and Applications  pp 49  Warn D R   Lighting Controls for Synthetic Im ages  im proceedings of SIGGRAPH  Computer Graphics  pp 21 WARNOCK J AND D K Wyatt   A Device Indepen dent Graphics Imaging Model for Use with Raster De vices  in proceedings of SIGGRAPH  Computer Graphics  pp 319  Watt A   Fundamentals of Three Dimensional Com puter Graphics Addison Wesley Wokingham England  Wat M   Light Water Interaction Using Backward Beam Tracing  in proceedings of SIGGRAPH  Cont puter Graphics  pp 386  Watt A AND M Wart  Advanced Animation and Ren dering Techniques Addison Wesley Wokingham Eng land  WecHorst H  G Hoorer Ant D P GREENBERG  Improved Computational Methods for Ray Tracing  ACM Transactions on Graphics  pp 69  WEIL J   The Synthesis of Cloth Objects  in pro ceedings of SIGGRAPH  Computer Graphics  pp 54 WEILER K AND P ATHERTON   Hidden Surface Re moval Using Polygon Area Sorting  in proceedings of SIGGRAPH  Computer Graphics  pp 222  WEILER K   Polygon Comparison Using a Graph Representation  in proceedings of SIGGRAPH  Computer Graphics  pp 18  WESTIN S H  J R ARVO AND K E TORRANCE   Pre dict ng Reflectance Functions from Complex Surfaces  in proceedings of SIGGRAPH  Computer Graphics  pp 264  Westover L   Footprint Evaluation for Volume Rendering  in proceedings of SIGGRAPH  Com puter Graphics  pp 376  WHiTTED T   An Improved Ilumination Model for Shaded Display  CACM  pp 349  WHITTED T AND D M WEIMER   A Software Testbed for the Development of 3D Raster Graphics Systems  ACM Transactions on Graphics  pp 58  Wuirtep T   Antialiased Line Drawing Using Brush Extrusion  in proceedings of SIGGRAPH  Camputer Graphics  pp 156  WILHELMS J   Toward Automatic Motion Control  IEEE Computer Graphics and Applications  pp 22  WILHELMS J AND A V GELDER   A Coherent Projec tion Approach for Direct Volume Rendering  in pro ceedings of SIGGRAPH  Computer Graphics  pp 284  WILHELMS J AND A VAN GELDER   Octrees for Faster Isosurface Generation  ACM Transactions on Graphics  pp 227  WILLIAMS L   Performance Driven Facial Anima tion  in proceedings of SIGGRAPH  Computer Graphics  pp 242  WILLIAMS P L   Visibility Ordering Meshed Polyhe dra  ACM Transactions on Graphics  pp 126  Witkin A AND W Wetcn   Fast Animation and Control of Nonrigid Structures  in proceedings of SIG GRAPH  Computer Graphics  pp 252  WITKIN A AND M Kass   Reaction Diffusion Tex tures  in proceedings of SIGGRAPH  Computer Graphies  pp 308  WOLFRAM S   Mathematica Addison Wesley Read ing MA  Woo A  P POULIN AND A FOURNIER   A Survey of Shadow Algorithms  IEEE Computer Graphics and Ap plications  pp 32  Wricut W E   Parallelization of Bresenham s Line and Circle Algorithms  EEE Computer Graphics and Ap plications  pp 67  Wu X   An Efficient Antialiasing Technique  in proceedings of SIGGRAPH  Computer Graphics  pp 152  Wyszeckl G AND W S Stites  Color Science John Wiley  Sons New York  Ww G  B WYVILL AND C MCPHEETERS   Solid Texturing of Soft Objects  IEEE Computer Graphics and Applications 12  pp 26  YagGER L  C UPSON AND R Myers   Combining Physical and Visual Simulation Creation of the Planet Jupiter for the Film   in proceedings of SIG GRAPH  Computer Graphics  pp 94  YAGEL R  D COHEN AND A KAUFMAN   Discrete Ray Tracing  IEEE Computer Graphics and Applications  pp 28  YAMAGUCHS K  T L KUNn AND FUJIMURA   Octree Related Data Structures and Algorithms  IEEE Com puter Graphics and Applications  pp 59  Youn D A   The X Window System  Programming and Applications with Xt OSF Motif Ediiion Prentice Hail Englewood Cliffs NJ  ZELEZNICK R C  D B CONNER M M WLOKA ET AL   An Object Oriented Framework for the Integration of Interactive Animation Techniques  in proceedings of SIGGRAPH  Computer Graphics  pp 112  Zevtzsr D   Motor Control Techniques for Figure Animation  IEEE Computer Graphics and Applications  pp 60  ZHANG Y AND R E WEBBER   Space Diffusion An Improved Parallel Halftoning Technique Using Space Filling Curves  in proceedings of SIGGRAPH  Com puter Graphics Proceedings pp 312   Subject Index A Absolute coordinates A buffer algorithm 76 Acoustic digitizer 67 Active edge list  Active matrix LCD Adaptive sampling 40 Adaptive spatial subdivision BSP tree ray tracing 38 Additive color model  Affine transformation Aliasing Alignment text  Ambient light  see aise Mlumination models Ambient reflection coefficient American National Standards Institute ANSI  Angle direction vector  incidence phase refraction rotation specular reflechon Angstrom Animation accelerations 94 action spevifications applications  18 24 cels color table  direct motion specification 95 double buffering dynamics 96 frame by frame functions goal directed in betweens inverse dynamics mverse kinematics key frame key frame systein kinernatics  95 Kochanek Bartels splines 27 languages morphing  91 motion specification 96 object definitions parametrized system physically based modeling 95  taster methods 87 real time  scene description scripting system storyboard ANSI Amencan National Standards Institute  Antialiasing area boundaries 78 area sampling  539 filtering 75 lines 76 Nyquist sampling interval Pitteway Watkinson 78 pret phasing  pixel weighting masks  prefiltering postfiltering in ray tracing 43 stochastic sampling 43 supersampling 74 40 surface boundaries 43 in texture mapping 56 Application icon Applications ser Graphics applications Approximation spline Area clipping 44 Area filling  ser afso Fill area antialiasing 78 boundary fill algorithm 30 bundled attributes curved boundaries 30 flood fill algorithm functions hatch  nonzero winding number rule 26 odd even rule scan line algonthm 27 soft fill 63 tint fill unbundled attributes Area sampling  Aspect ratio Aspect source flag Area subdivision visibility algorithm 85 Artificial reality see Virtual reality Attentuation function Attribute area fill 03 bundled 69 brush 52 character 68 70 color 57 curve 54 grayscale individual inquiry functions intensity level  see alse Color Intensity levels line color 50 69 hne type 46 69 line width 49 69 marker 68  Parameter pen 52 structure 54 system list table text 68 70 unbundled Axis reflection rotation  20 shear Axis vector rotatian  15 Axis vectors basis  Axonometric projection  B Back face detection 72 Back plane clipping Background ambient light Bar chart 12 38 Barn doors light control  Baseline character  Base vector  ser also Basis Basis coordinate vectors normal orthogonal orthonormal Basis functions  see also Blending functions Basis matrix spline  Beam penetration CRT 43 see also Cathode ray tube Bernstein polynomials Beta parameter Beta spline 47 Bevel join B zier blending functions 28 Bspline conversions closed curve cubic curve  curves 33 design techniques 31 matrix properties 30 surfaces 34 Bias parameter spline  346 Binary space partitioning tree  see also BSP tree  Binding language  Bisection root finding BitBie bit block transfer  Bit map  see also Frame buffer Bitrap font 33 Blending functions B zier 28 Bspline Subject Index Blending hinctions cont  cardinal Hermite Block transfer Blobby object  Body character nonrigid rigid  Boolean operations area fill raster transformations Boundary conditions splme  19 Boundary fill algorithms  connected region 4xonnected region 30 Boundary representation Bounding box rectangle  volume Box covering Box filter 75 Box dimension B rep boundary representation  Bresenham s algorithm circle line 92 Brightness light  Brownian mation Brush and pen attributes 52 BSP ray tracing tree visibility algorithm 82 B spline B zier conversions blending functions Cox deBoor recursion formulas cubic 41 curves 44 knot vector local control  matnx nonuniform  44 nanuniform rational NURB  open  44 periodic 41 properties 36 quadratic 39 44 rational surfaces 45 tension parameter uniform 44 Buffer  ser also Frame buffer Bump function  see aiso Frame mapping Bump mapping 59 Bundled attributes  Bundle table Business visualization  ser also Data visualizahon Butt line cap Button box  Cc Cabinet projection  CAD 11 Calligraphic vector display Camera viewing 36 Camera lens effects Capline character  Cardinal spline 25  Cardiowd 40 Cartesian coordinates 601 Cathode ray tube 40 ser also Video monitors  delta delta shadow mask electrostatic beam deflection 39 focusing high definition inline shadow maak magnetic beam deflection  persistence phosphor 39 refresh rate 41 resohution 40 RGB shadow mask 44 Catmull Rom spline Cavalier projection Cell array  Center of projection Central structure store CSS  CGI Computer Graphics Interface  CGM Computer Graphics Metafile  Character  attributes 68 haseline  body  bottom line capline  color  descender  fonts  functions 168 generation 34 grid 56 33 height  italic  kern  outline fonts 56 133 text precision 167 topline  typeface 33  up vector  width 65 Characteristic polygon Chart  bar 12 38 pie 12 40 time  line  37 Choice input device  Chromaticity  diagram 71 values CIE International Commission on I umination   Circle equation Cartesian nonparametric  parametric  polar  Circle generating algorithms 102 Bresenham midpoint 102 mid point function midpoint decision parameters  Circle symumetry  Cohen Sutherland line algorithm 30 curves  yrus Beck line algorithm exterior  hardware implementation 64 in homageneous coordinates 63 Liang Barsky line algorithm 32 Liang Barsky polygon algorithm Nichol Lee Nichal line algorithm 35 nonrectangular window in normalized coordinates  61 parallel methods parametric 32 planes 50 63 points polygons 43 region codes  straight line segments 37  61 Sutherland Hodgman polygon algorithm three dimensional 53 two dimensional 45 view volumes 50 63 Weiler Atherton polygon algorithm 43 window in world coordinates CMY color model 75 Codes ray tracing  Coefficient ambient reflection diffuse reflectian matrix specular reflechon transparency Cohen Sutherland line clipping algorithm 30 Coherence 24 Color chromatiaty chromaticity diagram 71 chromaticity values coding  complementary  cube 73 ser also Color models dominant frequency dominant wavelength  70 fill 63 gamut  71 hue  illuminat C in illumination models intuitive concepts 72 lightness HLS parameter  line 52 69 lookup table 56 marker  matching functions model  monitor 45 see aise video monitor nonspectral perception 67 primaries pure  purity purple line RGB 57 saturation  selection considerations 81 shades  spectrum electromagnetic  standard CIE primanes 69 table 56 text  tints 577 tones  tristimulus vision theory value HSV parameter  Color model  additive  CMY 75 HLS 80 HSB see HSV model HSV 77 HSV RGB conversion 79 RGB 73 RGB CMY conversion XYZ YIQ Color table animation 87 Column vector Command icon Commission Internationale de I Eclairage CIE  Complementary colors  Complex number  absolute value  conjugate  Euler s formula  imaginary part  length modulus  modulus  ordered pair representation  polar representation 17 pure imaginary  real part  roots Complex plane Composite monitor 45 Composition matrix  Computed tomography CT  Computer aided design CAD  11 Computer aided surgery Computer art 18 Computer Graphics interface CGD Computer Graphics Metafile CGM  Concatenation matrix  13 Concave polygon splitting 37 Cone filter  Cone receptors  Cone tracing  see aiso Ray tracing Conic curves 12 49 Conjugate complex  Constant intensity shading 23 Constraints 89 Constructive solid geometry CSG  mass calculations  octree methods 62 ray casting methods 59 volume calculations 59 Continuity conditions spline  geometric 19 parametric 18 Continuity parameter Continuous tone images  see also Halftone  Contour intensity border  Contour plots  applications   surface lines 90 three dimensional isosurfaces  two dimensional isolines  97 Contraction tensor  Control graph Control icon Control operations Centro point spline  Contra polygon Contro surface terrain  77 Convex hull 3lo Coordinate axis rotations 13 Coordinate ams vectors basis  Coordinate extents Coordinate point  Coordinates absolute current positon homogeneous relative screen Coordinate syste Cartesian 601 curvilinear cylindrical device left handed  local  master  modeling  29 normalized device normalized projection orthogonal  polar  right handed screen 76 spherical three dimensional transformation of  20 29 two dimensional 602 won  viewing  20 36 world  Copy function Cox deBoor recursion formulas Cramer s rule Cross hatch fil  Cross product vector  CRT 40 see also Cathode ray tube CSG  see also Constructive solid geometry CT Computed Tomography scan Cubic spline  beta M6 Bezier 33 B spline  interpolation 27 Current event record Current position Curve attnibutes 54 beta spline  B czier spline B spline 35 cardinal spline 24 cardioid 40 Catmull Rom spline circle  comic s ction 12 49 ellipse fractal 66 see also Fractal curves generalized function Hermite spline hyperbola  Koch fractal  Kochanek Bartels spline limagon 40 natural sphire  Subject Index Overhauser spline parabola parallel algorithms 13 Parametric representations  piecewise construction 16 polynomial spiral 40 spline  20 see also Spline curve superquadyic 13 symumetry considerations 98  Curved surface ellipsoid Parametric representations 20 quadric 12 rendenng see Surface rendering sphere spline  see also Spline surface superquadric 13 torus 12 visibility 990  see also Visible surface detection Curvilnear coordinates Cutaway views  Cylindrical coordinates Cyrus Beck line clipping algorithm  D Damping constant Dashed line 46 Data glove  93 see also Virtual reality Data tablet  see ciso Digauzer Data visualization  applicatrons 31 contour plots 97 field lines  glyphs  isolines 97 isosurfaces  taultivanate fields  pseudo colur methods  scalar fields 99 streamlines  tensor fields  vector fields 401 volume rendering DDA line algorithm 88 Deflection coils  see also Cathode ray tube Delta delta shadow mask CRT Density functian blobby object  Depth butter algonthm 75 Depth cueing 300 Depth sorhng algorithm 81 Descender character  Detectability filter 85 Determinant 14 Device codes 82 Device coordinates Differental scaling Diffuse reflection 500 Diffuse retraction Digitizer m4 accuracy  acoust  67 apphcatons 15 electromagnetic 66 locator device  resolution  sonic  stroke device  three dimensional  valuator device  Subyec t Index Dimenstn Finlidean fractal 3h4  fractional Direvied ime segment vector  Direchon angles  Pirechon cosines Direct view storage tube DVST  Display coprocessor controller devices 52 ner processors file  list 84 processor 5A Sn provessany unl Sn program Sn Pastancr paint to line 80 ray tracing path Phstribured light soure Distributed ray trasieg 43 Distribution rav tracing Ditheang dot dimnision method error diffusion method 22 matrin noise 20 ordered dither method random Dominant frequency Dormnant wavelength  76 Dot product Dot matrix printer Dot ciffasion algorithm Double buttering Dragging Drawing methods  DVS1  Dynamtns 96 ser afso Animation  aise Video monitors Display i Ldge bst stahlet 22  77 Ede vector  conaected region Elashe material nonrigid object  Flectomagnetic spectrum hlectror beam  see also Cathode ray tube convergence electrostatic deflectun  focusing intensity magnete deflection  spat size 40 Flectron yun  sev ae C athode ray tube Electrostatic printer Elevtrothermal printer Hlement structure  Element pointer  Hlevacion view Hhpse Carlesian equation focus pomt oiidpeint algorithm 10 Parametric representation properties symmetry EMapsord  Emussive displays emitter  Energy cloth modeling function  Energy distnbution light s urce  Energy propagation radios Environment array Environment mapping Error diffusion algorithm 22 Euler s formula  see alse Complex numbers Even odd polygon filling rule Event  input mode   queue Explicat representation  Exploded view  Exterior chipping  F False pos tion root hnding  Far plane chpping  Fast Phong shadimg 77 Feedback 76 Field hhnes Fill algorithms see Area filling area  attributes 63 see alse Area filling color hatch  patterns 62 soft 63 styles tint Filter box  cone 175 function Gaussian 75 structure 54 65 workstation pick detectabilic  85 Fixed posrhon scaling  193 Flaps light control  Flat pane display emissive gascuischarge light eruthing diode FIZ  47 liquid crystal LCD  nonemissive passive matny plasma 46 thin film electrolunuinescent Flat shading Flight simulators 24 Flood fill algorithm Flood gun Focus point ellipse  Font  see alse Typeface bitmap 33 cache outline 133 proportionally spaced Force constant Form factors radiosity  4e Forward differences  connected region 24 Fractal affine constructions 78 box covering methods Brownian motion 78 characteristics 63 classification dimension  67 generation procedures 64 generalor  geometne constructions 71 geametry ininator invariant set random mcpoint displacement methods self inverse self inversion methods 87 self similar seif similarity  Z self squaring self squaring methods  similarity dimension subdivision methods 78 topological covering methods 66 Fractal curve Brownian rrotion dimension fractional Brownian motion 75 geometric constriclians 68 invariant 87 inversion constrution methods 87 Julia set Koch Mandelbrot set boundary 84 midpoint displacement 75 Peano self affine  self inverse 87 self similar 71 self squanng  snowflake 68 Fractai sohd Fractal surface Brownian dimension four dimensiona  85 geometric constructions 71 midpoint dispiacement 78 self similar 71 self squaring 85 surface rendering terrain 78 Fractional Brownian motion 78 Frachonal dimension Frame animation  Frame buffer  bit block transfers copy function loading intensity values 95 lookup table 56 raster transformations 11 read function resalution write function Frame mapping  Fresne reflection laws  Frequency spectrum electromagnetic  Front plane ichpping  Full color system  Frustum Functions 78 sec also Function Index  Gamma correction 15 Gamut color  71 Gas discharge displays Gaussian bump  Gaussian density function 15 Gaussian elinunahon  Gaussian filter 75 Gauss Seidel method Generator fractal  Geometric continuity spline  19 Geometric models CGeometric object properties 17 Geometric production rules 89 Geometric table Geometric transformations  GKS Graphical Kernel System  GL Graphics Library      Global lighting effects 527 Glyph Goal directed motion Gouraud shading model 25 Graftal Graphical user interface applications backup and error handlirg 75 components 76 feedback 76 help facilities icons  interactive techmques 93 menus 273 model user dialogue 73 user s model windows  Graphics applications advertising  18 agriculture  animations  18 24 architecture  art 18 astronomy business 13 18  CAD 11 cartugraphy education 24 engineering entertainment 21 facility planning  flight simulators 24 geology graphs and charts 13 image processing 33 manufacturing mathematics 17 27 medicine  modeling and simulations  25 31 physical sciences 31 publishing scientific visualization 31 simulations 10 31 simulators 25 training 24 user interfaces virtual reality  67 Graphics controller  Graphics functions 78 see also Function Index Graphics monitors 52 see also Video monitors Graphics software packages hasic functions 78 GKS GL     PHIGS PHIGS  standards 79 three dimensionai Graph plotting 39 ser alsu Charts Graphics tablet  15 67 see aiso Diginzer  Gravitational acceleration Gravity field Grayscale Grids character  33 in interactive constructions 90 H Halftone approximations 19 color methods dithering 22 patterns Halfway vector Hard copy devices 75 Hatch fill  Hausdorff Besicovitch dimension Head mounted display  see also Virtual reali Hemicube radiosity  49 Hermite spline 23 Hexcone HSV  Hidden line elimination Hidden surface elimination  seealso Visible surface detection Hierarchical modeling 68 High definition video monitor Highlighting as depth cueing technique 300 primihves specular reflections  504 structures 54 HLS color model 80 Homogeneous coordinates Hooke s law Horizontai retrace Horner s polynomial factoring method HSB color model see HSV madel HSV color model 77 Hue  Hyperbola    Icon  Ideal reflector  IHuminant C  Ulumination model ambient light attentuation funcuon basic components 511 color considerations combined diffuse specular diffuse reflection 500 flaps ideal reflector intensity attentuation light sources  multiple light sources opacity factor Phong refraction 10 shadows Snell s law specular reflection 504 spotlights lransmission vector transparency 11 Warn  Image order scanning  Subject Index Image processing 33 Image scanners  Image space methods visibility detection  Imaginary number Impact printer Implicit representation In betweens Index of refraction Initiator fractal  Ink jei printer 73 Inner product vector  In line shadow mask CRT Input devices button box  choice  data glove  93 dials  digitizer 67 80 graphics tablet initializing 88 joystick 64 80 keyboard  80 light pen  locator  logical classification mouse  80 pick  80 scanner  spaceball string  stroke  switches  three dimensional sonic digitizers touch panel 70 trackball valuator  78 voice systems 71 Input functions  87 Input modes concurrent use event  87 request  85 sample  Input priority Inquiry functions Inside outside test polygon odd even rule polygon nonzero winding number rule 26 spatial plane surface Inside polygon face Instance  see also Modeling Integral equation solving rectangle approximations Simpson s rule  trapezord rule  Monte Carlo  eid 24 Intensity attentuation depth cueing 300 interpolation shading Gouraud  modeling 97 see also Illumination models radiosity model 51 Intensity level adjusting see Antaliasing assigning 13 color lookup tables 56 contours borders  frame buffer storage gamma correction 15 ratio RGB  video lookup table 513 Subject Index Interactive picture construction techniques 92 Interlacing scan lines  International Comision on Mlumination C1E   Interpolation spline  Inverse geometric transformations   22 Inverse dynamics  Inverse kinematics  Inverse matrix  Inverse quatermon  Inverse scanning  1SO International Standards Organization  Isohines 97 Isometric joystick  isometric projection  Isosurfaces  J Jaggnes  sec also Antahasing Antialiasing Jittering Joystick as locator device movable 64 as pick device Pressure sensitive isometric  as stroke device as valuator device Julia set  K Ker Keyboard  as choice device  as locator device  as pick device  as string device  as valuator device Key frame Key frame system Kinematics  96 see also Animation Knot vector Kochanek Rartels spline 27 Koch curve  L Lambertian reflector Lambert s cosine law Language binding Laser printer LCD liquid crystal display  48 Least squares data fitting LED hghi emitting diode  47 Left handed coordinates  Legible typeface Length complex number  vector L grammar Liang Barsky clipping  polygons  two dimensional lines 32 Light ambient  angle of madence  chromaticity  chromaticity diagram 71 diffuse reflection 500 diffuse refraction frequency band hue ideal reflector index of refraction illuminant C itumination model  see also humination models intensity level assignment 13 Lambert s cosine law Phong specular model properties 68 purity reflection coefficients 502 refraction angle saturation  579 spectrum specular reflection 504 specular refraction speed transparency coefficient wavelength white  Light buffer ray tracing  Light emitting diode LED  47 Lighting model  ser also IHumination model Lightness HLS parameter  Light pen  Light source brightness distributed dominant frequency dominant wavelength energy distribution frequency distribution Juminance multiple point Limagaon  Line bundled attributes 69 chart  37 clipping  see aise Line clipping color 50 contour   97 dashed 46 function 96 parametric representation  pen and brush options  sampling  RB A4 slope intercept equation type 46 width 49 Linear congruential generator Linear equation solving Cramer s rule Gaussian elimination Gauss Seidel Line caps Line clipping Cohen Sutherland 30 Cyrus Beck Liang Barsky  Nich3l Lee Nichol 35 nonrectangular clip window parallel methods parametric 32 three dimensional Line drawing algorithms 95 antialiasing 76 Bresenham 92 DDA 88 frame buffer loading 95 parallel 94 Liquid crystal display LCD  48 Local coordinates  Loxal control spline   Local transformation matrix Locator input device  Logical input device Look at point Lookup table 56 Luminance  M Mach band Mandelbrot set 84 Marching cubes algorithm see lsosurfaces Marker 34 Marker attributes 68 Mask  see also Pixel mask Mass calaulations CSG  Master coordinates  Matrix  addition basis spline  B zier B spline cardinal coefficient column  concatenation  13 determinant dither Hermite identity inverse multiplication 13 nonsingular reflection  vow rotation   12 20 scalar multiplication scaling   shear  singular 6L4 spline characterization square translation  transpose Medical applications 33 Menu  Mesh polygon  10 Metaball model Metafile Metric tensor 11 Midpoint circle algorithm 102 Midpoint displaceiment fractal generation 78 Midpoint etlipse algorithm 10 Miter join 49 Mode input device  Model Modelinig  ser also Graphics applications Object representations Ilumunation models basic concepts 64 coordinates  29 display procedures  geometni  hierarchical 63 inslance  local coordinates Master coordinates modules packages 64 physically based 95  representations 62 structure hierarchies 68 symbol symbo hierarchies 63 transformations  68 29 Modules Modulus complex  Monte Carlo methods 24 Monitor 52 see also Video monitor Monitor response curve Morphing  91 Motion blur  43 Motion specification 96 Mouse 63 as choice device as locator device as pick device as stroke device Multivariate data visualization  N National Television System Comunittee NTSC   Natural spline Near plane clipping  Newton Raphson root finding 22 Newton s second law of motion Nicholl Lee Nicholl line clipping 35 Noise dither  20 Nonemissive displays Nonemitter Nonlinear equation salving bisection false position Newton Raphson 22 Nonparametric representations 19 Nonrigid object Nonsingular matrix Nonspectra color Nonuniform B splines  344 Nonuniform differentia  scaling Nonuniform rational B spline NURB  Nonzero winding number rule 26 Normal has s Normalized device coordinates Normalized projection coordinates Normalized view volumes  see aiso Clipping Normal vector average polygon mesh  curved surface interpolation Phong shading  plane surface view plane 36 NTSC National Television System Committee   Numerical methods bisection method Cramer s rule false position method Gaussian elimination Gauss Seidel method integral evaluations 24 least squares data fitting linear equations 21 Monte Carlo methods 24 Newton Raphson method 22 nonlinear equations 22 root finding 22 Simpsaon s rule  trapezoid rule NURB Nonuniform rational B spline  Nyquist sampling interval  oO Object nonngid flexible  as picture component  nigid  97 Object geometry 17 Object representahon beta splines 47 B zier splines 34 boundary B rep  B splines 45 BSP trees blobby surfaces 15 CSG methods 59 cubic spline interpolation 27 data visualization 403 explicit fractal curves and surfaces 87 implicit nonparametnic 19 octrees 62 parametric 20 particle systems 92 physically based modeling 95 polygon 10 quadric surfaces 12 rational splines 49 shape grammars 89 space partitioning methods superquadnics 14 sweep constructions 56 Object space methods visibility detection  Oblique projection  43 50 53 Octree  CSG operatians 62 generation 41 visibility detection  87 volume element voxel Odd even polygon filling rule One point perspective projection Opacity factor Order spline curve continuity  19 Ordered dither Orthogonal basis  Orthogonal coordinates Orthographic projections   Orthonormal basis Outline fant  Output primitives cell array  circie 102 character 34 conic section 12 ellipse 10 fill area 30 marker 34 point 86 polynomial spline straight line segment  94 text 33 Outside polygon face Overhauser spline  Subject tndex P Paintbrush programs 16 92 Painter s algorithm depth sorting  Panning Paratola Parallel algorithms area filling 21 curve drawing 13 line drawing 94 Paral e projection 99 axonometric cabinet cavalier elevation view isometric 41 obl ique  43 50 53 orthographic  48 plan view principal axes shear transformation  view volume 50 Parametric continuity spline  18 Parametric representations 20 circle  curve   ellipse ellipsoid 12 sphere  spline  straight line  surface 20 torus 12 Parametrized system Parity odd even rule Particle systerns 92 Path text  Passive matrix LCD Pattern fll 61 index reference point 60 Tepresentation  size tiling Pattern mapping Pattern recognition Peano curve Pel Pen ard brush attributes  Penumbra shadow Perfect reflector Persistence Perspective projection  frustum one point principal vanishing point reference point shear transtormation 56 three point two point vanishing point view volume 49 PET Position emission tomography 33 Phase angle PHIGS  sve alsa Function index attributes   59 70 input 87 modeling 69 output primitives 96   structures 60 three dimensional transformations 26 three dimensional viewing 66 Subyect Index PHIGS cont two dimensional transformations  two dimensional viewing 23 workstation PHIGS  Phong specuiar reflection model Phong shading 27 Phospher 39 Photorealism Physically based modeling 95  Pick distance 80 filter 85 idenhtier input device  80 window  Pickability structure  Picking Picture element pixel  Piecewise approximation spline  16 Pie chart 12 40 Pitteway Watkins antialiasing 78 Pivot point PixBi Pixel addressing 16 end mask  51  patterns halftone  phasing ray 29 weighting mask  Pixel order scanning 55 Pixmap Plane clipping 61 coefficients complex equations far clipping  inside outside taces  near clipping  normal vector Pian view Plasma pancl display 46 Plotters  ser alsn Ponters heltbed color  drum flatbed  ink jel 73 laser  pen  rollfeed 75 Pount chpping control spline  coordinate  plotting 86 sampling as unit of character size Point light source Polar coordinates  Polar form complex number  17 Polygon active edge list  characteristic control edge vector fill 27 ee also Area filling inside lace inside outside tests  see also Plane mesh  10  normal vector outside face plane equation rendering shading  27 ray intersection 44 sorted edge table  splitting 37 surface surface detail tables 22  77 Polygon clipping paralle methods parametric methods Sutherland Hodgeman 42 three dimensional  Weiler Atherton 43 Polyline 96 Polyline connections  Polynomial curve Position emussion tomography PET  33 Positioning methods  Posthitering  see alsu Antialiasing Posting structures  Precision text  67 Prefiltering  see also Anttaliasing Presentation graphics 13 Pressure sensitive jovstich  Primary colors  Primitives  see aiso Output primit ves Principal axes Principal vanishing peint Printers dot matrix electrothermal impact laser  nonimpact electrostatic  ink jet 73 Priority structure view transformation input Procedural object representation 92 Procedural texture mapping 57 Production mules 84 Progressive refinement radiosity  50 Projecting square line cap Projection axonometric cabinet cavalier center of frustum isometric 41 oblique  43 50 53 orthographic  48 parallel  43 54 Perspective  47 56 plane reference point vector  53 view volume window Pseudo color methods  Pure color  Purity ight  Purple line  Q Quadric curves Quadric surfaces 12  Quadnilateral mesh 19 Quadtree Quaternion addition an fractal constructions 85 inverse magnitude multipheahon ordered pawr rearesentation  rotations 20 scalar multpheation scalar part  vector part  R Radiant energy Radiance  Radhosity model 51 energy transport equation form factors hemicube 49 luminance  progressive refinement 50 reflectivity factor surface enclosure Random dither noise  21 Random msdpoint displacement methods 78 Random scan monitor 42 color refresh display file Randomescan system display file  graphics controller rocessing unit Random walk Raster animation 87 Raster ops Raster scan monitor 41 bilevel bitmap color 45 frame buffer horizontal retrace interlacing pixel pixmap refresh butter vertical retrace Raster scan systera cell encoding display processor run length encoding scan conversion video controller 55 Raster trans ormations 11 Ratonal spline 49 Ray casting constructive solid geometry 59 visible surface detection 88 Ray tracing adaptive sarphng 40 adaptive subdivision 38 antialiasing 43 area sampling basic algorithm 31 bundles camera lens effects  cell traversal 37 codes cone tracing  distributed 43 eve ray see pixel ray equation  motion blur  43 pixel primary ray 29 polygon intersection 34 in radiosity model reflection ray  31 refraction ray  secondary ray shadow ray 30 space subdivision 38 sphere intersection 33 stochastic sampling supersampling 40 tree uniform subdivision Read function Readable typeface Real time animation  Reference point viewing   Reflection angle of inadence axis coefficients 502 diffuse 500 Fresnel laws halfway vector Lambertian mapping plane ray specular 504 vector  Reflection transformation  Reflectivity Reflectivity factor radiosity  Refraction angle S09 diffuse index ray  Snell s law S09 specular transmission vector  31 transparency coefficient vector  31 Refresh buffer  see aiso Frame buffer Refresh CRT 45 see also Cathode ray tube Refresh display file Refresh rate CRT  41 Region codes clipping  three dimensional two dimensional Relative coordinates Rendering see Surface rendering Requesl input mode  85 Resolution display device 40 halftone approximations Retrace electron beam  REYES RGB chromaticity coordinates RGB color mode  73 RGB monntor  see alse Video manitor Right hand coordinate system Right hand rule Rigid body transformation  97 Rigid motion Roots nonlinear equalions  complex numbers  Rotation angie axis  20 axis vector  15 composition inverse  matnx representation  93 12  19 pivat point quaternion 20 taster methods  three dimensional 20 two dimensional 87  93 X axis 12 y axis axis 11 Rotationa polygon splitting method Round join 149 Round line cap Row vector Rubber band methods  Run length encoding  S Sample input mode  Sampling adaptive 40 area  line  89 Nyquist interval point supersampling 74 40 weighted Sans serif typeface Saturation light  Scalar data field visualization 99 Scalar input methods 78 Scalar product of two vectors Scaling in arbitrary directions 94 composition curved objects differential factors  fixed point  mverse  22 matnix representation  nonuniform differential  parameters factors  raster methods  three dimensicnal 22 two dimensioral 88  94 uniform 88 Sean conversion areas 30 characters 33 circles 102 curved boundary areas 30 curved lines 13 ellipses 10 patterned Ail  points  polygons 27 straight lines 94 see also Line drawing algorithms structure list traversal Scan line Scan line interlaang Scan line algorithms area fillimg 27 63 visible surface detection 78 Subject Index Scanner  Scanning image order inverse pixel order 55 texture S54 Scientific visualization  see aiso Data visualization Screen coordinates   see also Coordinate system device Scripting system animation  Secondary ray Segment  Self affine fractals  78 Self inverse fractals  87 Self similar fractals  71 Self squaring fractals  85 Serif typeface Shades color  Shading algorithm  see Surface rendering Shading model  ser also Illumination model Shadow mask Shadow ray 30 Shadows modeling 30 penumbra umbra Shape grammars 90 Shear axis matrix in projection mapping  56 three dimensional two dimensional x direction y direction z direction Shift vector  see also Translation Similarity dimension Simpson s rule Simulations 10 31 sre aiso Graphics applications Simulators 25 Simultaneous linear equation solving 21 Singular matrix Sketching 16 92 Snell s law Snowflake fractal  68 Soft fill 63 Software standards 79 Solid angle 45 Solid modeling  see also Surtace Curved surtace applications 9 constructive solid geometry 59 sweep constructions 56 Solid texture Sonic digitizer Sorted edge table Spaceball SpaceGraph system Space sartitioning methods ray tracing  adaptive 38 light buffer ray bundles uniform Space partitioning representations Specular reflection  504 angle coefficient Fresnel laws halfway vector Parameter Subject Index Specular reflection cont   curve  16 approximation basis functions  bias parameter  B spline 44 characteristic polygon confinuity conditions 19 continuity parameter controt graph control points conversions 50 convex hull cubic interpolation 27 displaying 55 Hermite 23 interpolation knot vector  Kochanek Banels 27 lecal control  matrix representation natural NURB Overhauser rational 49 tension parameter   Spline generation Horner s method forward difference method 53 subdivision methods 55 Spline surface B zier 34 B spline 45 Splitting concave polygons rotational method vector method Spotlights Spring constant Spring network nonrigid body  Square mairix Stairstep effect Steradian 45 Stereoscopic glasses  headsets views  52  301 virtual reality applications  52 Stochastic Sampling Storyboard Streamlines String mput device  String precision text  Stroke input device  Stroke precision text 67 Stroke writing display  see alsa Video monitors  random scan  Structure  attributes 54 basic functions 54 central structure store C55   concepts 52 copying  creation 52 delection  displaying 3g Posting editing  element ms element pointer filters  85 hierarchy 68 highlighting filter  lists  metafile  pickability  posting  Subdivision methods adaptive ray tracing 38 BSP tree  spline generation 55 unifonm ray tracing Subtractive color model CMY  75 Superquadric 14 Supersampling 74 40 Surface blobby 15 curved  ser also Curved surfaces fractal  B5 parametric representation 20 plane quadric 12 spline  see also Spline surface superquadric 14 weighting Surface detail 60 bump mapping 59 environment mapping frame mapping 60 image order scanning inverse scanning pattern mapping pixel order scanning polygon mesh 54 procedural texturing 57 solid texture mapping texture mapping 56 texture scanning S54 Surface enclosure radiosity  Surface normal vector  Surface rendering 98 antialiasing 43 bump mapping 59 constant intensity shading 23 environment mapping fast Phong shading 27 flat shading frame mapping  CGouraud shading  25 intensity interpolation Mach bands normal vector interpolation Phong shading 27 polygon methods  27 polygon surface detail  procedural texturing 57 radiosity 50 ray tracing 43 texture mapping 56 Surface shading ser Surface rendering Sutherland Hodgeman potygon clipping 42 Sweep representations 56 Symbol  hierarchies 63 instance  in modeling 64 circle 98 in curve drawing algorithms 98  ellipse  T Table poly gon attribute edge 22  77 geometric sorted edge table vertex Tablet 67 see also Digitizer Task planning Tension parameter spline   Tensor contraction data field visualization metric  Terrain fractal  78 Tesselated surface Text  see also Character alignment attributes 67 7U clipping  generation 33 path Precision 67 Texture  see also Surface rendering mapping 56 procedural methods 57 scanning solid space  57 Thin film electroluminescent display Three point perspective projection Tiling  Time chart  Tint color  Tint fill Tone color  Topline character  Topological covering 66 Touch panel 70 Trackball Transformation affine basic geometric 200 22 commutative 95 composite 200 25 computational efficiency 97 coordinate system  29 functions  26 geometric  instance 63 local 68 matrix representations 90 modeling  68 29 noncommutative T94 parallel projection 99 Perspective projection  taster methods 11 Teflection  rotation 87 93 20 scaling 88  94 22 shear  three dimersional geometric 22 three dimensional viewing 56 translation 85  two dimensional geometnc 205 two dimensional viewing 22 viewing  22 56 window to viewport  22 workstation 22 world to viewing coordinate 20 38 Translation composition curved object distances  inverse  matrix representation  raster methods three dimensional two dimensional 85  vector  Transmission vector refraction  31 Transparency see also Refraction Ray tracing coefficient modeling 11 opacity factor vector  31 Transpose matrix  Trapezoid rule Traversal state list Triangle strip Tristimulus vision theory True color system Twist angle Two point perspective projection Typeface 33 see also Font legible readable sans serif serif  U Umbra shadow Unbundled attributes Uniform B splines 44 Uniform scahng 88 Uniform spatial subdivision octree 62 ray tracing Unit cube clipping  Up vector character  User dialogue 73 User help facilities User interface  76 93 see also Graphical user interface User model uvn coordinate system 38 uv plane  Vv Valuator input device  78 Value HSV parameter  Vanishing point Varifucal mirror Vector  12 addition  basis column components cross product data field visualization 401 direction angles direction cosines dot inner product knot magnitude length  polygon edge product propection  53 in quaternion representation  reflection  rotation 15 row scalar multiplicanon scalar dot product space  specular reflection 504 surface norma   transmission refraction  31 translauon  Vector method polygon splitting  Vector monitor Vertex table Vertical retrace Video controller 35 Video lookup tabie  Video monitor  see also Cathode ray tube calligraphic color CRT 45 composite 45 direct view storage rube DVST  emissive flat panel full color gas discharge LCD liquid crystal device  48 LED light emitting diode  47 nonemissive plasma panel random scan 42 raster scan 41 tefresh CRT 45 tesolution 40 RGB stereoscopic 52 thin film electroluminescent three dimensional true colur vector View look at point reference point  up vector  twist angle Viewing stereoscopic  52  301 three dimensiona  two dimensional 45 Viewing coordinates left handed three dimensional 34 two dimensional  20 Viewing transformation back far clipping plane clipping  63 front near clipping plane frustum functions 23  hardware implementation  Subject Index inpul prionty normalized projection coordinates normalized view volume 61 Pipeline 19 33 three dimensional 33 two dimensional 22 viewport  60 view volume window  workstation mapping 22 Viewing table  View plane 34 normal vector position 35 window Viewport chpping  61 function 23 prionty three dumensional see View volume two dimensional __ Workstation View reference point  View up vector  View volume unit cube normalized perspective 49 parallel 50 View window Virtual reality applications  67 display devices 52 input devices environments 93 Visible structure Visible line detection  see also Depth cucing Visible surface detection A buffer method 76 algorithm classification 71 area subdivision method 85 back face detection 72 BSP tree method 82 comparison of algorithms 92 curved surfaces 90 depth bulfer z buffer method 75 depth sorting method 81 function 91 image space methods object space methods octree methods 87 painter s algorithm depth sorting  ray casting method 88 scan line method 78 surface contour plots 90 wireframe methods Vision tristimulus theory  Visualization applications 31 methods 402 see afso Data visualization Voice systems 71 Volume calculations CSG  59 Volume element Volume rendering Voxel  Ww Warn lighting model Wavelength light  Weighted sampling  Weighting surface  Subyect Index Weiler Atherton polygon clipping algorithm Winding number Window functions 23 manager  projection rotated  20 three dimensicnal viewing 56 two dimensional viewing user intertace  view plane 34 workstation 22 Windowing transformation panning rooming 19 Window to viewport mapping  22 Wireframe  Wireframe visibility algorithms  Workstation in graphics applications 60 identifier PHIGS pick filter 85 structure filters  85 transformation 22 window 22 viewport  World coordinates  World to Viewing coordinate transformation  20 38 White function  x x axis rotation 12 x direchon shear X Window System XYZ color model  Y y axis rotation y direction shear YIQ color model  Zz axis rotahon 11 z buffer algorithm  see aiso Depth buffer algorithm  z direction shear  Z mouse  63 zooming  Function Index  A awattEvent  B buildTransformahonMatrix buildTransformationMatrix3  Cc celtArray changeStructureldentifier closeStructure  composeMatnx composeMatrix3 composeTransformationMatnx compose Transformation Matnix3 copy AllElementsFromStructure  1D deletcAlIStructures deletcElement deleteElementRange deletcElementsBetweenLabeis delet cStructure deleteStructureNetwork  E emptyStructure evaluate View MappingMatnx pvaliateViewMappingMatrix3 evaluate ViewOrientationMatnx  evaluate ViewOnentavonMatrixd execuleS ructure  F dllArea HllArea3 filAreaSet AllCrrcle AllCarcleArc nIEllipse HIIEipseArc nilRectangle  G generalizedDrawming nmutive gerChoice gertLocator getLocator3 getPick gerPixel geString getStroke 2h getValuator  ininaiizeChowe initializeLocator initializePick initializeString initializeStroke imihalizeValuator inquire   L label  O offsetElementPomte  openStructure  Pp polyline polyline3 polymarker postStructure  R request hoice requestLocator 83 requestPick requestString   requestStroke 83 TequestValuator rotate  TotateX rotateY rotateZ  S sampleChoice sampleLocator samplePick sampleString sampleStroke sampleValuator scale  scale3  setCharacterExpansionFactor  setCharacterHcight setCharacterSpacing setCharacter UpVector setChoiceMode  setColourRepresentalion setEditMode setElementPointer setElementPointer AtLabel set lighlight ngFilter  set HLHSRidentifier  set ndividual ASF setIntenorC o ourlndex setIntenorlndex setInteriorReprescntation setInteriorstvle setInteriorStvielndex setInvisibility Filter setLinetype setLinewidthScaleFactor setLocalTransformation setLocal Transformation  setLocatorMade 82 setMarkerSizeScaleFactor setMarkerly pe setPatternReferencePoint setPattern Representation setPattemSize setPickFultes setPickldentifier setPickMode 82 setPixel  setPolyhneC olourIndex setPolylinelndex setPolzlineRepresentation setPolymarkerColour ndex setPolymarkerindex setPolymarkerRepresentation setStringMode setStrokeMcde setTextAlignment setTextCulourlndex setTextFont  setTextIndex setTextMods 82 setTextPath setTextPrecision  Function Index setTextRepresentation setYaluatorMode  setViewLndex  setViewRepresentation setViewRepresentation3 setViewTransformationInpulPrionity setWorkstation View port setWorkstation Viewpart3 setWorkstation Window  setWorkstationWindow3  T text  text3  ansformPoint transformPoint3  translate translate3  26 U unpostAlIStructures unpostStructure 